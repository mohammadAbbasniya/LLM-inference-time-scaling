{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dbd9d7e",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "\n",
    "Throughout the notebook, we will explore several inference methods, including:\n",
    "\n",
    "- **Chain-of-Thought (CoT):** A method where the model generates intermediate reasoning steps before providing the final answer.\n",
    "- **Best-of-n Sampling:** An approach that generates multiple candidate responses and selects the best one based on a scoring function.\n",
    "- **Beam Search:** A technique that expands several possible sequences simultaneously, choosing the most promising ones based on probability.\n",
    "- **Self-Refinement:** An iterative process where the model revises its output to improve accuracy and coherence.\n",
    "\n",
    "The **Math Benchmark** is a suite of challenging mathematical problems designed to test the reasoning and problem-solving capabilities of LLMs. The benchmark includes a variety of questions ranging from basic arithmetic and algebra to more advanced topics such as geometry and calculus. For example, you might be asked to solve an equation like `2x + 5 = 15` or compute the derivative of a function, tasks that assess the model's ability to handle both straightforward and complex mathematical queries.\n",
    "\n",
    "Let's dive into the notebook and begin exploring how these methods perform on a challenging set of math problems!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7282ba5d",
   "metadata": {
    "id": "7282ba5d",
    "papermill": {
     "duration": 0.012565,
     "end_time": "2025-05-22T20:33:54.307368",
     "exception": false,
     "start_time": "2025-05-22T20:33:54.294803",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266b9c7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T20:33:54.341754Z",
     "iopub.status.busy": "2025-05-22T20:33:54.341437Z",
     "iopub.status.idle": "2025-05-22T20:36:13.188893Z",
     "shell.execute_reply": "2025-05-22T20:36:13.187863Z"
    },
    "id": "266b9c7d",
    "papermill": {
     "duration": 138.865808,
     "end_time": "2025-05-22T20:36:13.190618",
     "exception": false,
     "start_time": "2025-05-22T20:33:54.324810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q vllm\n",
    "!pip install -q transformers accelerate datasets\n",
    "\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3444575c",
   "metadata": {
    "id": "3444575c",
    "papermill": {
     "duration": 0.010908,
     "end_time": "2025-05-22T20:36:13.213895",
     "exception": false,
     "start_time": "2025-05-22T20:36:13.202987",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* You should use this cell if you're running the notebook on Google Colab. If you're using Kaggle, you don't need to run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d088b4c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T20:36:13.239120Z",
     "iopub.status.busy": "2025-05-22T20:36:13.238220Z",
     "iopub.status.idle": "2025-05-22T20:36:13.242816Z",
     "shell.execute_reply": "2025-05-22T20:36:13.242048Z"
    },
    "id": "d088b4c4",
    "papermill": {
     "duration": 0.01857,
     "end_time": "2025-05-22T20:36:13.244218",
     "exception": false,
     "start_time": "2025-05-22T20:36:13.225648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade numpy\n",
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dff3aa",
   "metadata": {
    "id": "01dff3aa",
    "papermill": {
     "duration": 0.011,
     "end_time": "2025-05-22T20:36:13.266590",
     "exception": false,
     "start_time": "2025-05-22T20:36:13.255590",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## vLLM: Accelerated Inference Engine for LLMs\n",
    "\n",
    "vLLM is an open-source project designed to optimize the loading and inference of large language models. By leveraging advanced memory management techniques and dynamic batching, vLLM significantly speeds up the inference process, making it easier to deploy and experiment with LLMs even on hardware with limited resources\n",
    "So we use vLLM to get results faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e842365",
   "metadata": {
    "id": "7e842365",
    "papermill": {
     "duration": 0.011392,
     "end_time": "2025-05-22T20:36:13.289398",
     "exception": false,
     "start_time": "2025-05-22T20:36:13.278006",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## VLLM Server Setup and Initialization\n",
    "\n",
    "In this section, we install the required packages, ensure that only one server instance is running, and start the VLLM server using the model `deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B`.\n",
    "\n",
    "**Installation and Cleanup:**\n",
    "- The necessary packages (`vllm`, `transformers`, `accelerate`, and `datasets`) are installed in a cell with hidden output to keep the notebook clean.\n",
    "- Any previously running VLLM server instances are terminated before starting a new one. This prevents multiple servers from running simultaneously.\n",
    "\n",
    "**Server Initialization:**\n",
    "- The server is launched as a background process using `subprocess.Popen`.\n",
    "- **Initialization Time:**  \n",
    "  The server typically takes about **1 minute** to fully initialize.\n",
    "- **GPU Memory Utilization:**  \n",
    "  Monitor your GPU memory usage. Initially, it will be at **0 GB**, and then it will gradually increase until it reaches approximately **12 GB** when the server is fully up and running.\n",
    "\n",
    "Please wait until the GPU memory stabilizes around **12 GB** before proceeding to the next steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f59f78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T20:36:13.315164Z",
     "iopub.status.busy": "2025-05-22T20:36:13.314329Z",
     "iopub.status.idle": "2025-05-22T20:39:33.344622Z",
     "shell.execute_reply": "2025-05-22T20:39:33.343689Z"
    },
    "id": "49f59f78",
    "outputId": "aa5f28c7-ecf5-4af7-c956-659aec35ac59",
    "papermill": {
     "duration": 200.119608,
     "end_time": "2025-05-22T20:39:33.420559",
     "exception": false,
     "start_time": "2025-05-22T20:36:13.300951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= run vllm server\n",
      "============= wait for 200 sec\n",
      "============= Server started!\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Kill any running VLLM server instances for the specified model\n",
    "kill_cmd = \"pkill -f 'vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'\"\n",
    "subprocess.run(kill_cmd, shell=True)\n",
    "\n",
    "# Command to start the VLLM server\n",
    "cmd = [\n",
    "    \"vllm\", \"serve\", \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    \"--port\", \"8000\", \"--dtype=half\", \"--max-model-len\", \"5192\"\n",
    "]\n",
    "\n",
    "print('============= run vllm server')\n",
    "server_process = subprocess.Popen(\n",
    "    cmd,\n",
    "    stdin=subprocess.DEVNULL,\n",
    "    stdout=subprocess.DEVNULL,\n",
    "    stderr=subprocess.DEVNULL\n",
    ")\n",
    "\n",
    "\n",
    "print('============= wait for 200 sec')\n",
    "time.sleep(200)\n",
    "\n",
    "print(\"============= Server started!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4110568",
   "metadata": {
    "id": "f4110568",
    "papermill": {
     "duration": 0.011298,
     "end_time": "2025-05-22T20:39:33.443461",
     "exception": false,
     "start_time": "2025-05-22T20:39:33.432163",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* you can debug last cell if doesn't work right with this cell (if that works you DO NOT run this cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7460c07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T20:39:33.467685Z",
     "iopub.status.busy": "2025-05-22T20:39:33.467378Z",
     "iopub.status.idle": "2025-05-22T20:39:33.471444Z",
     "shell.execute_reply": "2025-05-22T20:39:33.470623Z"
    },
    "id": "f7460c07",
    "papermill": {
     "duration": 0.01779,
     "end_time": "2025-05-22T20:39:33.472729",
     "exception": false,
     "start_time": "2025-05-22T20:39:33.454939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!vllm serve \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"   --port 8000   --dtype=half   --max-model-len 5192"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93402233",
   "metadata": {
    "id": "93402233",
    "papermill": {
     "duration": 0.01119,
     "end_time": "2025-05-22T20:39:33.495612",
     "exception": false,
     "start_time": "2025-05-22T20:39:33.484422",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Helper Functions Overview\n",
    "\n",
    "This section contains a series of helper functions designed to facilitate the evaluation of mathematical problem solving using the MATH-500 dataset and a local LLM server. These functions handle tasks such as dataset loading, answer extraction, normalization of various mathematical expressions, answer comparison, and result management. Below is an explanation of each group of functions:\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Loading\n",
    "\n",
    "- **`load_math500_dataset()`**  \n",
    "  Loads the test split of the MATH-500 dataset from the Hugging Face repository (`HuggingFaceH4/MATH-500`). This dataset provides the math problems and corresponding solutions used for evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## Answer Extraction\n",
    "\n",
    "- **`extract_answer(response: str) -> Optional[str]`**  \n",
    "  Searches the provided text for the last occurrence of the LaTeX command `\\boxed{...}` and extracts the content within it. This function is essential for retrieving the final answer from the formatted solutions.\n",
    "\n",
    "---\n",
    "\n",
    "## Normalization Functions\n",
    "\n",
    "Normalization functions standardize the format of mathematical expressions to enable accurate comparisons between predicted and correct answers. These functions account for various representations, ensuring that equivalent answers written in different formats are recognized as equal.\n",
    "\n",
    "- **`normalize_number(num_str: str) -> str`**  \n",
    "  Cleans and normalizes numeric strings by removing extraneous characters (e.g., commas, currency symbols, and measurement units) and formatting them into a consistent number format.\n",
    "\n",
    "- **`numerically_equal(str1: str, str2: str) -> bool`**  \n",
    "  Checks if two numeric strings represent the same value within a small tolerance, accounting for floating point precision issues.\n",
    "\n",
    "- **`normalize_fraction(fraction_str: str) -> str`**  \n",
    "  Converts various representations of fractions (with or without braces or using a slash) into a standard LaTeX format: `\\frac{numerator}{denominator}`.\n",
    "\n",
    "- **`normalize_matrix_entry(entry: str) -> str`**  \n",
    "  Standardizes individual matrix entries, especially handling fractions and slash-separated numbers, to ensure consistency within matrix representations.\n",
    "\n",
    "- **`normalize_matrix(matrix_str: str) -> str`**  \n",
    "  Processes a LaTeX matrix (formatted with `\\begin{pmatrix}` and `\\end{pmatrix}`) by normalizing each row and each entry using the matrix entry normalization.\n",
    "\n",
    "- **`normalize_algebraic_expression(expr: str) -> str`**  \n",
    "  Standardizes algebraic expressions by handling coefficients, variables, exponents, and special terms like π (pi). This helps compare algebraic answers regardless of minor formatting differences.\n",
    "\n",
    "- **`normalize_interval_bound(bound: str) -> str`**  \n",
    "  Normalizes the boundary of an interval, ensuring that symbols like infinity (`\\infty`) and other numeric boundaries are consistently formatted.\n",
    "\n",
    "- **`normalize_interval(interval_str: str) -> str`**  \n",
    "  Standardizes an interval provided in LaTeX, ensuring that both bounds are normalized and that the overall format (including brackets) is consistent.\n",
    "\n",
    "- **`normalize_ordered_tuple(tuple_str: str) -> str`**  \n",
    "  Normalizes an ordered tuple by splitting its elements and applying answer normalization to each component, ensuring a standard tuple representation.\n",
    "\n",
    "- **`normalize_answer(answer: str) -> str`**  \n",
    "  The central normalization function that applies the various normalization steps to a given answer. It cleans up LaTeX formatting, removes unnecessary spaces, and calls the specialized normalization functions to standardize numeric, fractional, algebraic, and other mathematical expressions.\n",
    "\n",
    "---\n",
    "\n",
    "## Answer Comparison\n",
    "\n",
    "- **`compare_answers(correct_answer: str, predicted_answer: Optional[str]) -> bool`**  \n",
    "  Compares the normalized versions of the correct answer and the predicted answer. This function ensures that answers are compared in a standardized format so that minor differences in formatting do not affect the evaluation outcome.\n",
    "\n",
    "---\n",
    "\n",
    "## Result Management Functions\n",
    "\n",
    "These functions handle saving and analyzing the results of the evaluation process.\n",
    "\n",
    "- **`load_existing_results(filename: str) -> list[Dict]`**  \n",
    "  Loads previously saved evaluation results from a JSON file. If the file does not exist, it returns an empty list.\n",
    "\n",
    "- **`save_result(filename: str, result: Dict)`**  \n",
    "  Appends a single evaluation result (including problem details, the LLM response, and correctness) to the results file in JSON format.\n",
    "\n",
    "- **`analyze_results(results: list[Dict])`**  \n",
    "  Analyzes the evaluation outcomes by summarizing the total number of problems, counting the correct answers, calculating the accuracy, and printing details for any problems that were answered incorrectly.\n",
    "\n",
    "---\n",
    "\n",
    "## Main Evaluation and Response Handling\n",
    "\n",
    "- **`evaluate()`**  \n",
    "  The primary function that orchestrates the evaluation process:\n",
    "  - Creates a results directory if it doesn't already exist.\n",
    "  - Loads the MATH-500 dataset.\n",
    "  - Iterates over each problem (while skipping already processed ones).\n",
    "  - Sends the problem text to the local LLM server using `get_llm_response`.\n",
    "  - Extracts and compares the answers, then saves the result.\n",
    "  - Finally, it analyzes and prints a summary of the evaluation.\n",
    "\n",
    "- **`get_llm_response(prompt: str) -> str`**  \n",
    "  Sends a prompt to the locally running LLM server (via an HTTP POST request to `http://localhost:8000/v1/chat/completions`) and returns the server's response. This function is key to obtaining the model's predicted answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c929ad7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T20:39:33.519981Z",
     "iopub.status.busy": "2025-05-22T20:39:33.519688Z",
     "iopub.status.idle": "2025-05-22T20:39:37.375859Z",
     "shell.execute_reply": "2025-05-22T20:39:37.375044Z"
    },
    "id": "1c929ad7",
    "papermill": {
     "duration": 3.87066,
     "end_time": "2025-05-22T20:39:37.377551",
     "exception": false,
     "start_time": "2025-05-22T20:39:33.506891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import Dict, Optional, Union\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Load the MATH-500 dataset\n",
    "def load_math500_dataset():\n",
    "    dataset = load_dataset(\"HuggingFaceH4/MATH-500\")[\"test\"]\n",
    "    return dataset\n",
    "\n",
    "# Extract the last boxed answer from text\n",
    "def extract_answer(response: str) -> Optional[str]:\n",
    "    if not response:\n",
    "        return None\n",
    "    start_idx = response.rfind('\\\\boxed{')\n",
    "    if start_idx == -1:\n",
    "        return None\n",
    "    brace_count = 1\n",
    "    pos = start_idx + 7  # length of '\\boxed{'\n",
    "    while pos < len(response) and brace_count > 0:\n",
    "        if response[pos] == '{':\n",
    "            brace_count += 1\n",
    "        elif response[pos] == '}':\n",
    "            brace_count -= 1\n",
    "        pos += 1\n",
    "    if brace_count == 0:\n",
    "        answer = response[start_idx + 7:pos - 1]\n",
    "        return answer.strip()\n",
    "    return None\n",
    "\n",
    "# Normalization and comparison functions (unchanged from original)\n",
    "def normalize_number(num_str: str) -> str:\n",
    "    try:\n",
    "        cleaned = re.sub(r'[,\\$\\\\]|\\s*(?:cm|m|kg|ft|in|lb|oz|ml|L)$|\\s*\\\\text{[^}]+}', '', num_str).strip()\n",
    "        if cleaned.startswith('.'):\n",
    "            cleaned = '0' + cleaned\n",
    "        num = float(cleaned)\n",
    "        if abs(num) < 1 and '.' in cleaned:\n",
    "            decimal_places = len(cleaned.split('.')[1])\n",
    "            format_str = f\"{{:.{decimal_places}f}}\"\n",
    "            result = format_str.format(num)\n",
    "        else:\n",
    "            result = str(num)\n",
    "        return result\n",
    "    except:\n",
    "        return num_str\n",
    "\n",
    "def numerically_equal(str1: str, str2: str) -> bool:\n",
    "    try:\n",
    "        return abs(float(str1) - float(str2)) < 1e-10\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def normalize_fraction(fraction_str: str) -> str:\n",
    "    try:\n",
    "        fraction_str = fraction_str.replace('\\\\dfrac', '\\\\frac')\n",
    "        fraction_str = ''.join(fraction_str.split())\n",
    "        fraction_str = re.sub(r'\\s*\\\\text{[^}]+}', '', fraction_str)\n",
    "        mixed_brace = re.match(r'^\\\\frac(\\d+)\\{(\\d+)\\}$', fraction_str)\n",
    "        if mixed_brace:\n",
    "            num, den = mixed_brace.groups()\n",
    "            return f\"\\\\frac{{{num}}}{{{den}}}\"\n",
    "        no_braces = re.match(r'^\\\\frac(\\d+)(\\d+)$', fraction_str)\n",
    "        if no_braces:\n",
    "            num, den = no_braces.groups()\n",
    "            return f\"\\\\frac{{{num}}}{{{den}}}\"\n",
    "        if '/' in fraction_str and not any(c in fraction_str for c in '\\\\{}'):\n",
    "            num, den = fraction_str.split('/')\n",
    "            return f\"\\\\frac{{{num.strip()}}}{{{den.strip()}}}\"\n",
    "        standard = re.match(r'^\\\\frac\\{([^{}]+)\\}\\{([^{}]+)\\}$', fraction_str)\n",
    "        if standard:\n",
    "            num, den = standard.groups()\n",
    "            return f\"\\\\frac{{{num}}}{{{den}}}\"\n",
    "    except:\n",
    "        return fraction_str\n",
    "\n",
    "def normalize_matrix_entry(entry: str) -> str:\n",
    "    entry = ''.join(entry.split())\n",
    "    if '/' in entry and not any(c in entry for c in '\\\\{}'):\n",
    "        if entry.startswith('-'):\n",
    "            num, den = entry[1:].split('/')\n",
    "            return f\"-{num.strip()}/{den.strip()}\"\n",
    "        else:\n",
    "            num, den = entry.split('/')\n",
    "            return f\"{num.strip()}/{den.strip()}\"\n",
    "    entry = entry.replace('\\\\dfrac', '\\\\frac')\n",
    "    frac_match = re.match(r'^(-)?\\\\frac\\{(\\d+)\\}\\{(\\d+)\\}$', entry)\n",
    "    if frac_match:\n",
    "        sign, num, den = frac_match.groups()\n",
    "        sign = sign if sign else ''\n",
    "        return f\"{sign}{num}/{den}\"\n",
    "    return entry\n",
    "\n",
    "def normalize_matrix(matrix_str: str) -> str:\n",
    "    try:\n",
    "        matrix_str = ''.join(matrix_str.split())\n",
    "        match = re.match(r'^\\\\begin\\{pmatrix\\}(.*?)\\\\end\\{pmatrix\\}$', matrix_str)\n",
    "        if not match:\n",
    "            return matrix_str\n",
    "        content = match.group(1)\n",
    "        rows = content.split('\\\\\\\\')\n",
    "        normalized_rows = []\n",
    "        for row in rows:\n",
    "            if '&' in row:\n",
    "                entries = [normalize_matrix_entry(entry) for entry in row.split('&')]\n",
    "            else:\n",
    "                entries = [normalize_matrix_entry(row)]\n",
    "            normalized_rows.append('&'.join(entries))\n",
    "        result = \"\\\\begin{pmatrix}\" + \"\\\\\\\\\".join(normalized_rows) + \"\\\\end{pmatrix}\"\n",
    "        return result\n",
    "    except:\n",
    "        return matrix_str\n",
    "\n",
    "def normalize_algebraic_expression(expr: str) -> str:\n",
    "    try:\n",
    "        expr = ''.join(expr.split())\n",
    "        monomial_match = re.match(r'^(-?\\d*\\.?\\d*)?([a-zA-Z])(?:\\^(-?\\d+))?$', expr)\n",
    "        if monomial_match:\n",
    "            coeff, var, exp = monomial_match.groups()\n",
    "            coeff = coeff if coeff and coeff not in ['+', '-'] else ('1' if not coeff else '-1')\n",
    "            exp = exp if exp else '1'\n",
    "            if coeff == '1' and exp == '1':\n",
    "                return var\n",
    "            elif coeff == '1':\n",
    "                return f\"{var}^{exp}\"\n",
    "            elif coeff == '-1' and exp == '1':\n",
    "                return f\"-{var}\"\n",
    "            elif coeff == '-1':\n",
    "                return f\"-{var}^{exp}\"\n",
    "            elif exp == '1':\n",
    "                return f\"{coeff}{var}\"\n",
    "            else:\n",
    "                return f\"{coeff}{var}^{exp}\"\n",
    "        pi_term_match = re.match(r'^(-?\\d*\\.?\\d*)\\\\?pi$', expr)\n",
    "        if pi_term_match:\n",
    "            coeff = pi_term_match.group(1)\n",
    "            if not coeff or coeff == '-':\n",
    "                coeff = '-1' if coeff == '-' else '1'\n",
    "            return f\"{coeff}\\\\pi\"\n",
    "        frac_pi_match = re.match(r'^\\\\frac{([^{}]+)}{([^{}]+)}\\\\?pi$', expr)\n",
    "        if frac_pi_match:\n",
    "            num, den = frac_pi_match.groups()\n",
    "            return f\"\\\\frac{{{num}}}{{{den}}}\\\\pi\"\n",
    "        frac_match = re.match(r'^\\\\frac{([^{}]+)}{([^{}]+)}$', expr)\n",
    "        if frac_match:\n",
    "            num, den = frac_match.groups()\n",
    "            return f\"\\\\frac{{{num}}}{{{den}}}\"\n",
    "    except:\n",
    "        return expr.lower()\n",
    "\n",
    "def normalize_interval_bound(bound: str) -> str:\n",
    "    if '\\\\infty' in bound:\n",
    "        sign = '-' if bound.startswith('-') else ''\n",
    "        return f\"{sign}\\\\infty\"\n",
    "    return normalize_answer(bound) or bound\n",
    "\n",
    "def normalize_interval(interval_str: str) -> str:\n",
    "    try:\n",
    "        interval_str = ''.join(interval_str.split())\n",
    "        match = re.match(r'^\\\\left?([\\[\\(])(.*?),(.*?)\\\\right?([\\]\\)])$', interval_str)\n",
    "        if not match:\n",
    "            match = re.match(r'^([\\[\\(])(.*?),(.*?)([\\]\\)])$', interval_str)\n",
    "            if not match:\n",
    "                return interval_str\n",
    "        left_bracket, left_bound, right_bound, right_bracket = match.groups()\n",
    "        norm_left = normalize_interval_bound(left_bound)\n",
    "        norm_right = normalize_interval_bound(right_bound)\n",
    "        return f\"\\\\left{left_bracket}{norm_left},{norm_right}\\\\right{right_bracket}\"\n",
    "    except:\n",
    "        return interval_str\n",
    "\n",
    "def normalize_ordered_tuple(tuple_str: str) -> str:\n",
    "    try:\n",
    "        tuple_str = tuple_str.replace('\\\\dfrac', '\\\\frac')\n",
    "        tuple_str = tuple_str.replace('\\\\left', '').replace('\\\\right', '')\n",
    "        tuple_str = re.sub(r'\\\\?\\s+', '', tuple_str)\n",
    "        inner = tuple_str.strip('()')\n",
    "        parts = inner.split(',')\n",
    "        normalized_parts = [normalize_answer(part.strip()) for part in parts if normalize_answer(part.strip())]\n",
    "        return f\"({','.join(normalized_parts)})\"\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def normalize_answer(answer: str) -> str:\n",
    "    if answer is None:\n",
    "        return \"\"\n",
    "    answer = re.sub(r'\\\\text{[^}]+(?:inches|feet|meters|cm|m|kg|ft|in|lb|oz|ml|L|per|second|minute|hour)[^}]*}', '', answer)\n",
    "    answer = re.sub(r'(?<!\\\\)\\s+', '', answer)\n",
    "    ordered_pair_match = re.match(r'^(?:\\\\left)?\\((.*?)(?:\\\\right)?\\)$', answer)\n",
    "    if ordered_pair_match:\n",
    "        content = ordered_pair_match.group(1)\n",
    "        parts = content.split(',')\n",
    "        normalized_parts = [normalize_answer(part) for part in parts if normalize_answer(part)]\n",
    "        return f\"({','.join(normalized_parts)})\"\n",
    "    answer = ''.join(answer.split())\n",
    "    if not answer:\n",
    "        return None\n",
    "    pm_match = re.match(r'^(.*?)(?:\\\\pm|-)(.*?)$', answer)\n",
    "    if pm_match:\n",
    "        left, right = pm_match.groups()\n",
    "        norm_left = normalize_answer(left) if left else \"\"\n",
    "        norm_right = normalize_answer(right) if right else \"\"\n",
    "        if norm_left or norm_right:\n",
    "            return f\"{norm_left}\\\\pm{norm_right}\"\n",
    "    trig_match = re.match(r'^\\\\(?:sin|cos|tan|cot|sec|csc)\\s*([a-zA-Z])$', answer)\n",
    "    if trig_match:\n",
    "        variable = trig_match.group(1)\n",
    "        func_name = re.match(r'^\\\\(.*?)(?:\\s|$)', answer).group(1)\n",
    "        return f\"\\\\{func_name}{variable}\"\n",
    "    text_match = re.match(r'^(?:\\\\text{)?([A-Za-z]+)(?:})?$', answer)\n",
    "    if text_match:\n",
    "        return text_match.group(1).lower()\n",
    "    if (answer.startswith('\\\\left[') or answer.startswith('\\\\left(') or\n",
    "        answer.startswith('[') or answer.startswith('(')) and \\\n",
    "       (answer.endswith('\\\\right]') or answer.endswith('\\\\right)') or\n",
    "        answer.endswith(']') or answer.endswith(')')):\n",
    "        return normalize_interval(answer)\n",
    "    if answer.startswith('\\\\begin{pmatrix}') and answer.endswith('\\\\end{pmatrix}'):\n",
    "        return normalize_matrix(answer)\n",
    "    answer = answer.replace('\\\\dfrac', '\\\\frac')\n",
    "    if '\\\\frac' in answer or '/' in answer:\n",
    "        return normalize_fraction(answer)\n",
    "    neg_sqrt_match = re.match(r'^-\\\\sqrt\\{?(\\d+)\\}?$', answer)\n",
    "    if neg_sqrt_match:\n",
    "        num = neg_sqrt_match.group(1)\n",
    "        return f\"-\\\\sqrt{{{num}}}\"\n",
    "    sqrt_match = re.match(r'^(\\d*)?\\\\sqrt\\{?(\\d+)\\}?$', answer)\n",
    "    if sqrt_match:\n",
    "        coeff, num = sqrt_match.groups()\n",
    "        coeff = coeff if coeff else '1'\n",
    "        return f\"\\\\sqrt{{{num}}}\" if coeff == '1' else f\"{coeff}\\\\sqrt{{{num}}}\"\n",
    "    sqrt_with_coeff_match = re.match(r'^(\\d+)\\\\sqrt\\{?(\\d+)\\}?$', answer)\n",
    "    if sqrt_with_coeff_match:\n",
    "        coeff, num = sqrt_with_coeff_match.groups()\n",
    "        return f\"{coeff}\\\\sqrt{{{num}}}\"\n",
    "    base_match = re.match(r'^(\\d+)(?:_\\{?(\\d+)\\}?|_(\\d+))$', answer)\n",
    "    if base_match:\n",
    "        number, base1, base2 = base_match.groups()\n",
    "        base = base1 if base1 else base2\n",
    "        return f\"{number}_{base}\"\n",
    "    percent_match = re.match(r'^(\\d+(?:\\.\\d*)?)\\s*\\\\?%$', answer)\n",
    "    if percent_match:\n",
    "        return normalize_number(percent_match.group(1))\n",
    "    unit_match = re.match(r'^(\\d+(?:\\.\\d*)?)\\s*(?:(?:\\\\[,\\s])|,)?\\s*(?:\\\\\\\\)?(?:\\\\text{(\\w+)}|\\\\?(?:cm|m|kg|ft|in|lb|oz|ml|L))$', answer)\n",
    "    if unit_match:\n",
    "        return normalize_number(unit_match.group(1))\n",
    "    currency_match = re.match(r'^\\\\?\\$?([\\d,]+\\.?\\d*)$', answer)\n",
    "    if currency_match:\n",
    "        return normalize_number(currency_match.group(1))\n",
    "    if re.match(r'^-?[\\d,]+$', answer):\n",
    "        return normalize_number(answer)\n",
    "    unit_match = re.match(r'^(-?[\\d,]+(?:\\.\\d*)?)\\s*(?:\\\\(?:mbox|text|hbox|displaystyle)\\{[^}]+\\})?(?:\\^?\\d)?$', answer)\n",
    "    if unit_match:\n",
    "        return normalize_number(unit_match.group(1))\n",
    "    mc_match = re.match(r'^\\\\text{\\(?([A-Za-z])\\)?}$|^\\(?([A-Za-z])\\)?$', answer)\n",
    "    if mc_match:\n",
    "        return (mc_match.group(1) or mc_match.group(2)).lower()\n",
    "    degree_match = re.match(r'^(-?[\\d,]+(?:\\.\\d*)?)\\s*(?:(?:\\^?\\\\circ)|(?:{\\\\circ})|(?:°))?$', answer)\n",
    "    if degree_match:\n",
    "        return normalize_number(degree_match.group(1))\n",
    "    answer = re.sub(r'\\\\text{([^{}]+)}', r'\\1', answer)\n",
    "    try:\n",
    "        return normalize_algebraic_expression(answer)\n",
    "    except:\n",
    "        pass\n",
    "    answer = answer.replace('\\\\left', '').replace('\\\\right', '')\n",
    "    answer = answer.replace('\\\\(', '(').replace('\\\\)', ')')\n",
    "    answer = answer.replace('\\\\[', '[').replace('\\\\]', ']')\n",
    "    answer = answer.replace('\\\\{', '{').replace('\\\\}', '}')\n",
    "    answer = re.sub(r'\\\\sqrt\\{?(\\d+)\\}?', r'\\\\sqrt{\\1}', answer)\n",
    "    answer = re.sub(r'\\\\sqrt{([^{}]+)}', r'\\\\sqrt\\1', answer)\n",
    "    if re.match(r'^\\d+\\\\%$', answer) or re.match(r'^\\d+$', answer):\n",
    "        answer = re.sub(r'\\\\%$', '', answer)\n",
    "    answer = re.sub(r'\\\\text{([^{}]+)}', r'\\1', answer)\n",
    "    while len(answer) >= 2 and answer[0] == '{' and answer[-1] == '}':\n",
    "        if '\\\\frac' in answer:\n",
    "            break\n",
    "        answer = answer[1:-1]\n",
    "    return answer.lower() if answer else None\n",
    "\n",
    "def compare_answers(correct_answer: str, predicted_answer: Optional[str]) -> bool:\n",
    "    if predicted_answer is None:\n",
    "        return False\n",
    "    if numerically_equal(correct_answer, predicted_answer):\n",
    "        return True\n",
    "    normalized_correct = normalize_answer(correct_answer)\n",
    "    normalized_predicted = normalize_answer(predicted_answer)\n",
    "    if not normalized_correct or not normalized_predicted:\n",
    "        return False\n",
    "    if normalized_correct == \"\" and normalized_predicted == \"\":\n",
    "        return False\n",
    "    if ('\\\\left[' in normalized_correct or '\\\\left(' in normalized_correct) and \\\n",
    "       ('\\\\left[' in normalized_predicted or '\\\\left(' in normalized_predicted):\n",
    "        return normalized_correct == normalized_predicted\n",
    "    return normalized_correct == normalized_predicted\n",
    "\n",
    "# Load existing results\n",
    "def load_existing_results(filename: str) -> list[Dict]:\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return []\n",
    "\n",
    "# Save a single result\n",
    "def save_result(filename: str, result: Dict):\n",
    "    results = load_existing_results(filename)\n",
    "    results.append(result)\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "# Analyze and print results\n",
    "def analyze_results(results: list[Dict]):\n",
    "    total = len(results)\n",
    "    correct = sum(1 for r in results if r['is_correct'])\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    print(\"\\n=== Results Summary ===\")\n",
    "    print(f\"Total problems: {total}\")\n",
    "    print(f\"Correct answers: {correct}\")\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    print(\"\\n=== Incorrect Problems ===\")\n",
    "    for r in results:\n",
    "        if not r['is_correct']:\n",
    "            print(f\"Problem {r['index']}:\")\n",
    "            print(f\"Expected: {r['correct_answer']}\")\n",
    "            print(f\"Predicted: {r['predicted_answer']}\")\n",
    "            print(\"---\")\n",
    "\n",
    "# Main evaluation function\n",
    "def evaluate():\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    results_file = \"evaluation_results_math500_deepseek.json\"\n",
    "    dataset = load_math500_dataset()\n",
    "    existing_results = load_existing_results(results_file)\n",
    "    processed_indexes = {result['index'] for result in existing_results}\n",
    "    cnt = 0\n",
    "    t=0\n",
    "    for idx, item in enumerate(tqdm(dataset, desc=\"Evaluating problems\")):\n",
    "        if idx in processed_indexes:\n",
    "            continue\n",
    "        t += 1\n",
    "        problem_text = item['problem']\n",
    "        correct_answer = extract_answer(item['solution'])  # Extract from 'solution', not 'answer'\n",
    "        response = get_llm_response(problem_text)\n",
    "        predicted_answer = extract_answer(response)\n",
    "        is_correct = compare_answers(correct_answer, predicted_answer)\n",
    "        result = {\n",
    "            \"index\": idx,\n",
    "            \"problem\": problem_text,\n",
    "            \"response\": response,\n",
    "            \"correct_answer\": correct_answer,\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"is_correct\": is_correct\n",
    "        }\n",
    "        save_result(results_file, result)\n",
    "        if is_correct:\n",
    "          cnt += 1\n",
    "        print(f\"cnt :  {cnt} idx: {t}\")\n",
    "    final_results = load_existing_results(results_file)\n",
    "    analyze_results(final_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3f9423",
   "metadata": {
    "id": "dd3f9423",
    "papermill": {
     "duration": 0.01091,
     "end_time": "2025-05-22T20:39:37.400449",
     "exception": false,
     "start_time": "2025-05-22T20:39:37.389539",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LLM Query Function\n",
    "\n",
    "* This Python function sends prompts to a locally-hosted LLM API and returns the generated response\n",
    "* you can change max_tokens and temperature as you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b382f17a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T20:39:37.425352Z",
     "iopub.status.busy": "2025-05-22T20:39:37.424700Z",
     "iopub.status.idle": "2025-05-22T20:39:37.429734Z",
     "shell.execute_reply": "2025-05-22T20:39:37.428898Z"
    },
    "id": "b382f17a",
    "papermill": {
     "duration": 0.018627,
     "end_time": "2025-05-22T20:39:37.431001",
     "exception": false,
     "start_time": "2025-05-22T20:39:37.412374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "def get_llm_response(prompt):\n",
    "    url = \"http://localhost:8000/v1/chat/completions\"\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 500,\n",
    "        \"temperature\": 0.6\n",
    "    }\n",
    "    response = requests.post(url, json=payload)\n",
    "    return response.json()['choices'][0]['message']['content'].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfbdeb9",
   "metadata": {
    "id": "ddfbdeb9",
    "papermill": {
     "duration": 0.01128,
     "end_time": "2025-05-22T20:39:37.453774",
     "exception": false,
     "start_time": "2025-05-22T20:39:37.442494",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test Prompt: Evaluating an Integral\n",
    "\n",
    "In this cell, we define a new math benchmark question to verify that the LLM server is correctly set up and that responses can be retrieved.\n",
    "\n",
    "**Question:**  \n",
    "What is the value of the integral  \n",
    "$$\\int_0^1 x^2\\,dx$$  \n",
    "\n",
    "**Expected Answer:**  \n",
    "$$\\boxed{\\frac{1}{3}}$$\n",
    "\n",
    "The cell sends this prompt to the LLM server using the `get_llm_response` function and prints the response. This helps confirm that the integration between the notebook and the LLM server is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba46741c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T20:39:37.478164Z",
     "iopub.status.busy": "2025-05-22T20:39:37.477431Z",
     "iopub.status.idle": "2025-05-22T20:39:42.165115Z",
     "shell.execute_reply": "2025-05-22T20:39:42.164314Z"
    },
    "id": "ba46741c",
    "outputId": "ad85db43-f160-489e-96cb-12990e471a45",
    "papermill": {
     "duration": 4.701411,
     "end_time": "2025-05-22T20:39:42.166615",
     "exception": false,
     "start_time": "2025-05-22T20:39:37.465204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: To evaluate the integral of \\( x^2 \\) from 0 to 1, I can use the power rule for integration. \n",
      "\n",
      "First, I find the antiderivative of \\( x^2 \\), which is \\( \\frac{x^3}{3} \\).\n",
      "\n",
      "Next, I apply the Fundamental Theorem of Calculus by substituting the upper and lower limits into the antiderivative and subtracting.\n",
      "\n",
      "Finally, I subtract \\( \\frac{0^3}{3} \\) from \\( \\frac{1^3}{3} \\) to get the value of the integral.\n",
      "</think>\n",
      "\n",
      "The value of the integral is calculated as follows:\n",
      "\n",
      "\\[\n",
      "\\int_0^1 x^2 \\, dx = \\left[ \\frac{x^3}{3} \\right]_0^1 = \\frac{1^3}{3} - \\frac{0^3}{3} = \\frac{1}{3}\n",
      "\\]\n",
      "\n",
      "So, the final answer is:\n",
      "\n",
      "\\[\n",
      "\\boxed{\\dfrac{1}{3}}\n",
      "\\]\n"
     ]
    }
   ],
   "source": [
    "# Define a new math benchmark question for testing\n",
    "question = \"What is the value of the integral $$\\\\int_0^1 x^2 dx$$ answer it directly in one sentence?\"\n",
    "# Real answer: \\boxed{\\frac{1}{3}}\n",
    "\n",
    "# Get response from the LLM server using the provided get_llm_response function\n",
    "response = get_llm_response(question)\n",
    "\n",
    "# Print the response to verify that the setup is working correctly\n",
    "print(\"Response:\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b567e678",
   "metadata": {
    "id": "b567e678",
    "papermill": {
     "duration": 0.011822,
     "end_time": "2025-05-22T20:39:42.190788",
     "exception": false,
     "start_time": "2025-05-22T20:39:42.178966",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Customizable CoT Prompt Template\n",
    "* modify cot prompt then evaluate on math benchmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d70b491",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T20:39:42.216170Z",
     "iopub.status.busy": "2025-05-22T20:39:42.215831Z",
     "iopub.status.idle": "2025-05-22T20:39:42.222124Z",
     "shell.execute_reply": "2025-05-22T20:39:42.220964Z"
    },
    "id": "7d70b491",
    "papermill": {
     "duration": 0.020465,
     "end_time": "2025-05-22T20:39:42.223531",
     "exception": false,
     "start_time": "2025-05-22T20:39:42.203066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Define the system prompt\n",
    "COT_PROMPT = '''You are solving mathematics problems.\n",
    "\n",
    "Please think step by step.\n",
    "\n",
    "Important: Always end your solution with the final answer in this format:\n",
    "\n",
    "\\\\[\n",
    "\\\\boxed{your_answer_here}\n",
    "\\\\]\n",
    "\n",
    "The entire answer should be contained completely within the \\\\boxed{} command.'''\n",
    "\n",
    "def get_COT_response(problem):\n",
    "    prompt = COT_PROMPT + \"\\n\" + problem\n",
    "    url = \"http://localhost:8000/v1/chat/completions\"\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "\n",
    "        ],\n",
    "    \"max_tokens\": 1900,\n",
    "    \"temperature\": 0.3\n",
    "    }\n",
    "    response = requests.post(url, json=payload)\n",
    "    return response.json()['choices'][0]['message']['content'].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92465a8c",
   "metadata": {
    "id": "92465a8c",
    "papermill": {
     "duration": 0.011796,
     "end_time": "2025-05-22T20:39:42.248181",
     "exception": false,
     "start_time": "2025-05-22T20:39:42.236385",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluate CoT\n",
    "* modify response generation part to evalute this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d65a0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T20:39:42.275499Z",
     "iopub.status.busy": "2025-05-22T20:39:42.275141Z",
     "iopub.status.idle": "2025-05-22T20:39:42.281905Z",
     "shell.execute_reply": "2025-05-22T20:39:42.281053Z"
    },
    "id": "53d65a0d",
    "papermill": {
     "duration": 0.022254,
     "end_time": "2025-05-22T20:39:42.283309",
     "exception": false,
     "start_time": "2025-05-22T20:39:42.261055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_cot():\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    results_file = \"evaluation_results_math500_deepseek_cot.json\"\n",
    "    dataset = load_math500_dataset()\n",
    "    existing_results = load_existing_results(results_file)\n",
    "    processed_indexes = {result['index'] for result in existing_results}\n",
    "    cnt = 0\n",
    "    for idx, item in enumerate(tqdm(dataset, desc=\"Evaluating problems\")):\n",
    "        if idx in processed_indexes:\n",
    "            continue\n",
    "        if idx >= 30:\n",
    "          break\n",
    "        problem_text = item['problem']\n",
    "        correct_answer = extract_answer(item['solution'])\n",
    "        ##########################################################\n",
    "        response = get_COT_response(problem_text)\n",
    "        predicted_answer = extract_answer(response)\n",
    "        ##########################################################\n",
    "        is_correct = compare_answers(correct_answer, predicted_answer)\n",
    "        result = {\n",
    "            \"index\": idx,\n",
    "            \"problem\": problem_text,\n",
    "            \"response\": response,\n",
    "            \"correct_answer\": correct_answer,\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"is_correct\": is_correct\n",
    "        }\n",
    "        save_result(results_file, result)\n",
    "        if is_correct:\n",
    "          cnt += 1\n",
    "        print(f\"corrects :  {cnt} idx: {idx}\")\n",
    "    final_results = load_existing_results(results_file)\n",
    "    analyze_results(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde194f2",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "0a6dd49dcbac4feea15ab6f64467a032",
      "3ffa2fb6df2545449633eefcf2f3bbd2",
      "04fe51fd28be4cd3b049b2941015b839"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-05-22T20:39:42.309413Z",
     "iopub.status.busy": "2025-05-22T20:39:42.308840Z",
     "iopub.status.idle": "2025-05-22T20:51:02.741104Z",
     "shell.execute_reply": "2025-05-22T20:51:02.740263Z"
    },
    "id": "bde194f2",
    "outputId": "c8d30658-7594-4745-e4c2-3595c9a0acfb",
    "papermill": {
     "duration": 680.447319,
     "end_time": "2025-05-22T20:51:02.742786",
     "exception": false,
     "start_time": "2025-05-22T20:39:42.295467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a6dd49dcbac4feea15ab6f64467a032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/412 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ffa2fb6df2545449633eefcf2f3bbd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.jsonl:   0%|          | 0.00/447k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04fe51fd28be4cd3b049b2941015b839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   0%|          | 1/500 [00:07<1:05:19,  7.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  1 idx: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   0%|          | 2/500 [00:40<3:06:39, 22.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  1 idx: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   1%|          | 3/500 [00:52<2:26:19, 17.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  2 idx: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   1%|          | 4/500 [00:59<1:52:32, 13.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  3 idx: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   1%|          | 5/500 [01:10<1:44:39, 12.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  4 idx: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   1%|          | 6/500 [01:17<1:27:23, 10.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  5 idx: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   1%|▏         | 7/500 [01:51<2:28:45, 18.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  5 idx: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   2%|▏         | 8/500 [02:19<2:54:24, 21.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  6 idx: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   2%|▏         | 9/500 [02:38<2:48:26, 20.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  7 idx: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   2%|▏         | 10/500 [03:11<3:20:56, 24.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  7 idx: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   2%|▏         | 11/500 [03:45<3:42:43, 27.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  7 idx: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   2%|▏         | 12/500 [04:18<3:57:37, 29.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  7 idx: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   3%|▎         | 13/500 [04:28<3:08:10, 23.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  7 idx: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   3%|▎         | 14/500 [04:34<2:27:01, 18.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  8 idx: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   3%|▎         | 15/500 [05:08<3:04:29, 22.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  8 idx: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   3%|▎         | 16/500 [05:41<3:30:22, 26.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  8 idx: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   3%|▎         | 17/500 [05:50<2:46:42, 20.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  9 idx: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   4%|▎         | 18/500 [06:23<3:17:38, 24.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  9 idx: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   4%|▍         | 19/500 [06:57<3:38:57, 27.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  9 idx: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   4%|▍         | 20/500 [07:29<3:50:33, 28.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  10 idx: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   4%|▍         | 21/500 [07:48<3:26:10, 25.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  10 idx: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   4%|▍         | 22/500 [08:22<3:44:15, 28.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  10 idx: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   5%|▍         | 23/500 [08:55<3:56:45, 29.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  10 idx: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   5%|▍         | 24/500 [09:23<3:50:19, 29.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  10 idx: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   5%|▌         | 25/500 [09:33<3:06:38, 23.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  10 idx: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   5%|▌         | 26/500 [10:07<3:29:52, 26.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  10 idx: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   5%|▌         | 27/500 [10:41<3:45:56, 28.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  10 idx: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   6%|▌         | 28/500 [10:50<2:59:42, 22.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  11 idx: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   6%|▌         | 29/500 [11:11<2:55:03, 22.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  11 idx: 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   6%|▌         | 30/500 [11:19<2:57:20, 22.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  12 idx: 29\n",
      "\n",
      "=== Results Summary ===\n",
      "Total problems: 30\n",
      "Correct answers: 12\n",
      "Accuracy: 40.00%\n",
      "\n",
      "=== Incorrect Problems ===\n",
      "Problem 1:\n",
      "Expected: p - q\n",
      "Predicted: None\n",
      "---\n",
      "Problem 6:\n",
      "Expected: 27\n",
      "Predicted: None\n",
      "---\n",
      "Problem 9:\n",
      "Expected: 4\n",
      "Predicted: None\n",
      "---\n",
      "Problem 10:\n",
      "Expected: 2220\n",
      "Predicted: None\n",
      "---\n",
      "Problem 11:\n",
      "Expected: \\frac{3}{56}\n",
      "Predicted: None\n",
      "---\n",
      "Problem 12:\n",
      "Expected: 284\n",
      "Predicted: 280\n",
      "---\n",
      "Problem 14:\n",
      "Expected: \\sqrt{51}\n",
      "Predicted: None\n",
      "---\n",
      "Problem 15:\n",
      "Expected: 6 - 5i\n",
      "Predicted: None\n",
      "---\n",
      "Problem 17:\n",
      "Expected: \\pi\n",
      "Predicted: \n",
      "---\n",
      "Problem 18:\n",
      "Expected: 28\n",
      "Predicted: None\n",
      "---\n",
      "Problem 20:\n",
      "Expected: 6+9i\n",
      "Predicted: 6 + 9i\n",
      "---\n",
      "Problem 21:\n",
      "Expected: 13535\n",
      "Predicted: None\n",
      "---\n",
      "Problem 22:\n",
      "Expected: 5\n",
      "Predicted: None\n",
      "---\n",
      "Problem 23:\n",
      "Expected: x=5\n",
      "Predicted: 5\n",
      "---\n",
      "Problem 24:\n",
      "Expected: 10\n",
      "Predicted: 10.32\\%\n",
      "---\n",
      "Problem 25:\n",
      "Expected: 1,-2\n",
      "Predicted: None\n",
      "---\n",
      "Problem 26:\n",
      "Expected: 144\n",
      "Predicted: None\n",
      "---\n",
      "Problem 28:\n",
      "Expected: -2 + 7i\n",
      "Predicted: -2 + 7i\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_cot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874062bb",
   "metadata": {
    "id": "874062bb",
    "papermill": {
     "duration": 0.020491,
     "end_time": "2025-05-22T20:51:02.790549",
     "exception": false,
     "start_time": "2025-05-22T20:51:02.770058",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Best-of-N\n",
    "\n",
    "The **Best-of-N** approach improves math problem-solving by generating *N* solutions and selecting the one with the highest average token log-likelihood. Each solution is crafted using a prompt that encourages step-by-step reasoning and includes a formatted answer. The final selected response is both reliable and well-presented.\n",
    "\n",
    "### Steps\n",
    "1. **Generate**: Produce *N* responses using a structured guiding prompt.\n",
    "2. **Evaluate**: Compute the average log-likelihood for each response based on token probabilities.\n",
    "3. **Select**: Identify and choose the response with the highest score.\n",
    "\n",
    "This method ensures a statistically robust and clearly formatted solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48341d35",
   "metadata": {
    "id": "48341d35",
    "papermill": {
     "duration": 0.015138,
     "end_time": "2025-05-22T20:51:02.821556",
     "exception": false,
     "start_time": "2025-05-22T20:51:02.806418",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Verification Methods in Best‑of‑N Evaluation\n",
    "\n",
    "When sampling multiple candidate solutions for each math problem, we need a reliable way to choose the single best answer. We support two complementary approaches:\n",
    "\n",
    "### Log‑Probability Scoring\n",
    "\n",
    "**Concept**  \n",
    "Each generated solution comes with token‑level log‑likelihoods. By averaging these values across all tokens in the response, we obtain a single score reflecting how “confident” the model is in that entire output.\n",
    "\n",
    "**Why Use It**  \n",
    "- **Self‑Contained & Fast**: Requires no external calls or additional models.  \n",
    "- **Cost‑Effective**: Purely internal computation, so it adds negligible expense.  \n",
    "\n",
    "**Limitations**  \n",
    "- A high likelihood does not always imply a correct or well‑reasoned solution, especially on complex math problems.\n",
    "\n",
    "---\n",
    "\n",
    "### LLM‑Based Verification\n",
    "\n",
    "**Concept**  \n",
    "Instead of trusting raw likelihoods, we hand all sampled responses off to a second, high‑quality language model (e.g. Gemini Mini). That model reads the original problem and the list of candidate boxed answers, then selects the one it judges to be correct.\n",
    "\n",
    "**Why Use It**  \n",
    "- **Deeper Reasoning**: A dedicated verifier can compare alternative answers and catch subtle mistakes.  \n",
    "- **Improved Robustness**: Mitigates cases where a flawed but high‑probability output would otherwise be chosen.\n",
    "\n",
    "**Trade‑Offs**  \n",
    "- **Slower**: Requires additional API calls and round‑trip latency.  \n",
    "- **External Cost**: Incurs usage fees on the verification model.\n",
    "\n",
    "---\n",
    "\n",
    "### Balancing Speed, Cost, and Accuracy\n",
    "\n",
    "By exposing a simple toggle between these two methods, you can:\n",
    "\n",
    "- **Optimize for Speed**: Use log‑prob scoring when you need rapid, low‑cost evaluation.  \n",
    "- **Optimize for Accuracy**: Use LLM‑based verification when correctness is paramount.  \n",
    "\n",
    "Experiment on your dataset to find the right trade‑off for your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bf2c7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T20:51:02.853321Z",
     "iopub.status.busy": "2025-05-22T20:51:02.852779Z",
     "iopub.status.idle": "2025-05-22T20:51:04.313712Z",
     "shell.execute_reply": "2025-05-22T20:51:04.312995Z"
    },
    "id": "07bf2c7a",
    "papermill": {
     "duration": 1.478681,
     "end_time": "2025-05-22T20:51:04.315225",
     "exception": false,
     "start_time": "2025-05-22T20:51:02.836544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# ⚠️ Replace this with your real Gemini API key\n",
    "# API from: https://aistudio.google.com/app/apikey\n",
    "api_key = \"AIzaSyBfQxdfkRjuJCvgP9HQR8vmINOVpJ-s14A\"\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# Use the cheapest Gemini model; swap to another if you like\n",
    "LLM_API_MODEL = \"gemini-2.0-flash\" #\"gemini-mini\"\n",
    "api_model = genai.GenerativeModel(\n",
    "    model_name=LLM_API_MODEL,\n",
    "    generation_config={\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_output_tokens\": 1024,\n",
    "    }\n",
    ")\n",
    "\n",
    "def get_api_response(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Send `prompt` to Gemini Mini and return its reply.\n",
    "    \"\"\"\n",
    "    response = api_model.generate_content(prompt)\n",
    "    return response.text\n",
    "# ───────────────────────────────────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1177f923",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T20:51:04.345836Z",
     "iopub.status.busy": "2025-05-22T20:51:04.345242Z",
     "iopub.status.idle": "2025-05-22T20:51:05.018488Z",
     "shell.execute_reply": "2025-05-22T20:51:05.017700Z"
    },
    "id": "1177f923",
    "outputId": "5626dc78-e20d-418f-e0c5-c32ce1fd429f",
    "papermill": {
     "duration": 0.689412,
     "end_time": "2025-05-22T20:51:05.019743",
     "exception": false,
     "start_time": "2025-05-22T20:51:04.330331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wa alaikum assalam. How can I help you today?\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_api_response('salam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adc7631",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T20:51:05.051832Z",
     "iopub.status.busy": "2025-05-22T20:51:05.051507Z",
     "iopub.status.idle": "2025-05-22T20:51:05.062981Z",
     "shell.execute_reply": "2025-05-22T20:51:05.062269Z"
    },
    "id": "6adc7631",
    "papermill": {
     "duration": 0.029244,
     "end_time": "2025-05-22T20:51:05.064457",
     "exception": false,
     "start_time": "2025-05-22T20:51:05.035213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import math\n",
    "import heapq\n",
    "import requests\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "VERIFY_SYSTEM_PROMPT = \"\"\"You are an expert math solver. Your task is to verify the correct answer to a problem based on several candidate solutions.\n",
    "\n",
    "Problem:\n",
    "{problem}\n",
    "\n",
    "Candidate final answers (in boxed format):\n",
    "{answers}\n",
    "\n",
    "Determine which boxed answer is correct based on the problem. Reply with only the correct final answer in boxed form, such as: \\\\boxed{{...}}.\n",
    "\n",
    "Do not include any explanation—only the final boxed answer.\"\"\"\n",
    "\n",
    "\n",
    "def verify_with_gpt(problem: str, outputs: List[str]) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Given the original problem and a list of candidate full-response texts,\n",
    "    asks a GPT API to pick the correct final answer (boxed).\n",
    "    \"\"\"\n",
    "    # TODO: Deduplicate `outputs` into a `unique_answers: List[str]` by extracting\n",
    "    #       each boxed answer via `extract_answer` (use \"<no_boxed_answer>\" if none).\n",
    "    unique_answers = []\n",
    "    seen_answers = set()\n",
    "    for output in outputs:\n",
    "        answer = extract_answer(output)\n",
    "        if answer is None:\n",
    "            answer = \"<no_boxed_answer>\"\n",
    "        if answer not in seen_answers:\n",
    "            seen_answers.add(answer)\n",
    "            unique_answers.append(answer)\n",
    "    if not unique_answers:\n",
    "        return None\n",
    "\n",
    "    # TODO: Build `options` as numbered lines of the form \"1. \\\\boxed{...}\" from `unique_answers`.\n",
    "    options = \"\\n\".join(f\"{i+1}. \\\\boxed{{{ans}}}\" for i, ans in enumerate(unique_answers))\n",
    "\n",
    "    # TODO: Compose `verify_prompt` with the problem and options.\n",
    "    verify_prompt = VERIFY_SYSTEM_PROMPT.format(problem=problem, answers=options)\n",
    "\n",
    "\n",
    "    # TODO: Call `get_api_response(verify_prompt)` and strip whitespace → `chosen`.\n",
    "    chosen = get_api_response(verify_prompt).strip()\n",
    "\n",
    "    # TODO: Return `extract_answer(chosen)` to normalize formatting.\n",
    "    normalize_answer = extract_answer(chosen)\n",
    "    return normalize_answer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = '''You are solving mathematics problems.\n",
    "\n",
    "Please think step by step.\n",
    "\n",
    "Important: Always end your solution with the final answer in this format:\n",
    "\n",
    "\\\\[\n",
    "\\\\boxed{your_answer_here}\n",
    "\\\\]\n",
    "\n",
    "The entire answer should be contained completely within the \\\\boxed{} command.'''\n",
    "\n",
    "\n",
    "\n",
    "def best_of_n_response(\n",
    "    problem: str,\n",
    "    N: int = 5,\n",
    "    use_logprob: bool = True,\n",
    "    model: str = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    port: int = 8000,\n",
    "    temp: float = 0.2\n",
    ") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Run N samples on your VLLM server, then:\n",
    "    - if use_logprob: pick the candidate with highest avg log-prob\n",
    "    - else: hand off all N outputs to GPT to choose the best boxed answer\n",
    "    \"\"\"\n",
    "    url = f\"http://localhost:{port}/v1/chat/completions\"\n",
    "    prompt = SYSTEM_PROMPT + \"\\n\" + problem\n",
    "\n",
    "    samples = []\n",
    "    for _ in range(N):\n",
    "        # TODO: Build the `payload` dict with model, messages, max_tokens, temperature, and logprobs.\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"max_tokens\": 500,\n",
    "            \"temperature\": temp,\n",
    "            \"logprobs\": True,\n",
    "            \"top_logprobs\": 5\n",
    "        }\n",
    "\n",
    "        # TODO: POST to `requests.post(url, json=payload)` and parse `.json()` → `resp`.\n",
    "        resp = requests.post(url, json=payload).json()\n",
    "\n",
    "        # TODO: Extract `text` from `resp['choices'][0]['message']['content']`.\n",
    "        text = resp['choices'][0]['message']['content'].strip()\n",
    "\n",
    "        # TODO: Compute `avg_lp` by collecting all `choice['logprobs']['content'][*]['logprob']` values.\n",
    "        logprobs = [\n",
    "            token['logprob']\n",
    "            for token in resp['choices'][0]['logprobs']['content']\n",
    "        ]\n",
    "        avg_lp = sum(logprobs)/len(logprobs) if logprobs else 0\n",
    "\n",
    "        # TODO: Append `{\"text\": text, \"avg_lp\": avg_lp}` to `samples`.\n",
    "        samples.append({\n",
    "            \"text\": text,\n",
    "            \"avg_lp\": avg_lp\n",
    "        })\n",
    "\n",
    "    if use_logprob:\n",
    "        # TODO: Select the `sample` with the highest `avg_lp`.\n",
    "        best = max(samples, key=lambda x: x['avg_lp'] if x['avg_lp'] is not None else float('-inf'))\n",
    "\n",
    "        # TODO: Return `extract_answer(best[\"text\"])`.\n",
    "        return extract_answer(best[\"text\"])\n",
    "\n",
    "    else:\n",
    "        # TODO: Gather `outs = [s[\"text\"] for s in samples]`.\n",
    "        outs = [s[\"text\"] for s in samples]\n",
    "\n",
    "        # TODO: Return `verify_with_gpt(problem, outs)`.\n",
    "        return verify_with_gpt(problem, outs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9631ece4",
   "metadata": {
    "id": "9631ece4",
    "papermill": {
     "duration": 0.014447,
     "end_time": "2025-05-22T20:51:05.094320",
     "exception": false,
     "start_time": "2025-05-22T20:51:05.079873",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluate best of n\n",
    "\n",
    "* modify response generation part to evalute this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35849029",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T20:51:05.125623Z",
     "iopub.status.busy": "2025-05-22T20:51:05.124878Z",
     "iopub.status.idle": "2025-05-22T20:51:05.131956Z",
     "shell.execute_reply": "2025-05-22T20:51:05.131377Z"
    },
    "id": "35849029",
    "papermill": {
     "duration": 0.024418,
     "end_time": "2025-05-22T20:51:05.133139",
     "exception": false,
     "start_time": "2025-05-22T20:51:05.108721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_SAMPLE_TEST = 30\n",
    "\n",
    "def evaluate_best_of_n(use_logprob: bool = True, N: int = 3):\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    results_file = (\n",
    "        \"evaluation_results_math500_deepseek_best_of_n_logprob.json\"\n",
    "        if use_logprob else\n",
    "        \"evaluation_results_math500_deepseek_best_of_n_gpt.json\"\n",
    "    )\n",
    "\n",
    "    dataset = load_math500_dataset()\n",
    "    existing = load_existing_results(results_file)\n",
    "    seen = {r['index'] for r in existing}\n",
    "    correct = 0\n",
    "\n",
    "    for idx, item in enumerate(tqdm(dataset, desc=\"Evaluating problems\")):\n",
    "        if idx in seen or idx >= MAX_SAMPLE_TEST:\n",
    "            continue\n",
    "\n",
    "        prob = item['problem']\n",
    "        true_ans = extract_answer(item['solution'])\n",
    "        pred_ans = best_of_n_response(prob, N=N, use_logprob=use_logprob, temp=0.12)\n",
    "        is_corr = compare_answers(true_ans, pred_ans)\n",
    "\n",
    "        save_result(results_file, {\n",
    "            \"index\": idx,\n",
    "            \"problem\": prob,\n",
    "            \"correct_answer\": true_ans,\n",
    "            \"predicted_answer\": pred_ans,\n",
    "            \"is_correct\": is_corr\n",
    "        })\n",
    "        if is_corr:\n",
    "            correct += 1\n",
    "        print(f\"corrects: {correct} / {idx+1}\")\n",
    "\n",
    "    analyze_results(load_existing_results(results_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e833c2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T20:51:05.164387Z",
     "iopub.status.busy": "2025-05-22T20:51:05.163868Z",
     "iopub.status.idle": "2025-05-22T20:51:05.167383Z",
     "shell.execute_reply": "2025-05-22T20:51:05.166802Z"
    },
    "id": "1e833c2a",
    "papermill": {
     "duration": 0.020552,
     "end_time": "2025-05-22T20:51:05.168683",
     "exception": false,
     "start_time": "2025-05-22T20:51:05.148131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clear results history\n",
    "# !rm /kaggle/working/evaluation_results_math500_deepseek_best_of_n_logprob.json\n",
    "# !rm /kaggle/working/evaluation_results_math500_deepseek_best_of_n_gpt.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543d0c68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T20:51:05.201606Z",
     "iopub.status.busy": "2025-05-22T20:51:05.200749Z",
     "iopub.status.idle": "2025-05-22T21:04:03.642159Z",
     "shell.execute_reply": "2025-05-22T21:04:03.641067Z"
    },
    "id": "543d0c68",
    "outputId": "2a35e804-4968-40c7-b529-a446c0fa1eca",
    "papermill": {
     "duration": 778.458104,
     "end_time": "2025-05-22T21:04:03.643342",
     "exception": false,
     "start_time": "2025-05-22T20:51:05.185238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   0%|          | 1/500 [00:25<3:28:21, 25.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 1 / 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   0%|          | 2/500 [00:52<3:39:03, 26.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 1 / 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   1%|          | 3/500 [01:19<3:41:40, 26.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 1 / 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   1%|          | 4/500 [01:40<3:23:31, 24.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 2 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   1%|          | 5/500 [02:08<3:31:45, 25.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 2 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   1%|          | 6/500 [02:31<3:24:45, 24.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 3 / 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   1%|▏         | 7/500 [02:58<3:30:34, 25.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 3 / 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   2%|▏         | 8/500 [03:26<3:34:57, 26.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 3 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   2%|▏         | 9/500 [03:50<3:28:20, 25.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 4 / 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   2%|▏         | 10/500 [04:17<3:32:45, 26.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 4 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   2%|▏         | 11/500 [04:44<3:35:18, 26.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 4 / 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   2%|▏         | 12/500 [05:12<3:36:51, 26.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 4 / 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   3%|▎         | 13/500 [05:39<3:37:10, 26.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 4 / 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   3%|▎         | 14/500 [05:57<3:17:35, 24.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 5 / 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   3%|▎         | 15/500 [06:25<3:24:17, 25.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 5 / 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   3%|▎         | 16/500 [06:52<3:29:03, 25.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 5 / 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   3%|▎         | 17/500 [07:19<3:30:12, 26.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 6 / 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   4%|▎         | 18/500 [07:46<3:33:10, 26.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 6 / 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   4%|▍         | 19/500 [08:14<3:34:50, 26.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 6 / 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   4%|▍         | 20/500 [08:41<3:35:30, 26.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 6 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   4%|▍         | 21/500 [08:57<3:09:29, 23.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 6 / 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   4%|▍         | 22/500 [09:24<3:17:16, 24.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 6 / 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   5%|▍         | 23/500 [09:52<3:23:27, 25.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 6 / 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   5%|▍         | 24/500 [10:19<3:26:45, 26.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 6 / 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   5%|▌         | 25/500 [10:46<3:29:08, 26.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 6 / 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   5%|▌         | 26/500 [11:14<3:30:38, 26.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 6 / 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   5%|▌         | 27/500 [11:41<3:31:28, 26.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 6 / 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   6%|▌         | 28/500 [12:08<3:32:03, 26.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 6 / 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   6%|▌         | 29/500 [12:35<3:31:58, 27.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 6 / 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems: 100%|██████████| 500/500 [12:57<00:00,  1.55s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 7 / 30\n",
      "\n",
      "=== Results Summary ===\n",
      "Total problems: 30\n",
      "Correct answers: 7\n",
      "Accuracy: 23.33%\n",
      "\n",
      "=== Incorrect Problems ===\n",
      "Problem 1:\n",
      "Expected: p - q\n",
      "Predicted: None\n",
      "---\n",
      "Problem 2:\n",
      "Expected: \\frac{14}{3}\n",
      "Predicted: None\n",
      "---\n",
      "Problem 4:\n",
      "Expected: \\text{Evelyn}\n",
      "Predicted: None\n",
      "---\n",
      "Problem 6:\n",
      "Expected: 27\n",
      "Predicted: None\n",
      "---\n",
      "Problem 7:\n",
      "Expected: 90^\\circ\n",
      "Predicted: None\n",
      "---\n",
      "Problem 9:\n",
      "Expected: 4\n",
      "Predicted: None\n",
      "---\n",
      "Problem 10:\n",
      "Expected: 2220\n",
      "Predicted: None\n",
      "---\n",
      "Problem 11:\n",
      "Expected: \\frac{3}{56}\n",
      "Predicted: None\n",
      "---\n",
      "Problem 12:\n",
      "Expected: 284\n",
      "Predicted: None\n",
      "---\n",
      "Problem 14:\n",
      "Expected: \\sqrt{51}\n",
      "Predicted: None\n",
      "---\n",
      "Problem 15:\n",
      "Expected: 6 - 5i\n",
      "Predicted: None\n",
      "---\n",
      "Problem 17:\n",
      "Expected: \\pi\n",
      "Predicted: None\n",
      "---\n",
      "Problem 18:\n",
      "Expected: 28\n",
      "Predicted: None\n",
      "---\n",
      "Problem 19:\n",
      "Expected: 3\n",
      "Predicted: None\n",
      "---\n",
      "Problem 20:\n",
      "Expected: 6+9i\n",
      "Predicted: 6 + 9i\n",
      "---\n",
      "Problem 21:\n",
      "Expected: 13535\n",
      "Predicted: None\n",
      "---\n",
      "Problem 22:\n",
      "Expected: 5\n",
      "Predicted: None\n",
      "---\n",
      "Problem 23:\n",
      "Expected: x=5\n",
      "Predicted: None\n",
      "---\n",
      "Problem 24:\n",
      "Expected: 10\n",
      "Predicted: None\n",
      "---\n",
      "Problem 25:\n",
      "Expected: 1,-2\n",
      "Predicted: None\n",
      "---\n",
      "Problem 26:\n",
      "Expected: 144\n",
      "Predicted: None\n",
      "---\n",
      "Problem 27:\n",
      "Expected: 78\n",
      "Predicted: None\n",
      "---\n",
      "Problem 28:\n",
      "Expected: -2 + 7i\n",
      "Predicted: None\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_best_of_n(use_logprob=True, N=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baea645",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T21:04:03.680066Z",
     "iopub.status.busy": "2025-05-22T21:04:03.679428Z",
     "iopub.status.idle": "2025-05-22T21:17:18.338022Z",
     "shell.execute_reply": "2025-05-22T21:17:18.337079Z"
    },
    "id": "0baea645",
    "outputId": "e87ea8e5-1b4e-40ce-d947-ec6142786be9",
    "papermill": {
     "duration": 794.678305,
     "end_time": "2025-05-22T21:17:18.339504",
     "exception": false,
     "start_time": "2025-05-22T21:04:03.661199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   0%|          | 1/500 [00:24<3:27:12, 24.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 1 / 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   0%|          | 2/500 [00:52<3:40:18, 26.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 1 / 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   1%|          | 3/500 [01:20<3:43:57, 27.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 1 / 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   1%|          | 4/500 [01:42<3:27:06, 25.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 2 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   1%|          | 5/500 [02:10<3:35:27, 26.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 3 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   1%|          | 6/500 [02:29<3:15:58, 23.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 4 / 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   1%|▏         | 7/500 [02:59<3:30:45, 25.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 5 / 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   2%|▏         | 8/500 [03:26<3:36:12, 26.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 6 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   2%|▏         | 9/500 [03:53<3:35:04, 26.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 7 / 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   2%|▏         | 10/500 [04:20<3:38:16, 26.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 7 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   2%|▏         | 11/500 [04:48<3:39:55, 26.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 8 / 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   2%|▏         | 12/500 [05:15<3:40:53, 27.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 8 / 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   3%|▎         | 13/500 [05:42<3:39:47, 27.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 8 / 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   3%|▎         | 14/500 [06:02<3:20:33, 24.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 9 / 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   3%|▎         | 15/500 [06:29<3:27:26, 25.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 10 / 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   3%|▎         | 16/500 [06:58<3:33:04, 26.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 10 / 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   3%|▎         | 17/500 [07:23<3:31:24, 26.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 11 / 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   4%|▎         | 18/500 [07:51<3:34:13, 26.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 12 / 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   4%|▍         | 19/500 [08:19<3:36:23, 26.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 12 / 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   4%|▍         | 20/500 [08:46<3:37:25, 27.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 13 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   4%|▍         | 21/500 [09:02<3:09:02, 23.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 13 / 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   4%|▍         | 22/500 [09:34<3:27:57, 26.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 14 / 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   5%|▍         | 23/500 [10:02<3:31:36, 26.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 14 / 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   5%|▍         | 24/500 [10:29<3:33:22, 26.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 14 / 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   5%|▌         | 25/500 [11:00<3:41:42, 28.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 15 / 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   5%|▌         | 26/500 [11:27<3:40:21, 27.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 15 / 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   5%|▌         | 27/500 [11:55<3:39:03, 27.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 15 / 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   6%|▌         | 28/500 [12:22<3:38:04, 27.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 16 / 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   6%|▌         | 29/500 [12:50<3:36:52, 27.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 16 / 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems: 100%|██████████| 500/500 [13:14<00:00,  1.59s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects: 17 / 30\n",
      "\n",
      "=== Results Summary ===\n",
      "Total problems: 30\n",
      "Correct answers: 17\n",
      "Accuracy: 56.67%\n",
      "\n",
      "=== Incorrect Problems ===\n",
      "Problem 1:\n",
      "Expected: p - q\n",
      "Predicted: 2q - p\n",
      "---\n",
      "Problem 2:\n",
      "Expected: \\frac{14}{3}\n",
      "Predicted: \\frac{1}{3}\n",
      "---\n",
      "Problem 9:\n",
      "Expected: 4\n",
      "Predicted: 11\n",
      "---\n",
      "Problem 11:\n",
      "Expected: \\frac{3}{56}\n",
      "Predicted: \\frac{2}{15}\n",
      "---\n",
      "Problem 12:\n",
      "Expected: 284\n",
      "Predicted: 286\n",
      "---\n",
      "Problem 15:\n",
      "Expected: 6 - 5i\n",
      "Predicted: 3 - \\sqrt{2} - (2 + \\sqrt{2})i\n",
      "---\n",
      "Problem 18:\n",
      "Expected: 28\n",
      "Predicted: 62\n",
      "---\n",
      "Problem 20:\n",
      "Expected: 6+9i\n",
      "Predicted: 6 + 9i\n",
      "---\n",
      "Problem 22:\n",
      "Expected: 5\n",
      "Predicted: 4\n",
      "---\n",
      "Problem 23:\n",
      "Expected: x=5\n",
      "Predicted: 5\n",
      "---\n",
      "Problem 25:\n",
      "Expected: 1,-2\n",
      "Predicted: -1, 0, 1, 2, 3\n",
      "---\n",
      "Problem 26:\n",
      "Expected: 144\n",
      "Predicted: 48\n",
      "---\n",
      "Problem 28:\n",
      "Expected: -2 + 7i\n",
      "Predicted: -2 + 7i\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_best_of_n(use_logprob=False,  N=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec001742",
   "metadata": {
    "id": "ec001742",
    "papermill": {
     "duration": 0.01987,
     "end_time": "2025-05-22T21:17:18.379950",
     "exception": false,
     "start_time": "2025-05-22T21:17:18.360080",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Beam Search\n",
    "\n",
    "This cell implements a beam search strategy for generating candidate reasoning chains. The method generates multiple continuations at each reasoning step, scoring each candidate based on its average token log-likelihood. By retaining and expanding only the top candidates, the approach efficiently searches for the most promising chain-of-thought that leads to the final answer in the required format.\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "- **Model Invocation & Token Scoring:**  \n",
    "  The `call_qwen_model_raw` function sends requests to a local Qwen model endpoint using step-specific prompts. It returns generated text together with the average token log-probability, which is used as a quality metric.\n",
    "\n",
    "- **Candidate Representation:**  \n",
    "  The `BeamCandidate` class encapsulates a reasoning chain. It stores the generated text (sequence), cumulative log-probability, per-step scores, token count, and a finished flag (indicating if the candidate contains the final answer).\n",
    "\n",
    "- **Step-wise Reasoning Generation:**  \n",
    "  The `generate_reasoning_steps` function creates multiple candidate continuations for each reasoning step. Different prompts guide the generation for understanding the problem, planning a strategy, and producing the final answer (which is always enclosed in a `\\boxed{}` block).\n",
    "\n",
    "- **Beam Search Process:**  \n",
    "  The `beam_search` function expands candidate chains over several steps. At each step, candidates are updated by appending the new reasoning text and averaging the log-probabilities from all tokens(you can use num_token now). Only the top candidates (based on cumulative score) are retained for further expansion.\n",
    "\n",
    "- **Final Answer Extraction:**  \n",
    "  The `run_qwen_beam_search` function initializes the prompt with the problem statement, runs the beam search, and extracts the final answer from the best candidate if it is complete.\n",
    "\n",
    "This structured approach ensures efficient exploration of possible reasoning paths while focusing on the most promising ones to arrive at the final answer in the expected format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40f6293",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T21:17:18.423064Z",
     "iopub.status.busy": "2025-05-22T21:17:18.422774Z",
     "iopub.status.idle": "2025-05-22T21:17:18.439902Z",
     "shell.execute_reply": "2025-05-22T21:17:18.439249Z"
    },
    "id": "b40f6293",
    "papermill": {
     "duration": 0.04091,
     "end_time": "2025-05-22T21:17:18.441282",
     "exception": false,
     "start_time": "2025-05-22T21:17:18.400372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from typing import Optional, List\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def score_with_gpt(problem: str, reasoning_step: str) -> float:\n",
    "    \"\"\"\n",
    "    Ask a high‑quality LLM (via get_api_response) to rate the given\n",
    "    reasoning step on a 0–1 scale. Returns the numeric score.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You are a rigorous math reasoning evaluator.\\n\\n\"\n",
    "        f\"Problem:\\n{problem}\\n\\n\"\n",
    "        \"Candidate reasoning step:\\n\"\n",
    "        f\"\\\"\\\"\\\"\\n{reasoning_step}\\n\\\"\\\"\\\"\\n\\n\"\n",
    "        \"On a scale from 0 (completely incorrect) to 1 (perfectly correct), \"\n",
    "        \"rate how valid and useful this step is toward solving the problem. \"\n",
    "        \"Reply with only a number between 0 and 1.\"\n",
    "    )\n",
    "    # TODO: Call get_api_response(prompt), strip the result\n",
    "    response = get_api_response(prompt).strip()\n",
    "\n",
    "    # TODO: Parse float(resp), fallback to 0.0 on ValueError, and return it\n",
    "    try:\n",
    "        score = float(response)\n",
    "        return max(0.0, min(1.0, score))\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def call_qwen_model_raw(prompt: str, step_num: int, temperature: float = 0.3):\n",
    "    \"\"\"\n",
    "    Sends a request to the local Qwen endpoint and returns the generated text\n",
    "    along with the average token log-probability and token count.\n",
    "    \"\"\"\n",
    "    max_tokens = {1: 500, 2: 800, 3: 1700}.get(step_num, 500)\n",
    "    url = \"http://localhost:8000/v1/chat/completions\"\n",
    "    payload = {\n",
    "        \"model\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"logprobs\": True,\n",
    "    }\n",
    "    # TODO: resp = requests.post(url, json=payload).json()\n",
    "    resp = requests.post(url, json=payload).json()\n",
    "\n",
    "    # TODO: Extract text = resp['choices'][0]['message']['content'].strip()\n",
    "    text = resp['choices'][0]['message']['content'].strip()\n",
    "\n",
    "    # TODO: Gather token_logprobs from resp['choices'][*]['logprobs']['content'][*]['logprob']\n",
    "    token_logprobs = [choice['logprob'] for choice in resp['choices'][0]['logprobs']['content']]\n",
    "\n",
    "    # TODO: Compute avg_logprob and num_token, then return (text, avg_logprob, num_token)\n",
    "    num_token = len(token_logprobs)\n",
    "    avg_logprob = sum(token_logprobs) / num_token if num_token > 0 else 0.0\n",
    "    return text, avg_logprob, num_token\n",
    "\n",
    "\n",
    "class BeamCandidate:\n",
    "    def __init__(\n",
    "        self,\n",
    "        sequence: str,\n",
    "        cumulative_log_prob: float,\n",
    "        step_scores: List[float],\n",
    "        finished: bool,\n",
    "        num_token: int\n",
    "    ):\n",
    "        self.sequence = sequence\n",
    "        self.cumulative_log_prob = cumulative_log_prob\n",
    "        self.step_scores = step_scores\n",
    "        self.finished = finished\n",
    "        self.num_token = num_token\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"BeamCandidate(score={self.cumulative_log_prob:.3f}, \"\n",
    "            f\"finished={self.finished}, sequence=[...])\"\n",
    "        )\n",
    "\n",
    "\n",
    "def generate_reasoning_steps(\n",
    "    context: str,\n",
    "    step_num: int,\n",
    "    top_k: int,\n",
    "    use_logprob: bool = True,\n",
    "    problem: Optional[str] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate top_k candidate continuations for the current reasoning step.\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    for i in range(top_k):\n",
    "        # TODO: Build `suffix` based on step_num (1=understand, 2=plan, 3=solve)\n",
    "        suffix = {\n",
    "            1: \"Let's understand the problem.\", # understand\n",
    "            2: \"Let's plan how to solve it.\", # plan\n",
    "            3: \"Let's solve it and put the final answer within \\\\boxed{}.\" # solve\n",
    "        }.get(step_num, \"\")\n",
    "\n",
    "        # TODO: prompt = context + suffix\n",
    "        prompt = context + \"\\n\" + suffix\n",
    "\n",
    "        # TODO: output, avg_token_prob, num_token = call_qwen_model_raw(prompt, step_num)\n",
    "        output, avg_token_prob, num_token = call_qwen_model_raw(prompt, step_num)\n",
    "\n",
    "        # TODO: if use_logprob: score = avg_token_prob else: assert problem, score = score_with_gpt(problem, output)\n",
    "        if use_logprob:\n",
    "            score = avg_token_prob\n",
    "        else:\n",
    "            assert problem is not None, \"Problem must be provided for GPT scoring\"\n",
    "            score = score_with_gpt(problem, output)\n",
    "\n",
    "        # TODO: finished = \"\\\\boxed{\" in output\n",
    "        finished = \"\\\\boxed{\" in output\n",
    "\n",
    "        # TODO: candidates.append((output.strip(), score, num_token, finished))\n",
    "        candidates.append((output.strip(), score, num_token, finished))\n",
    "\n",
    "    return candidates\n",
    "\n",
    "\n",
    "def beam_search(\n",
    "    init_problem_prompt: str,\n",
    "    beam_width: int = 3,\n",
    "    max_steps: int = 3,\n",
    "    top_k: int = 2,\n",
    "    use_logprob: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Beam search over reasoning steps. If use_logprob=False, uses GPT verifier\n",
    "    to score each node instead of token log-prob.\n",
    "    \"\"\"\n",
    "    # TODO: Extract `problem` from init_problem_prompt\n",
    "    problem = init_problem_prompt.split('\\n')[-1]\n",
    "\n",
    "    # TODO: initial = BeamCandidate(sequence=init_problem_prompt, cumulative_log_prob=0.0, step_scores=[], finished=False, num_token=0)\n",
    "    initial = BeamCandidate(sequence=init_problem_prompt, cumulative_log_prob=0.0, step_scores=[], finished=False, num_token=0)\n",
    "\n",
    "    # TODO: beams = [initial]\n",
    "    beams = [initial]\n",
    "\n",
    "    for step in range(1, max_steps + 1):\n",
    "        new_beams = []\n",
    "        # TODO: For each cand in beams:\n",
    "        for cand in beams:\n",
    "            if cand.finished:\n",
    "                new_beams.append(cand)\n",
    "                continue\n",
    "\n",
    "            step_cands = generate_reasoning_steps(cand.sequence, step, top_k, use_logprob, problem)\n",
    "\n",
    "            for text, score, n_tok, finished in step_cands:\n",
    "                seq = cand.sequence + \"\\n\" + text\n",
    "                total_tokens = cand.num_token + n_tok\n",
    "                cum = ((cand.cumulative_log_prob * cand.num_token) + score * n_tok) / total_tokens\n",
    "                new_beams.append(BeamCandidate(seq, cum, cand.step_scores + [score], finished, total_tokens))\n",
    "\n",
    "        # Sort new_beams by cumulative_log_prob desc and slice to beam_width\n",
    "        new_beams.sort(key=lambda x: x.cumulative_log_prob, reverse=True)\n",
    "\n",
    "        # beams = new_beams\n",
    "        beams = new_beams[:beam_width]\n",
    "\n",
    "        # If all beams finished: break\n",
    "        if all(b.finished for b in beams):\n",
    "            break\n",
    "\n",
    "    finished = [b for b in beams if b.finished]\n",
    "\n",
    "    # TODO: best = max(finished, key=...) if finished else beams[0]\n",
    "    best = max(finished, key=lambda b: b.cumulative_log_prob) if finished else beams[0]\n",
    "\n",
    "    # TODO: return best\n",
    "    return best\n",
    "\n",
    "\n",
    "def run_qwen_beam_search(\n",
    "    problem: str,\n",
    "    beam_width: int,\n",
    "    max_steps: int,\n",
    "    top_k: int,\n",
    "    log_level,\n",
    "    use_logprob: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs beam search and extracts the final boxed answer.\n",
    "    \"\"\"\n",
    "    prompt = f\"Consider this problem:\\n{problem}\"\n",
    "\n",
    "    best = beam_search(prompt, beam_width, max_steps, top_k, use_logprob)\n",
    "    if best.finished:\n",
    "        ans = extract_answer(best.sequence)\n",
    "        print(f\"\\nExtracted Final Answer: {ans}\")\n",
    "        return ans\n",
    "    else:\n",
    "        print(\"No final answer found.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e25ce35",
   "metadata": {
    "id": "6e25ce35",
    "papermill": {
     "duration": 0.020424,
     "end_time": "2025-05-22T21:17:18.482948",
     "exception": false,
     "start_time": "2025-05-22T21:17:18.462524",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluate beam search\n",
    "* modify response generation part to evalute this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cab3165",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T21:17:18.523958Z",
     "iopub.status.busy": "2025-05-22T21:17:18.523674Z",
     "iopub.status.idle": "2025-05-22T21:17:18.530766Z",
     "shell.execute_reply": "2025-05-22T21:17:18.530063Z"
    },
    "id": "4cab3165",
    "papermill": {
     "duration": 0.029155,
     "end_time": "2025-05-22T21:17:18.532087",
     "exception": false,
     "start_time": "2025-05-22T21:17:18.502932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_SAMPLE_TEST = 30\n",
    "\n",
    "\n",
    "def evaluate_beam_search(use_logprob: bool = True):\n",
    "    \"\"\"\n",
    "    Evaluate beam search on MATH‑500, toggling between log‑prob scoring\n",
    "    and GPT/Gemini–based verification for each reasoning node.\n",
    "    \"\"\"\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "    suffix = \"logprob\" if use_logprob else \"gpt\"\n",
    "    results_file = f\"evaluation_results_math500_deepseek_beam_search_{suffix}.json\"\n",
    "\n",
    "    dataset = load_math500_dataset()\n",
    "    existing_results = load_existing_results(results_file)\n",
    "    processed_indexes = {r['index'] for r in existing_results}\n",
    "\n",
    "    cnt = 0\n",
    "    for idx, item in enumerate(tqdm(dataset, desc=\"Evaluating problems\")):\n",
    "        if idx in processed_indexes or idx >= MAX_SAMPLE_TEST:\n",
    "            continue\n",
    "\n",
    "        problem_text   = item['problem']\n",
    "        correct_answer = extract_answer(item['solution'])\n",
    "\n",
    "        # Run beam search with the desired scoring method\n",
    "        response = run_qwen_beam_search(\n",
    "            problem     = problem_text,\n",
    "            beam_width  = 3,\n",
    "            max_steps   = 3,\n",
    "            top_k       = 2,\n",
    "            log_level   = 1,              # existing parameter\n",
    "            use_logprob = use_logprob     # True = token log‑prob, False = GPT verifier\n",
    "        )\n",
    "        predicted_answer = response\n",
    "\n",
    "        is_correct = compare_answers(correct_answer, predicted_answer)\n",
    "        save_result(results_file, {\n",
    "            \"index\": idx,\n",
    "            \"problem\": problem_text,\n",
    "            \"response\": response,\n",
    "            \"correct_answer\": correct_answer,\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"is_correct\": is_correct\n",
    "        })\n",
    "\n",
    "        if is_correct:\n",
    "            cnt += 1\n",
    "        print(f\"corrects: {cnt} / {idx+1}\")\n",
    "\n",
    "    final_results = load_existing_results(results_file)\n",
    "    analyze_results(final_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94aa018e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T21:17:18.575965Z",
     "iopub.status.busy": "2025-05-22T21:17:18.575295Z",
     "iopub.status.idle": "2025-05-22T22:56:33.089940Z",
     "shell.execute_reply": "2025-05-22T22:56:33.089172Z"
    },
    "id": "94aa018e",
    "outputId": "739ec144-b8e5-4403-feaf-f28e4b2f7eb9",
    "papermill": {
     "duration": 5954.537212,
     "end_time": "2025-05-22T22:56:33.091362",
     "exception": false,
     "start_time": "2025-05-22T21:17:18.554150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   0%|          | 1/500 [01:53<15:43:51, 113.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: \\left(3, \\frac{\\pi}{2}\\right)\n",
      "corrects: 1 / 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   0%|          | 2/500 [06:04<26:53:18, 194.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: p - q\n",
      "corrects: 2 / 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   1%|          | 3/500 [08:22<23:18:34, 168.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: \\dfrac{14}{3}\n",
      "corrects: 3 / 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   1%|          | 4/500 [10:46<21:54:45, 159.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: 9\n",
      "corrects: 4 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   1%|          | 5/500 [14:48<25:57:00, 188.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: Evelyn\n",
      "corrects: 5 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   1%|          | 6/500 [16:22<21:30:17, 156.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: 42\n",
      "corrects: 6 / 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   1%|▏         | 7/500 [20:28<25:25:28, 185.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: 27\n",
      "corrects: 7 / 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   2%|▏         | 8/500 [23:39<25:36:33, 187.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: 90\n",
      "corrects: 8 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   2%|▏         | 9/500 [25:30<22:19:13, 163.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: 3\\sqrt{13}\n",
      "corrects: 9 / 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   2%|▏         | 10/500 [29:48<26:14:43, 192.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No final answer found.\n",
      "corrects: 9 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   2%|▏         | 11/500 [33:57<28:30:21, 209.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: 2220\n",
      "corrects: 10 / 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   2%|▏         | 12/500 [38:17<30:32:34, 225.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No final answer found.\n",
      "corrects: 10 / 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   3%|▎         | 13/500 [42:33<31:42:45, 234.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: 284\n",
      "corrects: 11 / 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   3%|▎         | 14/500 [43:16<23:50:32, 176.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: 5\n",
      "corrects: 12 / 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   3%|▎         | 15/500 [46:05<23:28:23, 174.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: \\sqrt{51}\n",
      "corrects: 13 / 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   3%|▎         | 16/500 [50:24<26:53:25, 200.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: 6 - 5i\n",
      "corrects: 14 / 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   3%|▎         | 17/500 [53:16<25:41:00, 191.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: -50\n",
      "corrects: 15 / 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   4%|▎         | 18/500 [57:34<28:19:03, 211.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No final answer found.\n",
      "corrects: 15 / 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   4%|▍         | 19/500 [1:01:51<30:04:08, 225.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No final answer found.\n",
      "corrects: 15 / 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   4%|▍         | 20/500 [1:06:07<31:14:42, 234.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No final answer found.\n",
      "corrects: 15 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   4%|▍         | 21/500 [1:07:35<25:19:33, 190.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: 6 + 9i\n",
      "corrects: 15 / 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   4%|▍         | 22/500 [1:11:50<27:52:59, 210.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No final answer found.\n",
      "corrects: 15 / 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   5%|▍         | 23/500 [1:14:57<26:53:08, 202.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: 5\n",
      "corrects: 16 / 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   5%|▍         | 24/500 [1:18:44<27:48:34, 210.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: 5\n",
      "corrects: 16 / 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   5%|▌         | 25/500 [1:23:01<29:36:11, 224.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: 10\n",
      "corrects: 17 / 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   5%|▌         | 26/500 [1:27:20<30:53:09, 234.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: 1\n",
      "corrects: 17 / 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   5%|▌         | 27/500 [1:31:37<31:43:22, 241.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No final answer found.\n",
      "corrects: 17 / 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   6%|▌         | 28/500 [1:33:01<25:25:57, 193.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: \\$78\n",
      "corrects: 18 / 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   6%|▌         | 29/500 [1:36:13<25:19:41, 193.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: -2 + 7i\n",
      "corrects: 18 / 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems: 100%|██████████| 500/500 [1:39:13<00:00, 11.91s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: 225\n",
      "corrects: 19 / 30\n",
      "\n",
      "=== Results Summary ===\n",
      "Total problems: 30\n",
      "Correct answers: 19\n",
      "Accuracy: 63.33%\n",
      "\n",
      "=== Incorrect Problems ===\n",
      "Problem 9:\n",
      "Expected: 4\n",
      "Predicted: None\n",
      "---\n",
      "Problem 11:\n",
      "Expected: \\frac{3}{56}\n",
      "Predicted: None\n",
      "---\n",
      "Problem 17:\n",
      "Expected: \\pi\n",
      "Predicted: None\n",
      "---\n",
      "Problem 18:\n",
      "Expected: 28\n",
      "Predicted: None\n",
      "---\n",
      "Problem 19:\n",
      "Expected: 3\n",
      "Predicted: None\n",
      "---\n",
      "Problem 20:\n",
      "Expected: 6+9i\n",
      "Predicted: 6 + 9i\n",
      "---\n",
      "Problem 21:\n",
      "Expected: 13535\n",
      "Predicted: None\n",
      "---\n",
      "Problem 23:\n",
      "Expected: x=5\n",
      "Predicted: 5\n",
      "---\n",
      "Problem 25:\n",
      "Expected: 1,-2\n",
      "Predicted: 1\n",
      "---\n",
      "Problem 26:\n",
      "Expected: 144\n",
      "Predicted: None\n",
      "---\n",
      "Problem 28:\n",
      "Expected: -2 + 7i\n",
      "Predicted: -2 + 7i\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "evaluate_beam_search(use_logprob=True)   # pick by avg token log‑prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c89ce5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-22T22:56:33.136165Z",
     "iopub.status.busy": "2025-05-22T22:56:33.135578Z",
     "iopub.status.idle": "2025-05-23T00:39:01.509042Z",
     "shell.execute_reply": "2025-05-23T00:39:01.507904Z"
    },
    "id": "2c89ce5f",
    "outputId": "5117958f-5700-459c-c8ad-69cbe39d5ddd",
    "papermill": {
     "duration": 6148.397647,
     "end_time": "2025-05-23T00:39:01.510549",
     "exception": false,
     "start_time": "2025-05-22T22:56:33.112902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   0%|          | 1/500 [01:42<14:10:50, 102.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: (3, \\frac{\\pi}{2})\n",
      "corrects: 1 / 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   0%|          | 2/500 [05:42<25:24:43, 183.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: p - q\n",
      "corrects: 2 / 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   1%|          | 3/500 [07:58<22:18:56, 161.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: \\dfrac{14}{3}\n",
      "corrects: 3 / 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   1%|          | 4/500 [09:56<19:55:50, 144.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: 9\n",
      "corrects: 4 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   1%|          | 5/500 [14:17<25:37:05, 186.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: Evelyn\n",
      "corrects: 5 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   1%|          | 6/500 [15:30<20:16:36, 147.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: 42\n",
      "corrects: 6 / 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   1%|▏         | 7/500 [19:16<23:46:04, 173.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: 27\n",
      "corrects: 7 / 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   2%|▏         | 8/500 [23:22<26:51:20, 196.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: 90\n",
      "corrects: 8 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   2%|▏         | 9/500 [25:55<24:56:54, 182.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: 3\\sqrt{13}\n",
      "corrects: 9 / 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   2%|▏         | 10/500 [30:23<28:27:49, 209.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No final answer found.\n",
      "corrects: 9 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   2%|▏         | 11/500 [34:15<29:21:48, 216.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: 2220\n",
      "corrects: 10 / 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   2%|▏         | 12/500 [38:43<31:27:00, 232.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: \\dfrac{1}{9}\n",
      "corrects: 10 / 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   3%|▎         | 13/500 [42:52<32:03:30, 236.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: 284\n",
      "corrects: 11 / 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   3%|▎         | 14/500 [43:24<23:39:12, 175.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: 5\n",
      "corrects: 12 / 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   3%|▎         | 15/500 [46:25<23:49:56, 176.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: \\sqrt{51}\n",
      "corrects: 13 / 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   3%|▎         | 16/500 [50:50<27:22:16, 203.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: 6 - 5i\n",
      "corrects: 14 / 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   3%|▎         | 17/500 [55:10<29:34:33, 220.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: -50\n",
      "corrects: 15 / 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   4%|▎         | 18/500 [59:37<31:22:49, 234.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No final answer found.\n",
      "corrects: 15 / 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   4%|▍         | 19/500 [1:04:04<32:38:36, 244.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No final answer found.\n",
      "corrects: 15 / 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   4%|▍         | 20/500 [1:08:29<33:23:16, 250.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No final answer found.\n",
      "corrects: 15 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   4%|▍         | 21/500 [1:10:30<28:09:05, 211.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: 6 + 9i\n",
      "corrects: 15 / 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   4%|▍         | 22/500 [1:14:57<30:18:50, 228.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No final answer found.\n",
      "corrects: 15 / 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   5%|▍         | 23/500 [1:18:21<29:17:16, 221.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: 5\n",
      "corrects: 16 / 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   5%|▍         | 24/500 [1:21:38<28:14:31, 213.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: 5\n",
      "corrects: 16 / 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   5%|▌         | 25/500 [1:25:37<29:11:29, 221.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: 10\n",
      "corrects: 17 / 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   5%|▌         | 26/500 [1:30:05<30:58:30, 235.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: 1\n",
      "corrects: 17 / 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   5%|▌         | 27/500 [1:34:32<32:09:28, 244.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No final answer found.\n",
      "corrects: 17 / 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   6%|▌         | 28/500 [1:36:25<26:54:32, 205.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: 78\n",
      "corrects: 18 / 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   6%|▌         | 29/500 [1:39:11<25:19:32, 193.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: -2 + 7i\n",
      "corrects: 18 / 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems: 100%|██████████| 500/500 [1:42:27<00:00, 12.30s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Final Answer: 225\n",
      "corrects: 19 / 30\n",
      "\n",
      "=== Results Summary ===\n",
      "Total problems: 30\n",
      "Correct answers: 19\n",
      "Accuracy: 63.33%\n",
      "\n",
      "=== Incorrect Problems ===\n",
      "Problem 9:\n",
      "Expected: 4\n",
      "Predicted: None\n",
      "---\n",
      "Problem 11:\n",
      "Expected: \\frac{3}{56}\n",
      "Predicted: \\dfrac{1}{9}\n",
      "---\n",
      "Problem 17:\n",
      "Expected: \\pi\n",
      "Predicted: None\n",
      "---\n",
      "Problem 18:\n",
      "Expected: 28\n",
      "Predicted: None\n",
      "---\n",
      "Problem 19:\n",
      "Expected: 3\n",
      "Predicted: None\n",
      "---\n",
      "Problem 20:\n",
      "Expected: 6+9i\n",
      "Predicted: 6 + 9i\n",
      "---\n",
      "Problem 21:\n",
      "Expected: 13535\n",
      "Predicted: None\n",
      "---\n",
      "Problem 23:\n",
      "Expected: x=5\n",
      "Predicted: 5\n",
      "---\n",
      "Problem 25:\n",
      "Expected: 1,-2\n",
      "Predicted: 1\n",
      "---\n",
      "Problem 26:\n",
      "Expected: 144\n",
      "Predicted: None\n",
      "---\n",
      "Problem 28:\n",
      "Expected: -2 + 7i\n",
      "Predicted: -2 + 7i\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_beam_search(use_logprob=False)  # pick by GPT/Gemini verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a23481",
   "metadata": {
    "id": "b6a23481",
    "papermill": {
     "duration": 0.026349,
     "end_time": "2025-05-23T00:39:01.565260",
     "exception": false,
     "start_time": "2025-05-23T00:39:01.538911",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Self-Refinement\n",
    "\n",
    "This cell implements a self-refinement approach to solving math problems. Initially, it generates a solution using a fixed system prompt that enforces a step-by-step reasoning process and a final answer format enclosed in `\\boxed{}`. Then, through iterative feedback, the model is asked to analyze its own output and refine it if necessary. This loop ensures that the final answer is both correct and clearly formatted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78493110",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T00:39:01.619973Z",
     "iopub.status.busy": "2025-05-23T00:39:01.619661Z",
     "iopub.status.idle": "2025-05-23T00:39:01.629523Z",
     "shell.execute_reply": "2025-05-23T00:39:01.628576Z"
    },
    "id": "78493110",
    "papermill": {
     "duration": 0.039269,
     "end_time": "2025-05-23T00:39:01.630907",
     "exception": false,
     "start_time": "2025-05-23T00:39:01.591638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "\n",
    "SYSTEM_PROMPT = '''You are solving mathematics problems.\n",
    "\n",
    "Please think step by step.\n",
    "\n",
    "Important: Always end your solution with the final answer in this format:\n",
    "\n",
    "\\\\[\n",
    "\\\\boxed{your_answer_here}\n",
    "\\\\]\n",
    "\n",
    "The entire answer should be contained completely within the \\\\boxed{} command.'''\n",
    "\n",
    "\n",
    "def generate_content(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Sends `prompt` to the local Qwen endpoint and returns the generated text.\n",
    "    \"\"\"\n",
    "    url = \"http://localhost:8000/v1/chat/completions\"\n",
    "    payload = {\n",
    "        \"model\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"max_tokens\": 1504,\n",
    "        \"temperature\": 0.3,\n",
    "    }\n",
    "    # TODO: Send HTTP POST to `url` with `payload`, parse the JSON response\n",
    "    resp = requests.post(url, json=payload).json()\n",
    "\n",
    "    # TODO: Extract `content` from `resp['choices'][0]['message']['content']` and strip whitespace\n",
    "    text = resp['choices'][0]['message']['content'].strip()\n",
    "\n",
    "    # TODO: Return the resulting string\n",
    "    return text\n",
    "\n",
    "\n",
    "FEEDBACK_SYSTEM_PROMPT = \"\"\"\n",
    "Given the problem and the current solution attempt, provide constructive feedback.\n",
    "\n",
    "[PROBLEM]\n",
    "{prompt}\n",
    "\n",
    "[CURRENT_SOLUTION]\n",
    "{current_output}\n",
    "\n",
    "Please provide:\n",
    "1. [FEEDBACK] Detailed feedback about any issues or improvements.\n",
    "2. [REFINEMENT_NEEDED] Answer \"yes\" if the solution needs refinement, otherwise \"no\".\n",
    "\n",
    "Use the exact format:\n",
    "[FEEDBACK] your feedback here\n",
    "[REFINEMENT_NEEDED] yes/no\n",
    "\"\"\"\n",
    "\n",
    "REFINE_SYSTEM_PROMPT = \"\"\"\n",
    "You previously solved the following problem:\n",
    "\n",
    "{prompt}\n",
    "\n",
    "Here was your attempt:\n",
    "{current_output}\n",
    "\n",
    "Here is the feedback you received:\n",
    "{feedback}\n",
    "\n",
    "Please revise your solution accordingly.\n",
    "\"\"\"\n",
    "\n",
    "def self_refine(problem: str, max_iter: int = 2) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Iteratively refines the model’s output on `problem` using feedback loops.\n",
    "    \"\"\"\n",
    "    # TODO: Build initial `prompt` by concatenating SYSTEM_PROMPT and `problem`\n",
    "    prompt = SYSTEM_PROMPT + \"\\n\" + problem\n",
    "\n",
    "    # TODO: Call `generate_content(prompt)` to get `current_output`\n",
    "    current_output = generate_content(prompt)\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        # TODO: Construct `feedback_prompt` that includes:\n",
    "        #         - the original `prompt`\n",
    "        #         - the `current_output`\n",
    "        #         - instructions to output [FEEDBACK] and [REFINEMENT_NEEDED]\n",
    "        feedback_prompt = FEEDBACK_SYSTEM_PROMPT.format(prompt=prompt, current_output=current_output)\n",
    "\n",
    "        # TODO: Call `generate_content(feedback_prompt)` → `feedback_response`\n",
    "        feedback_response = generate_content(feedback_prompt)\n",
    "\n",
    "        # TODO: Use `re.search` with pattern r\"\\[(?i:feedback)\\](.*?)\\[(?i:refinement_needed)\\](.*)\"\n",
    "        #       to extract `feedback` and `refinement_flag`\n",
    "        # TODO: Determine `refinement_needed` (default True, set False if flag is \"no\" or feedback contains stop phrases)\n",
    "        match = re.search(r\"\\[(?i:feedback)\\](.*?)\\[(?i:refinement_needed)\\](.*)\", feedback_response, re.DOTALL)\n",
    "        if match:\n",
    "            feedback = match.group(1).strip()\n",
    "            refinement_flag = match.group(2).strip().lower()\n",
    "            refinement_needed = refinement_flag not in {\"no\", \"false\"}\n",
    "        else:\n",
    "            feedback = \"\"\n",
    "            refinement_needed = True\n",
    "\n",
    "        # TODO: If `not refinement_needed`: break out of loop\n",
    "        if not refinement_needed:\n",
    "            break\n",
    "\n",
    "        # TODO: Build `refine_prompt` that includes:\n",
    "        #         - the original `prompt`\n",
    "        #         - the `current_output`\n",
    "        #         - the extracted `feedback`\n",
    "        refine_prompt = REFINE_SYSTEM_PROMPT.format(\n",
    "            prompt = prompt,\n",
    "            current_output = current_output,\n",
    "            feedback = feedback\n",
    "        )\n",
    "\n",
    "        # TODO: Call `generate_content(refine_prompt)` → `refined_output`\n",
    "        refined_output = generate_content(refine_prompt)\n",
    "\n",
    "        # TODO: If `refined_output.strip() == current_output.strip()`: break\n",
    "        if refined_output.strip() == current_output.strip():\n",
    "            break\n",
    "\n",
    "        # TODO: Otherwise, set `current_output = refined_output`\n",
    "        current_output = refined_output\n",
    "\n",
    "    # TODO: Use `extract_answer(current_output)` to get the final boxed answer\n",
    "    # TODO: Return that answer (or None if no boxed answer found)\n",
    "    final = extract_answer(current_output)\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f018035a",
   "metadata": {
    "id": "f018035a",
    "papermill": {
     "duration": 0.025578,
     "end_time": "2025-05-23T00:39:01.682863",
     "exception": false,
     "start_time": "2025-05-23T00:39:01.657285",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluate self refiner\n",
    "* modify response generation part to evalute this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfd8436",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T00:39:01.736790Z",
     "iopub.status.busy": "2025-05-23T00:39:01.735781Z",
     "iopub.status.idle": "2025-05-23T00:39:01.742873Z",
     "shell.execute_reply": "2025-05-23T00:39:01.742056Z"
    },
    "id": "cdfd8436",
    "papermill": {
     "duration": 0.035141,
     "end_time": "2025-05-23T00:39:01.744292",
     "exception": false,
     "start_time": "2025-05-23T00:39:01.709151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_SAMPLE_TEST = 30\n",
    "\n",
    "def evaluate_self_refiner():\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    results_file = \"evaluation_results_math500_deepseek_self_refiner.json\"\n",
    "    dataset = load_math500_dataset()\n",
    "    existing_results = load_existing_results(results_file)\n",
    "    processed_indexes = {result['index'] for result in existing_results}\n",
    "    cnt = 0\n",
    "    for idx, item in enumerate(tqdm(dataset, desc=\"Evaluating problems\")):\n",
    "        if idx in processed_indexes :\n",
    "            continue\n",
    "        if idx >= MAX_SAMPLE_TEST:\n",
    "          break\n",
    "        problem_text = item['problem']\n",
    "        correct_answer = extract_answer(item['solution'])\n",
    "        ##########################################################\n",
    "        response = self_refine(problem_text,3)\n",
    "        predicted_answer = response\n",
    "        ##########################################################\n",
    "        is_correct = compare_answers(correct_answer, predicted_answer)\n",
    "        result = {\n",
    "            \"index\": idx,\n",
    "            \"problem\": problem_text,\n",
    "            \"response\": response,\n",
    "            \"correct_answer\": correct_answer,\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"is_correct\": is_correct\n",
    "        }\n",
    "        save_result(results_file, result)\n",
    "        if is_correct:\n",
    "          cnt += 1\n",
    "        print(f\"corrects :  {cnt} idx: {idx}\")\n",
    "    final_results = load_existing_results(results_file)\n",
    "    analyze_results(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d16e2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T00:39:01.797773Z",
     "iopub.status.busy": "2025-05-23T00:39:01.797445Z",
     "iopub.status.idle": "2025-05-23T01:27:43.085222Z",
     "shell.execute_reply": "2025-05-23T01:27:43.084283Z"
    },
    "id": "69d16e2d",
    "outputId": "64033f38-3340-4729-be0e-70dd0d792705",
    "papermill": {
     "duration": 2921.346027,
     "end_time": "2025-05-23T01:27:43.116543",
     "exception": false,
     "start_time": "2025-05-23T00:39:01.770516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   0%|          | 1/500 [01:23<11:31:08, 83.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  1 idx: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   0%|          | 2/500 [02:31<10:19:59, 74.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  2 idx: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   1%|          | 3/500 [03:35<9:36:23, 69.58s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  3 idx: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   1%|          | 4/500 [04:06<7:28:42, 54.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  4 idx: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   1%|          | 5/500 [05:59<10:22:52, 75.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  5 idx: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   1%|          | 6/500 [06:12<7:27:22, 54.34s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  6 idx: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   1%|▏         | 7/500 [07:38<8:50:21, 64.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  7 idx: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   2%|▏         | 8/500 [09:26<10:44:07, 78.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  8 idx: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   2%|▏         | 9/500 [09:44<8:07:20, 59.55s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  9 idx: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   2%|▏         | 10/500 [12:53<13:31:28, 99.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  9 idx: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   2%|▏         | 11/500 [13:30<10:55:24, 80.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  10 idx: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   2%|▏         | 12/500 [14:22<9:44:29, 71.86s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  10 idx: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   3%|▎         | 13/500 [16:59<13:12:46, 97.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  10 idx: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   3%|▎         | 14/500 [17:14<9:46:51, 72.45s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  11 idx: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   3%|▎         | 15/500 [17:45<8:06:59, 60.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  12 idx: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   3%|▎         | 16/500 [20:52<13:12:01, 98.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  12 idx: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   3%|▎         | 17/500 [21:31<10:48:24, 80.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  12 idx: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   4%|▎         | 18/500 [23:13<11:38:53, 87.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  13 idx: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   4%|▍         | 19/500 [26:18<15:33:03, 116.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  13 idx: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   4%|▍         | 20/500 [29:25<18:19:48, 137.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  13 idx: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   4%|▍         | 21/500 [29:53<13:55:30, 104.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  13 idx: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   4%|▍         | 22/500 [32:59<17:09:34, 129.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  13 idx: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   5%|▍         | 23/500 [36:06<19:23:34, 146.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  13 idx: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   5%|▍         | 24/500 [38:29<19:14:35, 145.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  13 idx: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   5%|▌         | 25/500 [39:13<15:09:38, 114.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  14 idx: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   5%|▌         | 26/500 [42:20<18:00:07, 136.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  14 idx: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   5%|▌         | 27/500 [45:28<19:57:19, 151.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  14 idx: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   6%|▌         | 28/500 [46:33<16:30:25, 125.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  15 idx: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   6%|▌         | 29/500 [47:19<13:19:40, 101.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  15 idx: 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   6%|▌         | 30/500 [48:40<12:42:38, 97.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corrects :  16 idx: 29\n",
      "\n",
      "=== Results Summary ===\n",
      "Total problems: 30\n",
      "Correct answers: 16\n",
      "Accuracy: 53.33%\n",
      "\n",
      "=== Incorrect Problems ===\n",
      "Problem 9:\n",
      "Expected: 4\n",
      "Predicted: None\n",
      "---\n",
      "Problem 11:\n",
      "Expected: \\frac{3}{56}\n",
      "Predicted: None\n",
      "---\n",
      "Problem 12:\n",
      "Expected: 284\n",
      "Predicted: None\n",
      "---\n",
      "Problem 15:\n",
      "Expected: 6 - 5i\n",
      "Predicted: None\n",
      "---\n",
      "Problem 16:\n",
      "Expected: -50\n",
      "Predicted: None\n",
      "---\n",
      "Problem 18:\n",
      "Expected: 28\n",
      "Predicted: 56^{\\circ}\n",
      "---\n",
      "Problem 19:\n",
      "Expected: 3\n",
      "Predicted: None\n",
      "---\n",
      "Problem 20:\n",
      "Expected: 6+9i\n",
      "Predicted: 6 + 9i\n",
      "---\n",
      "Problem 21:\n",
      "Expected: 13535\n",
      "Predicted: None\n",
      "---\n",
      "Problem 22:\n",
      "Expected: 5\n",
      "Predicted: None\n",
      "---\n",
      "Problem 23:\n",
      "Expected: x=5\n",
      "Predicted: 5\n",
      "---\n",
      "Problem 25:\n",
      "Expected: 1,-2\n",
      "Predicted: None\n",
      "---\n",
      "Problem 26:\n",
      "Expected: 144\n",
      "Predicted: None\n",
      "---\n",
      "Problem 28:\n",
      "Expected: -2 + 7i\n",
      "Predicted: -2 + 7i\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_self_refiner()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46636fb5",
   "metadata": {
    "id": "46636fb5",
    "papermill": {
     "duration": 0.028189,
     "end_time": "2025-05-23T01:27:43.173525",
     "exception": false,
     "start_time": "2025-05-23T01:27:43.145336",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Part 2: \n",
    "Implementing A*, Monte Carlo Tree Search (MCTS), and Tree of Thoughts (ToT)\n",
    "\n",
    "Ok so now every thing is ready to start part 2, This part aims to explore three sophisticated search and reasoning algorithms—**A\\***, **Monte Carlo Tree Search (MCTS)**, and **Tree of Thoughts (ToT)**—to solve challenging mathematical problems, specifically using the MATH-500 dataset and an LLM (Language Model). Before diving into the implementation, we provide a comprehensive overview of each algorithm, highlighting their core mechanisms, practical considerations, and potential challenges.\n",
    "\n",
    "---\n",
    "\n",
    "## 🌟 1. A* Search Algorithm\n",
    "\n",
    "**A*** is an informed search algorithm designed for efficiently finding the shortest path or optimal solution in a search space using heuristics.\n",
    "\n",
    "### 🔹 Core Principles:\n",
    "- **Best-first search:** Expands nodes based on a cost function, \\(f(n)\\), prioritizing paths that seem closer to a goal.\n",
    "- **Heuristic evaluation:** Uses a heuristic function \\(h(n)\\) to estimate the cost from the current node to the goal.\n",
    "\n",
    "### 🔹 Components:\n",
    "- **Cost function \\(f(n) = g(n) + h(n)\\)**, where:\n",
    "  - \\(g(n)\\): Actual cost from start node to node \\(n\\).\n",
    "  - \\(h(n)\\): Estimated cost from node \\(n\\) to the goal (heuristic).\n",
    "\n",
    "### 🔹 Practical Considerations:\n",
    "- Heuristic function must be **efficient** and **accurate**.\n",
    "- A good heuristic drastically reduces computation time and search complexity.\n",
    "\n",
    "### ⚠️ Challenges in Implementation:\n",
    "- **Designing an effective heuristic:**\n",
    "  - Challenge to accurately estimate \"distance\" from partial reasoning steps to the solution.\n",
    "  - Requires leveraging language models to score plausibility.\n",
    "- **Computational efficiency:**\n",
    "  - Heuristic evaluation via LLM queries could be computationally costly if not managed carefully.\n",
    "- **Admissibility and consistency:**\n",
    "  - Ideally, heuristic must be admissible (never overestimates the true cost) to guarantee optimality.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎲 2. Monte Carlo Tree Search (MCTS)\n",
    "\n",
    "**MCTS** is a probabilistic algorithm widely used in decision-making scenarios, especially effective in complex problems with uncertain outcomes, such as mathematical reasoning guided by language models.\n",
    "\n",
    "### 🔹 Core Principles:\n",
    "MCTS explores decision trees using **randomized simulations** (rollouts) and statistical sampling.\n",
    "\n",
    "**Four main steps in MCTS**:\n",
    "1. **Selection**: Uses UCT (Upper Confidence Bound) to balance exploration and exploitation.\n",
    "2. **Expansion**: Adds new unexplored nodes to the tree.\n",
    "3. **Simulation**: Conducts rollouts from newly expanded nodes to estimate potential outcomes.\n",
    "4. **Backpropagation**: Updates statistical measures based on simulation results.\n",
    "\n",
    "### 🔹 Components:\n",
    "- **UCT formula** for selection:\n",
    "  $$\n",
    "  \\text{UCT} = \\frac{w_i}{n_i} + C\\sqrt{\\frac{\\ln N}{n_i}}\n",
    "  $$\n",
    "  - \\(w_i\\): Total rewards.\n",
    "  - \\(n_i\\): Visits to node \\(i\\).\n",
    "  - \\(N\\): Visits to parent node.\n",
    "  - \\(C\\): Exploration constant (usually \\(\\sqrt{2}\\)).\n",
    "\n",
    "- **Rollout (simulation)**:\n",
    "  - Typically involves letting the LLM complete reasoning steps to the end, evaluating correctness.\n",
    "\n",
    "### 🔹 Practical Considerations:\n",
    "- Effective rollout policies significantly impact accuracy and efficiency.\n",
    "- Balance exploration (testing new reasoning paths) and exploitation (refining known good solutions).\n",
    "\n",
    "### ⚠️ Challenges in Implementation:\n",
    "- **Computational overhead**:\n",
    "  - Running multiple LLM-based rollouts per node can be slow and computationally expensive.\n",
    "- **Optimal parameter tuning**:\n",
    "  - Choosing the exploration constant \\(C\\) and number of simulations impacts performance significantly.\n",
    "- **Quality of simulation outcomes**:\n",
    "  - Poor rollout outcomes (random or inaccurate completions) can misguide the search tree.\n",
    "\n",
    "---\n",
    "\n",
    "## 🌳 3. Tree of Thoughts (ToT)\n",
    "\n",
    "**Tree of Thoughts (ToT)** is specifically designed for structured reasoning tasks with language models, extending their capabilities through explicit evaluation and pruning of reasoning paths.\n",
    "\n",
    "### 🔹 Core Principles:\n",
    "- Generate multiple candidate \"thoughts\" (reasoning paths).\n",
    "- Evaluate each thought explicitly (often through LLM-based scoring or consistency checking).\n",
    "- Iteratively prune weaker reasoning paths, keeping the most promising solutions.\n",
    "\n",
    "### 🔹 Components:\n",
    "- **Thought generation**: Multiple candidate reasoning steps generated at each node.\n",
    "- **Thought evaluation**: Explicit scoring (via LLMs) to judge mathematical correctness or plausibility.\n",
    "- **Pruning**: Remove less promising reasoning branches based on evaluation.\n",
    "\n",
    "### 🔹 Practical Considerations:\n",
    "- Explicit node evaluation adds a structured layer of reasoning not present in simpler methods.\n",
    "- Allows LLMs to reason more deliberately by systematically exploring and eliminating alternatives.\n",
    "\n",
    "### ⚠️ Challenges in Implementation:\n",
    "- **Evaluation complexity**:\n",
    "  - Frequent explicit evaluations by LLM can slow the process.\n",
    "  - Requires efficient prompting and scoring techniques.\n",
    "- **Self-consistency**:\n",
    "  - Maintaining logical consistency across multiple branches can be difficult, especially for complex math problems.\n",
    "- **Scaling**:\n",
    "  - Managing multiple branches of reasoning can quickly become computationally expensive without careful control.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Next, we will begin the practical implementation step-by-step.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d7c855",
   "metadata": {
    "id": "72d7c855",
    "papermill": {
     "duration": 0.028185,
     "end_time": "2025-05-23T01:27:43.230538",
     "exception": false,
     "start_time": "2025-05-23T01:27:43.202353",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Implementing Tree of Thoughts (ToT)\n",
    "\n",
    "Before writing any code, it’s essential to map out the steps and functions we need for our Tree of Thoughts (ToT) implementation. In ToT, we aren’t just following one linear chain of reasoning but instead generating a tree of candidate reasoning paths (“thoughts”), evaluating them, and then expanding the most promising ones. We can leverage our existing helper functions (such as those for extracting and normalizing answers) as part of the evaluation process.\n",
    "\n",
    "Below is an outline of the steps and functions we’ll need:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **Node Representation**\n",
    "\n",
    "We need a way to represent each node in our reasoning tree. A node could include:\n",
    "- **Current reasoning text**: The partial solution or thought generated so far.\n",
    "- **Candidate thoughts**: A list of potential next steps (children nodes).\n",
    "- **Evaluation score**: A score indicating how promising the node is (based on plausibility or correctness).\n",
    "- **Metadata**: Such as depth in the tree or a reference to the parent node.\n",
    "\n",
    "**Potential Functions/Classes:**\n",
    "- `class Node`: A class that encapsulates the above properties.\n",
    "- `add_child(self, child_node)`: A method to attach a new candidate thought.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Candidate Thought Generation**\n",
    "\n",
    "This function will use the LLM (via our `get_llm_response` function) to generate multiple candidate reasoning steps given a node's current state.\n",
    "\n",
    "**Key Points:**\n",
    "- The prompt should be carefully crafted to ask the LLM for alternative reasoning steps.\n",
    "- We can use techniques such as few-shot prompting to guide the LLM in generating diverse thoughts.\n",
    "\n",
    "**Potential Function:**\n",
    "- `def generate_candidate_thoughts(node: Node, num_candidates: int) -> List[str]:`\n",
    "  - This function takes the current reasoning state from `node` and returns a list of candidate thoughts (as strings).\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Candidate Thought Evaluation**\n",
    "\n",
    "Once we have multiple candidate thoughts, we need to score them. The evaluation could be based on:\n",
    "- **Model’s self-assessment:** Ask the LLM to rate each candidate on a scale (e.g., 1 to 10) for mathematical plausibility or correctness.\n",
    "- **Heuristics based on helper functions:** Use helper functions like `extract_answer` and `normalize_answer` to check whether a candidate thought moves closer to a correct answer or simplifies the expression.\n",
    "\n",
    "**Potential Function:**\n",
    "- `def evaluate_candidate_thought(candidate: str) -> float:`\n",
    "  - This function might prompt the LLM with the candidate reasoning step, asking, “How plausible or correct is this step?” and return a numeric score.\n",
    "  - Alternatively, it might combine an LLM score with our own heuristic checks.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Pruning and Selection**\n",
    "\n",
    "After evaluating the candidates, we must select the most promising ones for further expansion. Pruning involves:\n",
    "- Ranking candidate thoughts by their evaluation score.\n",
    "- Keeping only the top N candidates (to control the tree size).\n",
    "\n",
    "**Potential Function:**\n",
    "- `def select_best_candidates(candidates: List[str], scores: List[float], top_n: int) -> List[str]:`\n",
    "  - This function will combine the candidate list and their scores to select the best ones for further exploration.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. **Node Expansion**\n",
    "\n",
    "For each node, the process is:\n",
    "1. **Generate candidate thoughts** using the generation function.\n",
    "2. **Evaluate each candidate** using the evaluation function.\n",
    "3. **Select the best candidate(s)** using the pruning/selection function.\n",
    "4. **Expand the tree** by creating child nodes for each selected candidate.\n",
    "\n",
    "**Potential Function:**\n",
    "- `def expand_node(node: Node, num_candidates: int, top_n: int) -> None:`\n",
    "  - This function integrates candidate generation, evaluation, and pruning to add new child nodes to the given `node`.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. **Stopping Criteria**\n",
    "\n",
    "We need clear criteria for when to stop expanding the tree:\n",
    "- **Complete solution found:** When a node contains a complete solution (e.g., using `extract_answer` to verify that a boxed answer exists).\n",
    "- **Depth or resource limits:** When a maximum depth is reached or computational resources are constrained.\n",
    "\n",
    "**Potential Function:**\n",
    "- `def is_solution(node: Node) -> bool:`\n",
    "  - This function checks if a node’s reasoning contains a valid, complete answer.\n",
    "- `def stop_expansion(node: Node, max_depth: int) -> bool:`\n",
    "  - This function checks if the node has reached the maximum allowed depth.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. **Integration with Helper Functions**\n",
    "\n",
    "Our previous helper functions play a crucial role in the ToT implementation:\n",
    "- **Extracting and normalizing answers:**  \n",
    "  Use `extract_answer` and `normalize_answer` to interpret candidate outputs and compare them against the expected solution.\n",
    "- **Comparison functions:**  \n",
    "  Use `compare_answers` to help decide if a candidate thought is moving in the right direction.\n",
    "- **LLM response function:**  \n",
    "  `get_llm_response` is used both for generating candidate thoughts and possibly for scoring them.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. **Overall ToT Flow**\n",
    "\n",
    "Putting it all together, here is an outline of the overall ToT process:\n",
    "1. **Initialize the root node** with the initial problem statement.\n",
    "2. **While the stopping criteria are not met:**\n",
    "   - For the current node, generate candidate thoughts.\n",
    "   - Evaluate each candidate.\n",
    "   - Select and expand the best candidates to form new child nodes.\n",
    "3. **Once a candidate thought leads to a complete solution:**\n",
    "   - Use helper functions to verify correctness.\n",
    "   - Return or record the successful reasoning path.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3fd570",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T01:27:43.288663Z",
     "iopub.status.busy": "2025-05-23T01:27:43.288330Z",
     "iopub.status.idle": "2025-05-23T01:27:43.305566Z",
     "shell.execute_reply": "2025-05-23T01:27:43.304806Z"
    },
    "id": "8e3fd570",
    "papermill": {
     "duration": 0.048256,
     "end_time": "2025-05-23T01:27:43.306837",
     "exception": false,
     "start_time": "2025-05-23T01:27:43.258581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from typing import List, Optional\n",
    "\n",
    "def score_with_gpt(problem: str, candidate: str) -> float:\n",
    "    \"\"\"\n",
    "    Ask a high‑quality LLM (via get_api_response) to rate the given\n",
    "    final answer on a scale from 1 (poor) to 10 (excellent).\n",
    "    \"\"\"\n",
    "    # TODO: build `eval_prompt` string using `problem` and `candidate`\n",
    "    # resp = get_api_response(eval_prompt).strip()\n",
    "    # TODO: use `re.search` to extract the first numeric score from `resp`\n",
    "    # return float(match) or 0.0 on failure\n",
    "    eval_prompt = (\n",
    "        \"You are a precise mathematics grader. \"\n",
    "        f\"Problem: {problem}\\n\\n\"\n",
    "        f\"Candidate solution: {candidate}\\n\\n\"\n",
    "        \"On a scale from 1 (completely incorrect) to 10 (perfectly correct), \"\n",
    "        \"how accurate is this solution? Reply with only a number between 1 and 10.\"\n",
    "    )\n",
    "    # eval_prompt = f\"\"\"You are a math problem solver. Given this problem:\n",
    "    #     {problem}\n",
    "\n",
    "    #     Candidate solution step:\n",
    "    #     {candidate}\n",
    "\n",
    "    #     Rate the mathematical correctness and progress towards solution from 0 (completely wrong) to 10 (perfectly correct). Reply with only a number between 0 and 10.\"\"\"\n",
    "\n",
    "    resp = get_api_response(eval_prompt).strip()\n",
    "    # match = re.search(r'\\d+', resp)\n",
    "    match = re.search(r'\\d+(?:\\.\\d+)?', resp)\n",
    "    # return float(match.group()) if match else 0.0\n",
    "    return max(10.0, min(1.0, float(match.group()))) if match else 0.0\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, state: str, depth: int = 0, parent: Optional['Node'] = None):\n",
    "        self.state = state\n",
    "        self.depth = depth\n",
    "        self.parent = parent\n",
    "        self.children: List[Node] = []      # easy: list to hold child nodes\n",
    "        self.score: Optional[float] = None  # will be set once evaluated\n",
    "\n",
    "    def add_child(self, child_node: 'Node'):\n",
    "        self.children.append(child_node)\n",
    "\n",
    "    def __repr__(self):\n",
    "        # easy: show depth, score, and a truncated preview of `state`\n",
    "        preview = self.state if len(self.state) < 50 else self.state[:47] + \"...\"\n",
    "        return f\"Node(depth={self.depth}, score={self.score}, state='{preview}')\"\n",
    "\n",
    "\n",
    "\n",
    "RETRY_PROMPT = \"\"\"Fix this solution attempt to ensure it ends with a valid boxed answer.\n",
    "\n",
    "Problem:\n",
    "{root_state}\n",
    "\n",
    "Attempt:\n",
    "{state}\n",
    "\n",
    "Return the full corrected solution with a \\\\boxed{{...}} answer.\"\"\"\n",
    "\n",
    "\n",
    "class TreeOfThoughts:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_candidates: int = 3,\n",
    "        top_n: int = 2,\n",
    "        max_depth: int = 3,\n",
    "        verbose: bool = False\n",
    "    ):\n",
    "        self.num_candidates = num_candidates\n",
    "        self.top_n = top_n\n",
    "        self.max_depth = max_depth\n",
    "        self.verbose = verbose\n",
    "        self.root_state: Optional[str] = None\n",
    "\n",
    "    def generate_candidate_thoughts(self, node: Node) -> List[str]:\n",
    "        \"\"\"\n",
    "        Prompt the LLM to produce `num_candidates` boxed answers for `node.state`.\n",
    "        \"\"\"\n",
    "        # TODO: compose prompt with node.state\n",
    "        # TODO: call `get_llm_response(prompt)`\n",
    "        # TODO: split into non-empty lines and return exactly `self.num_candidates` answers\n",
    "        prompt = (\n",
    "            \"You are a mathematical problem solver. \"\n",
    "            f\"Problem: {node.state}\\n\\n\"\n",
    "            f\"Generate {self.num_candidates} distinct possible next steps or solutions \"\n",
    "            \"to this problem. Each solution should end with a final answer in the LaTeX boxed format \"\n",
    "            \"\\\\boxed{...}. Provide each solution on a new line.\"\n",
    "        )\n",
    "        response = get_llm_response(prompt)\n",
    "        # answers = re.findall(r'\\\\boxed\\{(.*?)\\}', response) # if did not work test with answers\n",
    "        # candidates = answers\n",
    "        candidates = [line.strip() for line in response.split('\\n') if line.strip()]\n",
    "\n",
    "        # Ensure we have exactly num_candidates\n",
    "        if len(candidates) < self.num_candidates:\n",
    "            candidates += [candidates[-1]] * (self.num_candidates - len(candidates))\n",
    "        return candidates[:self.num_candidates]\n",
    "        # return [f\"\\\\boxed{{{ans}}}\" for ans in answers[:self.num_candidates]]\n",
    "\n",
    "    def evaluate_candidate_thought(self, candidate: str) -> float:\n",
    "        # simple wrapper around score_with_gpt\n",
    "        assert self.root_state is not None\n",
    "        return score_with_gpt(self.root_state, candidate)\n",
    "\n",
    "    def select_best_candidates(self, candidates: List[str]) -> List[str]:\n",
    "        # TODO: for each cand in candidates:\n",
    "        #         - score = self.evaluate_candidate_thought(cand)\n",
    "        #         - time.sleep(0.5)\n",
    "        #       sort by score descending and return top `self.top_n`\n",
    "        scored = []\n",
    "        for cand in candidates:\n",
    "            score = self.evaluate_candidate_thought(cand)\n",
    "            time.sleep(0.5)  # Rate limit API calls\n",
    "            scored.append((score, cand))\n",
    "\n",
    "        scored.sort(reverse=True, key=lambda x: x[0])\n",
    "        return [cand for score, cand in scored[:self.top_n]]\n",
    "\n",
    "    def expand_node(self, node: Node) -> None:\n",
    "        if self.verbose:\n",
    "            print(f\"\\nExpanding depth {node.depth} state:\\n{node.state}\\n\")\n",
    "        # TODO: raw = self.generate_candidate_thoughts(node)\n",
    "        # TODO: best = self.select_best_candidates(raw)\n",
    "        raw = self.generate_candidate_thoughts(node)\n",
    "        best = self.select_best_candidates(raw)\n",
    "\n",
    "        # for ans in []:  # replace [] with `best`\n",
    "        for ans in best:\n",
    "            child = Node(state=ans, depth=node.depth + 1, parent=node)\n",
    "            # TODO: child.score = self.evaluate_candidate_thought(ans)\n",
    "            # TODO: if not boxed, retry up to 3 times with a strict prompt\n",
    "            child.score = self.evaluate_candidate_thought(ans)\n",
    "\n",
    "            # Ensure we have a boxed answer\n",
    "            retries = 0\n",
    "            while not self.is_solution(child) and retries < 3:\n",
    "                fallback_prompt = RETRY_PROMPT.format(root_state=self.root_state, state=child.state)\n",
    "                child.state = get_llm_response(fallback_prompt).strip()\n",
    "                retries += 1\n",
    "\n",
    "            node.add_child(child)\n",
    "            if self.verbose:\n",
    "                print(f\"Added child: {child}\")\n",
    "\n",
    "    def is_solution(self, node: Node) -> bool:\n",
    "        # TODO: return True if `extract_answer(node.state)` yields non-empty\n",
    "        return extract_answer(node.state) is not None\n",
    "\n",
    "    def stop_expansion(self, node: Node) -> bool:\n",
    "        return node.depth >= self.max_depth\n",
    "\n",
    "    def search(self, root_state: str) -> Node:\n",
    "        \"\"\"\n",
    "        Build the tree until solutions found or max depth reached.\n",
    "        \"\"\"\n",
    "        self.root_state = root_state\n",
    "        root = Node(state=root_state, depth=0)\n",
    "        frontier = [root]\n",
    "        while frontier:\n",
    "            node = frontier.pop(0)\n",
    "            if self.is_solution(node) or self.stop_expansion(node):\n",
    "                continue\n",
    "            self.expand_node(node)\n",
    "            frontier.extend(node.children)\n",
    "        return root\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0631e4d0",
   "metadata": {
    "id": "0631e4d0",
    "papermill": {
     "duration": 0.028079,
     "end_time": "2025-05-23T01:27:43.363567",
     "exception": false,
     "start_time": "2025-05-23T01:27:43.335488",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Testing the Tree of Thoughts Method with Minimal Hyperparameters\n",
    "\n",
    "This cell demonstrates a test run of the Tree of Thoughts (ToT) framework using minimal hyperparameters. The goal is to ensure that a complete final answer (in the format `\\boxed{...}`) is extracted from the model's output.\n",
    "\n",
    "**Key Steps:**\n",
    "\n",
    "- **Instantiate TOT:**  \n",
    "  The TOT instance is created with `num_candidates=1`, `top_n=1`, and `max_depth=1` in verbose mode. This minimal setup is used for quick testing.\n",
    "\n",
    "- **Run the Search:**  \n",
    "  The TOT search is executed on the sample problem:  \n",
    "  *\"Solve the integral: \\( \\int_0^1 x^2 \\, dx \\)\"*\n",
    "\n",
    "- **Print the TOT Tree:**  \n",
    "  A recursive function (`print_tree`) prints the entire search tree, allowing inspection of each node's state and depth.\n",
    "\n",
    "- **Extract Final Answer:**  \n",
    "  All nodes in the tree are collected. If a node is found that contains a final answer (determined via the helper function `extract_answer`), then the node's state is overwritten to display only the final answer in the correct boxed format.  \n",
    "  If no such node is found, a fallback prompt forces the model to output the final answer exclusively.\n",
    "\n",
    "This setup helps verify that the TOT framework correctly isolates and formats the final answer, ensuring the result is comparable to the expected output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b447af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T01:27:43.420846Z",
     "iopub.status.busy": "2025-05-23T01:27:43.420540Z",
     "iopub.status.idle": "2025-05-23T01:28:20.511671Z",
     "shell.execute_reply": "2025-05-23T01:28:20.510846Z"
    },
    "id": "64b447af",
    "outputId": "96558746-59d0-467a-b812-f28af33cbce7",
    "papermill": {
     "duration": 37.121151,
     "end_time": "2025-05-23T01:28:20.512971",
     "exception": false,
     "start_time": "2025-05-23T01:27:43.391820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expanding depth 0 state:\n",
      "Solve the integral: \\( \\int_0^1 x^2 \\, dx \\)\n",
      "\n",
      "Added child: Node(depth=1, score=10.0, state='Okay, so I need to fix the solution attempt for...')\n",
      "Node(depth=0, score=None, state='Solve the integral: \\( \\int_0^1 x^2 \\, dx \\)')\n",
      "  Node(depth=1, score=10.0, state='Okay, so I need to fix the solution attempt for...')\n",
      "---\n",
      "\n",
      "No complete final answer was found. Forcing final answer:\n",
      "\\boxed{1/3}\n"
     ]
    }
   ],
   "source": [
    "# Test the TreeOfThoughts with minimal settings.\n",
    "# Generation of candidate answers still uses your primary LLM via get_llm_response,\n",
    "# while evaluation/scoring uses only the GPT/Gemini verifier.\n",
    "\n",
    "tot = TreeOfThoughts(num_candidates=1,\n",
    "                     top_n=1,\n",
    "                     max_depth=1,\n",
    "                     verbose=True)\n",
    "\n",
    "initial_problem = \"Solve the integral: \\\\( \\\\int_0^1 x^2 \\\\, dx \\\\)\"\n",
    "tot_tree = tot.search(initial_problem)\n",
    "\n",
    "# Helper to print the whole tree\n",
    "def print_tree(node: Node, indent: str = \"\"):\n",
    "    print(indent + repr(node))\n",
    "    for child in node.children:\n",
    "        print_tree(child, indent + \"  \")\n",
    "\n",
    "print_tree(tot_tree)\n",
    "\n",
    "# Collect all nodes and find the first complete solution\n",
    "def collect_all_nodes(node: Node) -> List[Node]:\n",
    "    nodes = [node]\n",
    "    for child in node.children:\n",
    "        nodes.extend(collect_all_nodes(child))\n",
    "    return nodes\n",
    "\n",
    "all_nodes = collect_all_nodes(tot_tree)\n",
    "solution_node = next((n for n in all_nodes if tot.is_solution(n)), None)\n",
    "\n",
    "print(\"---\")\n",
    "if solution_node:\n",
    "    final = extract_answer(solution_node.state)\n",
    "    solution_node.state = f\"\\\\boxed{{{final}}}\"\n",
    "    print(\"\\nFinal Answer Found:\")\n",
    "    print(solution_node.state)\n",
    "else:\n",
    "    # If no solution node was produced, force a final answer via GPT verifier\n",
    "    fallback_prompt = (\n",
    "        f\"Based on the problem: \\\"{initial_problem}\\\", please provide ONLY your final answer \"\n",
    "        \"in the exact format \\\\boxed{...}.\"\n",
    "    )\n",
    "    forced = get_api_response(fallback_prompt).strip()\n",
    "    final = extract_answer(forced) or forced\n",
    "    forced = f\"\\\\boxed{{{final}}}\"\n",
    "    print(\"\\nNo complete final answer was found. Forcing final answer:\")\n",
    "    print(forced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4edf305",
   "metadata": {
    "id": "f4edf305",
    "papermill": {
     "duration": 0.029356,
     "end_time": "2025-05-23T01:28:20.572108",
     "exception": false,
     "start_time": "2025-05-23T01:28:20.542752",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluation of the Tree of Thoughts (ToT) Method on the Math500 Dataset\n",
    "\n",
    "This evaluation framework is designed to test the ToT method on a subset of the Math500 dataset. It provides flexibility in hyperparameter configuration and is set up for both debugging with detailed output and large-scale evaluation. The framework saves results and summarizes key metrics, and it includes mechanisms to force the model to output a final answer in the expected format.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Unique Results File:**  \n",
    "  Uses a dedicated results file (e.g., `evaluation_results_tot_test.json`) to store evaluation data. The file is cleared at the beginning of each run to prevent interference from previous evaluations.\n",
    "\n",
    "- **Configurable Hyperparameters:**  \n",
    "  You can adjust parameters such as:\n",
    "  - `num_candidates`: Number of candidate final answers generated per node.\n",
    "  - `top_n`: Number of top candidates selected for expanding the tree.\n",
    "  - `max_depth`: Maximum depth of the search tree.\n",
    "  \n",
    "  These parameters enable you to test different reasoning strategies and trade-offs between search breadth and depth.\n",
    "\n",
    "- **Sample Selection:**  \n",
    "  The `max_samples` parameter controls the number of problems from the dataset to evaluate. This allows you to start by testing on a single sample and then scale up to 100 or even more problems as desired.\n",
    "\n",
    "- **Debug and Fallback Mechanisms:**  \n",
    "  - The framework prints the entire TOT tree for each problem to facilitate inspection.\n",
    "  - If no node contains a complete final answer (i.e., a final answer wrapped in `\\boxed{...}`), a fallback prompt is issued to force the model to output the final answer.\n",
    "  - Detailed debug outputs help track the process and diagnose any issues in final answer extraction.\n",
    "\n",
    "- **Final Answer Extraction:**  \n",
    "  After processing the tree, the system extracts just the final answer from the model's output (using a helper function like `extract_answer`), ensuring that only the final answer (and no chain-of-thought explanations) is compared against the correct answer.\n",
    "\n",
    "- **Result Saving and Analysis:**  \n",
    "  The framework saves each problem’s evaluation (including problem text, responses, and correctness) and produces a summary report that includes metrics like total problems, correct answers, and overall accuracy.\n",
    "\n",
    "## Encouragement for Further Improvement\n",
    "\n",
    "**Prompt Engineering & Fallback Strategies:**  \n",
    "The current method forces the model to provide a final answer using a fallback prompt when the initial generation does not meet the required format. While this approach works, it is not perfect:\n",
    "- **Prompt Tuning:** Experiment with different wording and structure in the prompts. For example, try different phrasings that emphasize \"ONLY your final answer\" and \"no additional explanations\" to see if the model can be nudged into generating a cleaner response.\n",
    "- **Iterative Refinement:** Consider implementing iterative prompt refinement mechanisms or leveraging additional post-processing steps to filter out unwanted chain-of-thought text.\n",
    "- **Open Research Problem:** The issue of controlling a language model’s output to include only the final answer (and not intermediate reasoning) is an active area of research. There is significant potential to explore improved strategies that maintain reasoning power while enforcing output constraints.\n",
    "\n",
    "**Scalability:**  \n",
    "Once you have fine-tuned the hyperparameters and the prompting strategy on a small sample, encourage testing over a larger set (such as 100 or more problems). Evaluating on a larger dataset can help identify trends and potential improvements that might not be apparent on a smaller scale.\n",
    "\n",
    "**Experiment and Innovate:**  \n",
    "Do not hesitate to modify the prompts, fallback mechanisms, and even the underlying structure of the TOT class. Every change you experiment with might lead to better results and a deeper understanding of how to steer the model toward producing just the final answer. Your experimentation is key to achieving a more robust and reliable evaluation system.\n",
    "\n",
    "This framework is designed to be flexible—feel free to tweak the parameters and strategies to suit your research or production needs, and continue to iterate toward better performance!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b31ebbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T01:28:20.632142Z",
     "iopub.status.busy": "2025-05-23T01:28:20.631836Z",
     "iopub.status.idle": "2025-05-23T01:28:47.126926Z",
     "shell.execute_reply": "2025-05-23T01:28:47.125977Z"
    },
    "id": "7b31ebbf",
    "outputId": "584dac20-5faa-4011-b71e-a28bc0ea001b",
    "papermill": {
     "duration": 26.526872,
     "end_time": "2025-05-23T01:28:47.128363",
     "exception": false,
     "start_time": "2025-05-23T01:28:20.601491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expanding depth 0 state:\n",
      "Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the form $(r,\\theta),$ where $r > 0$ and $0 \\le \\theta < 2 \\pi.$\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems: 100%|██████████| 500/500 [00:25<00:00, 19.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added child: Node(depth=1, score=10.0, state='To convert the rectangular coordinate (0,3) to ...')\n",
      "\n",
      "--- DEBUG: Full TOT tree for problem index 0 ---\n",
      "Node(depth=0, score=None, state='Convert the point $(0,3)$ in rectangular coordi...')\n",
      "  Node(depth=1, score=10.0, state='To convert the rectangular coordinate (0,3) to ...')\n",
      "--- End of TOT tree ---\n",
      "\n",
      "DEBUG: Found solution node: To convert the rectangular coordinate (0,3) to polar coordinates, I'll follow these steps:\n",
      "\n",
      "1. **Calculate the radius (r):**\n",
      "   - Use the formula \\( r = \\sqrt{x^2 + y^2} \\).\n",
      "   - Substituting the values: \\( r = \\sqrt{0^2 + 3^2} = \\sqrt{9} = 3 \\).\n",
      "\n",
      "2. **Determine the angle (θ):**\n",
      "   - Use the formula \\( θ = \\arctan\\left(\\frac{y}{x}\\right) \\).\n",
      "   - Since x is 0, this becomes \\( θ = \\arctan\\left(\\frac{3}{0}\\right) \\).\n",
      "   - Recognizing that dividing by zero is undefined, I know that when x is 0, the point lies on the y-axis.\n",
      "   - Since y is positive, θ is \\( \\frac{\\pi}{2} \\) radians or 90 degrees.\n",
      "\n",
      "Therefore, the polar coordinates are \\( (3, \\frac{\\pi}{2}) \\).\n",
      "</think>\n",
      "\n",
      "To convert the rectangular coordinate \\((0, 3)\\) to polar coordinates, we use the following steps:\n",
      "\n",
      "1. **Calculate the radius \\( r \\):**\n",
      "   \\[\n",
      "   r = \\sqrt{x^2 + y^2} = \\sqrt{0^2 + 3^2} = \\sqrt{9} = 3\n",
      "   \\]\n",
      "\n",
      "2. **Determine the angle \\( \\theta \\):**\n",
      "   - Since \\( x = 0 \\) and \\( y = 3 \\), the point lies on the positive \\( y \\)-axis.\n",
      "   - Therefore, \\( \\theta = \\frac{\\pi}{2} \\) radians.\n",
      "\n",
      "**Final Answer:**\n",
      "\\[\n",
      "\\boxed{\\left(3, \\frac{\\pi}{2}\\right)}\n",
      "\\]\n",
      "Correct: 1/1 | Index: 0\n",
      "\n",
      "=== Results Summary ===\n",
      "Total problems: 1\n",
      "Correct answers: 1\n",
      "Accuracy: 100.00%\n",
      "\n",
      "=== Incorrect Problems ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_tot(max_samples: int = 1):\n",
    "    \"\"\"\n",
    "    Evaluate the Tree of Thoughts (ToT) method on a subset of the Math500 dataset.\n",
    "    Uses GPT/Gemini exclusively for verification and fallback.\n",
    "    Assumes the existence of helper functions:\n",
    "      - load_math500_dataset()\n",
    "      - extract_answer(solution_text)\n",
    "      - compare_answers(correct_answer, predicted_answer)\n",
    "      - save_result(results_file, result_dict)\n",
    "      - load_existing_results(results_file)\n",
    "      - analyze_results(results_list)\n",
    "      - get_llm_response(prompt)     # for initial candidate generation\n",
    "      - get_api_response(prompt)     # for GPT/Gemini–based verification\n",
    "    \"\"\"\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    results_file = \"results/evaluation_results_tot_test.json\"\n",
    "    if os.path.exists(results_file):\n",
    "        os.remove(results_file)\n",
    "\n",
    "    dataset = load_math500_dataset()\n",
    "    existing = load_existing_results(results_file)\n",
    "    processed = {r['index'] for r in existing}\n",
    "\n",
    "    tot = TreeOfThoughts(num_candidates=1, top_n=1, max_depth=1, verbose=True)\n",
    "    correct_count = 0\n",
    "    evaluated = 0\n",
    "\n",
    "    def collect_all_nodes(node):\n",
    "        nodes = [node]\n",
    "        for child in node.children:\n",
    "            nodes.extend(collect_all_nodes(child))\n",
    "        return nodes\n",
    "\n",
    "    for idx, item in enumerate(tqdm(dataset, desc=\"Evaluating problems\")):\n",
    "        if idx in processed or evaluated >= max_samples:\n",
    "            continue\n",
    "\n",
    "        problem_text = item['problem']\n",
    "        correct_answer = extract_answer(item['solution'])\n",
    "\n",
    "        # Run the Tree‑of‑Thoughts search\n",
    "        tot_tree = tot.search(problem_text)\n",
    "\n",
    "        # Debug print of the entire tree\n",
    "        print(f\"\\n--- DEBUG: Full TOT tree for problem index {idx} ---\")\n",
    "        def print_tree(node, indent=\"\"):\n",
    "            print(indent + repr(node))\n",
    "            for c in node.children:\n",
    "                print_tree(c, indent + \"  \")\n",
    "        print_tree(tot_tree)\n",
    "        print(\"--- End of TOT tree ---\\n\")\n",
    "\n",
    "        # Find first node with a boxed answer\n",
    "        all_nodes = collect_all_nodes(tot_tree)\n",
    "        solution_node = next((n for n in all_nodes if tot.is_solution(n)), None)\n",
    "\n",
    "        if solution_node:\n",
    "            response = solution_node.state\n",
    "            print(\"DEBUG: Found solution node:\", response)\n",
    "        else:\n",
    "            # Fallback: ask GPT/Gemini directly for final answer\n",
    "            print(\"DEBUG: No solution node found. Using GPT fallback.\")\n",
    "            fallback_prompt = (\n",
    "                f\"Based on the problem: \\\"{problem_text}\\\", provide ONLY your final answer \"\n",
    "                \"in the exact format \\\\boxed{<final answer>}. Do not include any chain-of-thought.\"\n",
    "            )\n",
    "            response = get_api_response(fallback_prompt)\n",
    "            print(\"DEBUG: Fallback response:\", response)\n",
    "\n",
    "        predicted = extract_answer(response) or \"\"\n",
    "        if not predicted:\n",
    "            print(\"DEBUG: predicted_answer is empty. Raw response was:\\n\", response)\n",
    "\n",
    "        is_correct = compare_answers(correct_answer, predicted)\n",
    "        save_result(results_file, {\n",
    "            \"index\": idx,\n",
    "            \"problem\": problem_text,\n",
    "            \"response\": response,\n",
    "            \"correct_answer\": correct_answer,\n",
    "            \"predicted_answer\": predicted,\n",
    "            \"is_correct\": is_correct\n",
    "        })\n",
    "\n",
    "        if is_correct:\n",
    "            correct_count += 1\n",
    "        evaluated += 1\n",
    "        print(f\"Correct: {correct_count}/{evaluated} | Index: {idx}\")\n",
    "\n",
    "    final = load_existing_results(results_file)\n",
    "    analyze_results(final)\n",
    "\n",
    "# Example: test a single sample\n",
    "evaluate_tot(max_samples=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7694f26",
   "metadata": {
    "id": "c7694f26",
    "papermill": {
     "duration": 0.031268,
     "end_time": "2025-05-23T01:28:47.190440",
     "exception": false,
     "start_time": "2025-05-23T01:28:47.159172",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# A* Search Algorithm for Mathematical Reasoning\n",
    "\n",
    "The A* (A-star) search algorithm is a popular informed search method that combines both actual cost and an estimated cost to reach the goal. When applied to mathematical problem-solving with language models, each node in the search tree represents a partial reasoning process. The goal is to guide the search toward the correct final answer efficiently.\n",
    "\n",
    "## Core Concepts\n",
    "\n",
    "- **Nodes and States:**  \n",
    "  Each node represents a state in the reasoning process—a partial solution or chain-of-thought step. The root node is the initial problem, and child nodes represent possible next steps in reasoning.\n",
    "\n",
    "- **Cost Function (g(n)):**  \n",
    "  This function measures the cost accumulated from the start node to the current node. In our context, it might represent the complexity or length of the reasoning chain so far.\n",
    "\n",
    "- **Heuristic Function (h(n)):**  \n",
    "  A heuristic estimates the cost (or “distance”) from the current node to the goal (a complete final answer). For mathematical reasoning, this could be designed to reflect how promising the current partial solution is—possibly by prompting the LLM to provide a confidence or plausibility score.\n",
    "\n",
    "- **Evaluation Function (f(n)):**  \n",
    "  A* uses the function:\n",
    "  \\[\n",
    "  f(n) = g(n) + h(n)\n",
    "  \\]\n",
    "  to choose which node to expand next. Nodes with lower f(n) are expanded first, steering the search toward the most promising reasoning paths.\n",
    "\n",
    "## How A* Works in Mathematical Reasoning\n",
    "\n",
    "1. **Initialization:**  \n",
    "   The algorithm starts with the initial problem as the root node, with an initial cost \\(g(n)=0\\).\n",
    "\n",
    "2. **Expansion:**  \n",
    "   From the current node, the model generates several potential next steps (child nodes). Each child node represents a possible continuation of the reasoning process.\n",
    "\n",
    "3. **Cost and Heuristic Calculation:**  \n",
    "   - **g(n):** Represents the cost accumulated so far (e.g., the number of reasoning steps taken).\n",
    "   - **h(n):** An estimate of how “far” the current state is from a complete final answer. This can be derived via LLM-based evaluations or comparisons to known correct patterns.\n",
    "\n",
    "4. **Priority Queue and Node Selection:**  \n",
    "   The algorithm uses a priority queue to maintain nodes, sorted by their \\( f(n) \\) value. The node with the smallest \\(f(n)\\) (i.e., the most promising combination of current cost and estimated remaining cost) is expanded next.\n",
    "\n",
    "5. **Goal Test:**  \n",
    "   The process continues until a node is found that meets the goal—a node whose state contains a complete final answer in the expected format (e.g., a LaTeX expression wrapped in `\\boxed{...}`).\n",
    "\n",
    "## Challenges in Applying A* to Reasoning\n",
    "\n",
    "- **Heuristic Design:**  \n",
    "  Defining an effective h(n) is challenging. The heuristic must correlate well with the true “distance” to a correct final answer. For language models, this might involve model confidence scores or custom prompt evaluations.\n",
    "\n",
    "- **Balancing Exploration and Exploitation:**  \n",
    "  Overemphasis on g(n) might favor shorter, less-complete reasoning chains, while too much reliance on h(n) might cause the search to overestimate the quality of partially correct answers.\n",
    "\n",
    "- **Computational Expense:**  \n",
    "  Evaluating each node’s heuristic (potentially via additional LLM queries) can be computationally expensive, especially in a large search space.\n",
    "\n",
    "- **Scalability:**  \n",
    "  The state space for reasoning is vast. A well-tuned A* algorithm can efficiently prune irrelevant paths, but without a robust heuristic, the number of nodes to explore can grow exponentially.\n",
    "\n",
    "\n",
    "\n",
    "Next, we will move on to the code implementation of the A* algorithm for our reasoning framework.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f7f15b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T01:28:47.254752Z",
     "iopub.status.busy": "2025-05-23T01:28:47.254448Z",
     "iopub.status.idle": "2025-05-23T01:28:47.271469Z",
     "shell.execute_reply": "2025-05-23T01:28:47.270736Z"
    },
    "id": "75f7f15b",
    "papermill": {
     "duration": 0.05044,
     "end_time": "2025-05-23T01:28:47.272834",
     "exception": false,
     "start_time": "2025-05-23T01:28:47.222394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import heapq\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "class AStarNode:\n",
    "    def __init__(self, state: str, g: float = 0.0, parent: Optional['AStarNode'] = None):\n",
    "        self.state = state                      # easy: store the current problem or partial answer\n",
    "        self.g = g                              # easy: cost so far (depth)\n",
    "        self.h: float = 0.0                     # will be set by heuristic evaluator\n",
    "        self.f: float = 0.0                     # f = g + h\n",
    "        self.parent = parent                    # link back to parent for solution path\n",
    "        self.children: List[AStarNode] = []     # easy: list to hold generated successors\n",
    "\n",
    "    def __lt__(self, other: 'AStarNode'):\n",
    "        return self.f < other.f                 # easy: allow heapq to compare nodes by f‑value\n",
    "\n",
    "    def __repr__(self):\n",
    "        preview = self.state if len(self.state) < 50 else self.state[:47] + \"...\"\n",
    "        return f\"AStarNode(g={self.g}, h={self.h}, f={self.f}, state='{preview}')\"\n",
    "\n",
    "\n",
    "GEN_SYSTEM_PROMPT = \"\"\"\n",
    "You are a mathematical problem solver.\n",
    "\n",
    "Problem:\n",
    "{node}\n",
    "\n",
    "Generate {num_candidates} possible next steps or solutions.\n",
    "Each should end with a final answer in the LaTeX boxed format \\\\boxed{{...}}.\n",
    "Provide each solution on a new line.\n",
    "\"\"\"\n",
    "\n",
    "EVAL_PROMPT = \"\"\"\n",
    "You are a precise mathematics grader.\n",
    "\n",
    "Original problem:\n",
    "{root_problem}\n",
    "\n",
    "Current reasoning state: {state}\n",
    "\n",
    "On a scale from 0 (complete final answer in \\\\boxed{{...}} format) to 10\n",
    "(completely unrelated to the problem), how incomplete is this reasoning step?\n",
    "Reply with only a number between 0 and 10.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "RETRY_PROMPT = \"\"\"\n",
    "f\"Problem:\n",
    "{root_problem}\n",
    "\n",
    "Please provide ONLY your final answer in the exact format \\\\boxed{{...}}\n",
    "with no additional explanation.\n",
    "\"\"\"\n",
    "\n",
    "class AStarSearch:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_candidates: int = 3,\n",
    "        max_depth: int = 3,\n",
    "        verbose: bool = False,\n",
    "        max_fallback: int = 3\n",
    "    ):\n",
    "        self.num_candidates = num_candidates    # how many answers to generate per node\n",
    "        self.max_depth = max_depth              # search will stop at this depth\n",
    "        self.verbose = verbose                  # if True, print debug info\n",
    "        self.max_fallback = max_fallback        # retries for enforcing boxed format\n",
    "        self.root_problem: Optional[str] = None  # will hold the original problem text\n",
    "\n",
    "    def is_solution(self, state: str) -> bool:\n",
    "        # TODO: return True if `extract_answer(state)` yields a non‑empty string\n",
    "        return extract_answer(state) is not None\n",
    "\n",
    "    def generate_candidates(self, node: AStarNode) -> List[str]:\n",
    "        \"\"\"\n",
    "        Ask the LLM for `num_candidates` boxed answers to node.state.\n",
    "        \"\"\"\n",
    "        # TODO: compose a prompt with node.state asking for LaTeX \\\\boxed{...} answers\n",
    "        prompt = GEN_SYSTEM_PROMPT.format(node=node.state, num_candidates=self.num_candidates)\n",
    "\n",
    "        # TODO: call `get_llm_response(prompt)`\n",
    "        response = get_llm_response(prompt)\n",
    "\n",
    "        # TODO: split the response into non‑empty lines\n",
    "        candidates = [line.strip() for line in response.split('\\n') if line.strip()]\n",
    "\n",
    "        # TODO: if fewer than `self.num_candidates`, replicate lines to reach that count\n",
    "        # TODO: return exactly `self.num_candidates` answer strings\n",
    "        if len(candidates) < self.num_candidates:\n",
    "            candidates += [candidates[-1]] * (self.num_candidates - len(candidates))\n",
    "        return candidates[:self.num_candidates]\n",
    "\n",
    "\n",
    "    def evaluate_heuristic(self, state: str) -> float:\n",
    "        \"\"\"\n",
    "        Use GPT (via get_api_response) to score how incomplete a candidate is:\n",
    "        0 means perfect boxed answer; higher means more incomplete.\n",
    "        \"\"\"\n",
    "        assert self.root_problem is not None, \"Root problem must be set before heuristic evaluation\"\n",
    "        # TODO: build `eval_prompt` using self.root_problem and the current `state`\n",
    "        eval_prompt = EVAL_PROMPT.format(root_problem=self.root_problem, state=state)\n",
    "\n",
    "        # TODO: call `get_api_response(eval_prompt).strip()`\n",
    "        resp = get_api_response(eval_prompt).strip()\n",
    "\n",
    "        # TODO: extract the first numeric value with `re.search`\n",
    "        match = re.search(r'\\d+(?:\\.\\d+)?', resp)\n",
    "\n",
    "        # TODO: return that float, or a fallback like `self.max_depth * 10` if parsing fails\n",
    "        return max(0.0, min(10.0, float(match.group()))) if match else self.max_depth * 10\n",
    "\n",
    "\n",
    "    def expand_node(self, node: AStarNode) -> List[AStarNode]:\n",
    "        if self.verbose:\n",
    "            print(f\"\\nExpanding node at depth {node.g}:\\n{node.state}\\n\")\n",
    "\n",
    "        if self.root_problem is None:\n",
    "            self.root_problem = node.state\n",
    "        candidates = self.generate_candidates(node)\n",
    "        children: List[AStarNode] = []\n",
    "        # TODO: for each `cand` in candidates:\n",
    "        for cand in candidates:\n",
    "            attempts = 0\n",
    "            #      - while not self.is_solution(cand) and attempts < self.max_fallback:\n",
    "            #            * send strict fallback prompt via get_llm_response\n",
    "            #            * cand = response.strip()\n",
    "            #            * attempts += 1\n",
    "            #            * if verbose: print fallback info\n",
    "            while not self.is_solution(cand) and attempts < self.max_fallback:\n",
    "                fallback_prompt = RETRY_PROMPT.format(root_problem = self.root_problem)\n",
    "                cand = get_llm_response(fallback_prompt).strip()\n",
    "                attempts += 1\n",
    "                if self.verbose: print(f\"Fallback attempt {attempts}: {cand}\")\n",
    "\n",
    "            child = AStarNode(state=cand, g=node.g + 1, parent=node)\n",
    "            child.h = self.evaluate_heuristic(child.state)\n",
    "            child.f = child.g + child.h\n",
    "            node.children.append(child)\n",
    "            children.append(child)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"Generated child: {child}\")\n",
    "        return children\n",
    "\n",
    "    def search(self, initial_problem: str) -> Optional[AStarNode]:\n",
    "        \"\"\"\n",
    "        Run A* until a boxed solution is found or max_depth is exceeded.\n",
    "        \"\"\"\n",
    "        self.root_problem = initial_problem\n",
    "        root = AStarNode(state=initial_problem, g=0.0)\n",
    "        root.h = self.evaluate_heuristic(root.state)\n",
    "        root.f = root.g + root.h\n",
    "\n",
    "        frontier: List[AStarNode] = []\n",
    "        heapq.heappush(frontier, root)\n",
    "\n",
    "        while frontier:\n",
    "            node = heapq.heappop(frontier)\n",
    "            if self.verbose:\n",
    "                print(f\"Expanding: {node}\")\n",
    "\n",
    "            if self.is_solution(node.state):\n",
    "                return node\n",
    "\n",
    "            if node.g >= self.max_depth:\n",
    "                continue\n",
    "\n",
    "            for child in self.expand_node(node):\n",
    "                heapq.heappush(frontier, child)\n",
    "\n",
    "        # TODO: if no solution found, return None\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788e0372",
   "metadata": {
    "id": "788e0372",
    "papermill": {
     "duration": 0.030188,
     "end_time": "2025-05-23T01:28:47.333949",
     "exception": false,
     "start_time": "2025-05-23T01:28:47.303761",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test Code for A* Search Method with Minimal Hyperparameters\n",
    "\n",
    "Below is a description of the test procedure for the A* search method using minimal hyperparameters. This test ensures that the algorithm returns only the final answer in the proper format, without extra chain-of-thought text.\n",
    "\n",
    "- **Initialization:**\n",
    "  - An A* search instance is created with the following settings:\n",
    "    - `num_candidates`: 1 (only one candidate is generated per node)\n",
    "    - `max_depth`: 1 (the search tree is kept shallow for testing)\n",
    "    - `verbose`: True (detailed debug information is printed)\n",
    "    - `max_fallback`: 3 (up to three fallback attempts are made to force a final answer)\n",
    "  - The initial problem is set as:\n",
    "    - \"Solve the integral: \\( \\int_0^1 x^2 \\, dx \\)\"\n",
    "\n",
    "- **Search Execution:**\n",
    "  - The A* search is executed on the initial problem to obtain a solution node.\n",
    "  \n",
    "- **Tree Printing:**\n",
    "  - A recursive function (e.g., `print_astar_tree`) is used to print the entire A* search tree, starting from the root. This allows inspection of all nodes and the reasoning process.\n",
    "\n",
    "- **Final Answer Extraction:**\n",
    "  - If a solution node is found, the algorithm backtracks to the root to print the entire tree.\n",
    "  - Then it extracts the final answer from the solution node using a helper function (e.g., `extract_answer`). The state of the solution node is reformatted to display only the final answer in the exact format (e.g., `\\boxed{<final answer>}`).\n",
    "\n",
    "- **Fallback Handling:**\n",
    "  - If no solution node is found, a fallback prompt is issued. This prompt instructs the model to provide ONLY its final answer in the correct format, with no extra explanation.\n",
    "  - The fallback final answer is then printed.\n",
    "\n",
    "This test code is designed to verify that, with minimal hyperparameters, the A* search method consistently returns a complete final answer, ensuring the output is directly comparable with the expected result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad61fd73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T01:28:47.395591Z",
     "iopub.status.busy": "2025-05-23T01:28:47.395288Z",
     "iopub.status.idle": "2025-05-23T01:29:05.691113Z",
     "shell.execute_reply": "2025-05-23T01:29:05.690094Z"
    },
    "id": "ad61fd73",
    "outputId": "251ab1bd-d9de-4917-a901-c179b947773a",
    "papermill": {
     "duration": 18.328728,
     "end_time": "2025-05-23T01:29:05.692535",
     "exception": false,
     "start_time": "2025-05-23T01:28:47.363807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanding: AStarNode(g=0.0, h=10.0, f=10.0, state='Solve the integral: \\( \\int_0^1 x^2 \\, dx \\)')\n",
      "\n",
      "Expanding node at depth 0.0:\n",
      "Solve the integral: \\( \\int_0^1 x^2 \\, dx \\)\n",
      "\n",
      "Fallback attempt 1: Okay, so I need to solve the integral of x squared from 0 to 1. Hmm, let's see. I remember that integrals are about finding the area under a curve, right? In this case, the curve is y = x squared, and we're looking from x=0 to x=1. \n",
      "\n",
      "First, I should recall the basic rules of integration. I think the integral of x^n dx is (x^(n+1))/(n+1) + C, where C is the constant of integration. But wait, since we're doing a definite integral from 0 to 1, I don't need the constant. \n",
      "\n",
      "So, applying that formula to x squared. Here, n is 2 because it's x^2. So the integral should be (x^(2+1))/(2+1) = x^3 / 3. \n",
      "\n",
      "Now, I need to evaluate this from 0 to 1. That means I plug in the upper limit, which is 1, into the antiderivative, and then subtract the value of the antiderivative at the lower limit, which is 0. \n",
      "\n",
      "Let me write that out:\n",
      "\n",
      "∫₀¹ x² dx = [x³ / 3] from 0 to 1.\n",
      "\n",
      "Calculating the upper limit: (1)³ / 3 = 1/3.\n",
      "\n",
      "Calculating the lower limit: (0)³ / 3 = 0/3 = 0.\n",
      "\n",
      "Subtracting the lower limit from the upper limit: 1/3 - 0 = 1/3.\n",
      "\n",
      "So, the integral of x squared from 0 to 1 is 1/3. \n",
      "\n",
      "Wait, let me double-check to make sure I didn't make any mistakes. The formula seems right, and when I plug in 1, I get 1/3, and plugging in 0 gives 0. Subtracting them gives 1/3. Yeah, that seems correct.\n",
      "\n",
      "I don't think there are any other steps needed here. It's a straightforward integral, and I applied the power rule correctly. So, I think I'm confident that the answer is 1/3.\n",
      "\n",
      "**Final Answer**\n",
      "The value of the integral is \\boxed{\\dfrac{1}{3}}.\n",
      "</think>\n",
      "\n",
      "To solve the integral \\( \\int_0^1 x^2 \\, dx \\), we use the power rule for integration\n",
      "Generated child: AStarNode(g=1.0, h=0.0, f=1.0, state='Okay, so I need to solve the integral of x squa...')\n",
      "Expanding: AStarNode(g=1.0, h=0.0, f=1.0, state='Okay, so I need to solve the integral of x squa...')\n",
      "AStarNode(g=0.0, h=10.0, f=10.0, state='Solve the integral: \\( \\int_0^1 x^2 \\, dx \\)')\n",
      "  AStarNode(g=1.0, h=0.0, f=1.0, state='Okay, so I need to solve the integral of x squa...')\n",
      "\n",
      "Final Answer Found:\n",
      "\\boxed{\\dfrac{1}{3}}\n"
     ]
    }
   ],
   "source": [
    "# Test Code for A* Search Method with Minimal Hyperparameters\n",
    "\n",
    "# Initialize the A* search instance with minimal settings.\n",
    "# Candidate generation still uses get_llm_response(...),\n",
    "# but all verification and final fallback use get_api_response(...) (Gemini).\n",
    "astar = AStarSearch(\n",
    "    num_candidates=1,\n",
    "    max_depth=1,\n",
    "    verbose=True,\n",
    "    max_fallback=3\n",
    ")\n",
    "\n",
    "initial_problem = \"Solve the integral: \\\\( \\\\int_0^1 x^2 \\\\, dx \\\\)\"\n",
    "solution_node = astar.search(initial_problem)\n",
    "\n",
    "# Recursive printer for the A* search tree.\n",
    "def print_astar_tree(node, indent=\"\"):\n",
    "    print(indent + repr(node))\n",
    "    for child in node.children:\n",
    "        print_astar_tree(child, indent + \"  \")\n",
    "\n",
    "if solution_node:\n",
    "    # Backtrack to the root.\n",
    "    root = solution_node\n",
    "    while root.parent is not None:\n",
    "        root = root.parent\n",
    "\n",
    "    # Print the entire tree from the root.\n",
    "    print_astar_tree(root)\n",
    "\n",
    "    # Extract and normalize the final answer.\n",
    "    final = extract_answer(solution_node.state)\n",
    "    if final:\n",
    "        solution_node.state = f\"\\\\boxed{{{final}}}\"\n",
    "    print(\"\\nFinal Answer Found:\")\n",
    "    print(solution_node.state)\n",
    "else:\n",
    "    # No solution found: use GPT/Gemini directly for the final answer.\n",
    "    fallback_prompt = (\n",
    "        f\"Based on the problem: \\\"{initial_problem}\\\", please provide ONLY your final answer \"\n",
    "        \"in the exact format \\\\boxed{<final answer>}. Do not include any chain-of-thought or extra explanation.\"\n",
    "    )\n",
    "    forced_final_answer = get_api_response(fallback_prompt).strip()\n",
    "    final = extract_answer(forced_final_answer) or forced_final_answer\n",
    "    forced_final_answer = f\"\\\\boxed{{{final}}}\"\n",
    "    print(\"\\nNo complete solution node found. Forced final answer:\")\n",
    "    print(forced_final_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f64b71d",
   "metadata": {
    "id": "5f64b71d",
    "papermill": {
     "duration": 0.029534,
     "end_time": "2025-05-23T01:29:05.752444",
     "exception": false,
     "start_time": "2025-05-23T01:29:05.722910",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluation of the A* Search Method on the Math500 Dataset\n",
    "\n",
    "This evaluation framework is designed to test the A* search method on a subset of the Math500 dataset. It provides flexibility in hyperparameter configuration and is set up for both detailed output and large-scale evaluation. The framework saves results, summarizes key metrics, and includes mechanisms to force the model to output a final answer in the expected format.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Unique Results File:**  \n",
    "  Uses a dedicated results file (e.g., `evaluation_results_astar_test.json`) to store evaluation data. The file is cleared at the start of each run to prevent interference from previous evaluations.\n",
    "\n",
    "- **Configurable Hyperparameters:**  \n",
    "  You can adjust parameters such as:\n",
    "  - `num_candidates`: Number of candidate final answers generated per node.\n",
    "  - `max_depth`: Maximum depth of the search tree.\n",
    "  \n",
    "  These settings enable you to test different reasoning strategies and trade-offs between search exploration depth and the precision of the final answer.\n",
    "\n",
    "- **Sample Selection:**  \n",
    "  The `max_samples` parameter controls the number of problems from the dataset to evaluate. This allows you to start by testing on a single sample and then scale up to 100 or more problems as desired.\n",
    "\n",
    "- **Fallback Mechanisms:**  \n",
    "  If no node in the A* search tree contains a complete final answer (i.e., one wrapped in `\\boxed{...}`), a fallback prompt is issued to force the model to output only the final answer. This guarantees that every evaluated problem produces a final answer in the proper format.\n",
    "\n",
    "- **Final Answer Extraction:**  \n",
    "  After processing the search tree, the framework extracts just the final answer from the model’s output (using a helper function like `extract_answer`). This ensures that only the final answer is compared against the expected solution, without any additional chain-of-thought text.\n",
    "\n",
    "- **Result Saving and Analysis:**  \n",
    "  The framework saves detailed evaluation data—including the problem text, the model's raw response, the extracted final answer, and correctness—and produces a summary report with metrics such as total problems evaluated, number of correct answers, and overall accuracy.\n",
    "\n",
    "## Encouragement for Further Improvement\n",
    "\n",
    "**Prompt Engineering & Fallback Strategies:**  \n",
    "The current method forces the model to provide a final answer using a fallback prompt if the initial search does not yield the required format. Experiment with:\n",
    "- **Prompt Tuning:** Adjust the wording and structure to further emphasize \"ONLY your final answer\" and \"no extra text.\"\n",
    "- **Iterative Refinement:** Consider iterative prompt refinement or additional post-processing to isolate the final answer more reliably.\n",
    "- **Innovative Approaches:** This challenge of extracting only the final answer is an active area of research. Exploring new strategies may lead to better performance and more robust results.\n",
    "\n",
    "**Scalability:**  \n",
    "Once the hyperparameters and prompting strategy are fine-tuned on a small set of problems, scale the evaluation to a larger sample (e.g., 100+ problems). A broader evaluation can reveal trends and help identify further improvements.\n",
    "\n",
    "**Experiment and Innovate:**  \n",
    "Feel free to modify prompts, adjust hyperparameters, and refine fallback mechanisms. Comparing the A* search method with other approaches, such as the Tree of Thoughts method, can provide valuable insights. Your experimentation is key to developing a more robust and reliable evaluation system.\n",
    "\n",
    "This flexible framework is designed to meet diverse research and production needs—keep iterating and exploring until you achieve optimal results!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f3e58b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T01:29:05.814374Z",
     "iopub.status.busy": "2025-05-23T01:29:05.813583Z",
     "iopub.status.idle": "2025-05-23T01:29:42.748332Z",
     "shell.execute_reply": "2025-05-23T01:29:42.747396Z"
    },
    "id": "26f3e58b",
    "outputId": "a5d959e9-1bd5-4b2c-f950-f4d7fba5f52d",
    "papermill": {
     "duration": 36.966515,
     "end_time": "2025-05-23T01:29:42.749858",
     "exception": false,
     "start_time": "2025-05-23T01:29:05.783343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanding: AStarNode(g=0.0, h=10.0, f=10.0, state='The expression $10x^2-x-24$ can be written as $...')\n",
      "\n",
      "Expanding node at depth 0.0:\n",
      "The expression $10x^2-x-24$ can be written as $(Ax-8)(Bx+3),$ where $A$ and $B$ are integers. What is $AB + B$?\n",
      "\n",
      "Fallback attempt 1: Okay, so I have this problem where I need to express the quadratic expression \\(10x^2 - x - 24\\) as a product of two binomials in the form \\((Ax - 8)(Bx + 3)\\), where \\(A\\) and \\(B\\) are integers. Then, I have to find the value of \\(AB + B\\). Hmm, okay, let me break this down step by step.\n",
      "\n",
      "First, I remember that when you multiply two binomials, you can use the FOIL method to expand them and then compare the coefficients with the original expression. The original expression is \\(10x^2 - x - 24\\), and the target form is \\((Ax - 8)(Bx + 3)\\). So, let me write that out using the FOIL method.\n",
      "\n",
      "FOIL stands for First, Outer, Inner, Last. That means I'll multiply the first terms, then the outer terms, then the inner terms, and finally the last terms.\n",
      "\n",
      "So, applying that to \\((Ax - 8)(Bx + 3)\\):\n",
      "\n",
      "First: \\(Ax \\times Bx = ABx^2\\)\n",
      "\n",
      "Outer: \\(Ax \\times 3 = 3Ax\\)\n",
      "\n",
      "Inner: \\(-8 \\times Bx = -8Bx\\)\n",
      "\n",
      "Last: \\(-8 \\times 3 = -24\\)\n",
      "\n",
      "Now, let's combine these terms:\n",
      "\n",
      "\\(ABx^2 + 3Ax - 8Bx - 24\\)\n",
      "\n",
      "Now, I can combine like terms. The middle terms both have an \\(x\\), so I can factor that out:\n",
      "\n",
      "\\(ABx^2 + (3A - 8B)x - 24\\)\n",
      "\n",
      "So, comparing this to the original expression \\(10x^2 - x - 24\\), their coefficients must be equal. That gives me two equations:\n",
      "\n",
      "1. Coefficient of \\(x^2\\): \\(AB = 10\\)\n",
      "2. Coefficient of \\(x\\): \\(3A - 8B = -1\\)\n",
      "\n",
      "So, I now have a system of two equations:\n",
      "\n",
      "1. \\(AB = 10\\)\n",
      "2. \\(3A - 8B = -1\\)\n",
      "\n",
      "My goal is to find integers \\(A\\) and \\(B\\) that satisfy both equations.\n",
      "\n",
      "Let me list the pairs of integers \\(A\n",
      "Fallback attempt 2: Okay, so I have this problem here: I need to express the quadratic expression \\(10x^2 - x - 24\\) as a product of two binomials in the form \\((Ax - 8)(Bx + 3)\\), where \\(A\\) and \\(B\\) are integers. Then, I have to find the value of \\(AB + B\\). Hmm, let me think about how to approach this.\n",
      "\n",
      "First, I remember that when you multiply two binomials, you can use the FOIL method—First, Outer, Inner, Last. So, let me apply that here. If I have \\((Ax - 8)(Bx + 3)\\), multiplying them should give me \\(ABx^2 + (3A - 8B)x - 24\\). Let me check that:\n",
      "\n",
      "- First: \\(Ax \\times Bx = ABx^2\\)\n",
      "- Outer: \\(Ax \\times 3 = 3Ax\\)\n",
      "- Inner: \\(-8 \\times Bx = -8Bx\\)\n",
      "- Last: \\(-8 \\times 3 = -24\\)\n",
      "\n",
      "So, combining the middle terms: \\(3Ax - 8Bx = (3A - 8B)x\\). Therefore, the expanded form is \\(ABx^2 + (3A - 8B)x - 24\\).\n",
      "\n",
      "Now, this should be equal to the original expression \\(10x^2 - x - 24\\). So, I can set up equations by equating the coefficients of corresponding terms.\n",
      "\n",
      "Let me write that out:\n",
      "\n",
      "1. The coefficient of \\(x^2\\): \\(AB = 10\\)\n",
      "2. The coefficient of \\(x\\): \\(3A - 8B = -1\\)\n",
      "3. The constant term: \\(-24\\) which matches, so that's good.\n",
      "\n",
      "So, now I have two equations:\n",
      "\n",
      "1. \\(AB = 10\\)\n",
      "2. \\(3A - 8B = -1\\)\n",
      "\n",
      "I need to find integers \\(A\\) and \\(B\\) that satisfy both equations. Let me list the possible integer pairs for \\(A\\) and \\(B\\) such that their product is 10.\n",
      "\n",
      "The pairs are:\n",
      "\n",
      "- (1, 10)\n",
      "- (2, 5)\n",
      "- (5, 2)\n",
      "- (10,\n",
      "Fallback attempt 3: Okay, so I have this problem here: I need to rewrite the quadratic expression 10x² - x - 24 in the form (Ax - 8)(Bx + 3), where A and B are integers. Then, I have to find AB + B. Hmm, let me think about how to approach this.\n",
      "\n",
      "First, I remember that when you factor a quadratic expression like ax² + bx + c, you can express it as (mx + n)(px + q), where m, n, p, q are integers. In this case, the given expression is 10x² - x - 24, and I need to factor it into (Ax - 8)(Bx + 3). So, that means I have to find integers A and B such that when I multiply (Ax - 8) and (Bx + 3), I get 10x² - x - 24.\n",
      "\n",
      "Let me write down the multiplication:\n",
      "\n",
      "(Ax - 8)(Bx + 3) = ABx² + (3A - 8B)x - 24.\n",
      "\n",
      "Now, comparing this to the original expression 10x² - x - 24, we can set up equations for the coefficients:\n",
      "\n",
      "1. For x² term: AB = 10\n",
      "2. For x term: 3A - 8B = -1\n",
      "3. For the constant term: -24, which matches, so that's good.\n",
      "\n",
      "So, I need to find integers A and B such that AB = 10 and 3A - 8B = -1.\n",
      "\n",
      "Let me list the possible integer pairs for A and B that multiply to 10. Since 10 is positive, both A and B must be either positive or both negative.\n",
      "\n",
      "Possible pairs:\n",
      "- (1, 10)\n",
      "- (2, 5)\n",
      "- (5, 2)\n",
      "- (10, 1)\n",
      "- (-1, -10)\n",
      "- (-2, -5)\n",
      "- (-5, -2)\n",
      "- (-10, -1)\n",
      "\n",
      "Now, I need to test these pairs in the equation 3A - 8B = -1.\n",
      "\n",
      "Let's start with positive pairs:\n",
      "\n",
      "1. A=1, B=10:\n",
      "   3(1) - 8(10) = 3 -\n",
      "Generated child: AStarNode(g=1.0, h=7.0, f=8.0, state='Okay, so I have this problem here: I need to re...')\n",
      "Expanding: AStarNode(g=1.0, h=7.0, f=8.0, state='Okay, so I have this problem here: I need to re...')\n",
      "DEBUG: No solution node found. Using GPT fallback.\n",
      "DEBUG: Fallback response: \\boxed{11}\n",
      "Correct: 0/1 | Index: 316\n",
      "\n",
      "=== Results Summary ===\n",
      "Total problems: 1\n",
      "Correct answers: 0\n",
      "Accuracy: 0.00%\n",
      "\n",
      "=== Incorrect Problems ===\n",
      "Problem 316:\n",
      "Expected: 12\n",
      "Predicted: 11\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_astar_random(max_samples: int = 1):\n",
    "    \"\"\"\n",
    "    Evaluate the A* search method on randomly selected problems from the Math500 dataset.\n",
    "    Uses GPT/Gemini (via get_api_response) for any fallback final-answer requests.\n",
    "    Assumes existence of:\n",
    "      - load_math500_dataset()\n",
    "      - extract_answer(solution_text)\n",
    "      - compare_answers(correct_answer, predicted_answer)\n",
    "      - save_result(results_file, result_dict)\n",
    "      - load_existing_results(results_file)\n",
    "      - analyze_results(results_list)\n",
    "      - get_llm_response(prompt)   # for candidate generation\n",
    "      - get_api_response(prompt)   # for GPT/Gemini–based fallback\n",
    "    \"\"\"\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    results_file = \"results/evaluation_results_astar_random_test.json\"\n",
    "\n",
    "    # Clear previous test results\n",
    "    if os.path.exists(results_file):\n",
    "        os.remove(results_file)\n",
    "\n",
    "    dataset = load_math500_dataset()\n",
    "    total = len(dataset)\n",
    "\n",
    "    # Pick random unique indices\n",
    "    selected = set()\n",
    "    while len(selected) < max_samples:\n",
    "        selected.add(random.randint(0, total - 1))\n",
    "\n",
    "    astar = AStarSearch(num_candidates=1, max_depth=1, verbose=True, max_fallback=3)\n",
    "    correct_count = 0\n",
    "    evaluated = 0\n",
    "\n",
    "    def collect_all_nodes(node):\n",
    "        nodes = [node]\n",
    "        for c in node.children:\n",
    "            nodes.extend(collect_all_nodes(c))\n",
    "        return nodes\n",
    "\n",
    "    for idx in selected:\n",
    "        item = dataset[idx]\n",
    "        problem_text = item['problem']\n",
    "        correct_answer = extract_answer(item['solution'])\n",
    "\n",
    "        solution_node = astar.search(problem_text)\n",
    "        if solution_node:\n",
    "            response = solution_node.state\n",
    "            print(\"DEBUG: Found solution node:\", response)\n",
    "        else:\n",
    "            print(\"DEBUG: No solution node found. Using GPT fallback.\")\n",
    "            fallback_prompt = (\n",
    "                f\"Based on the problem: \\\"{problem_text}\\\", provide ONLY your final answer \"\n",
    "                \"in the exact format \\\\boxed{<final answer>}. Do not include any chain-of-thought or explanation.\"\n",
    "            )\n",
    "            response = get_api_response(fallback_prompt).strip()\n",
    "            print(\"DEBUG: Fallback response:\", response)\n",
    "\n",
    "        predicted_answer = extract_answer(response)\n",
    "        if not predicted_answer:\n",
    "            print(\"DEBUG: predicted_answer is empty. Raw response was:\\n\", response)\n",
    "\n",
    "        is_correct = compare_answers(correct_answer, predicted_answer or \"\")\n",
    "\n",
    "        save_result(results_file, {\n",
    "            \"index\": idx,\n",
    "            \"problem\": problem_text,\n",
    "            \"response\": response,\n",
    "            \"correct_answer\": correct_answer,\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"is_correct\": is_correct\n",
    "        })\n",
    "\n",
    "        if is_correct:\n",
    "            correct_count += 1\n",
    "        evaluated += 1\n",
    "        print(f\"Correct: {correct_count}/{evaluated} | Index: {idx}\")\n",
    "\n",
    "    final_results = load_existing_results(results_file)\n",
    "    analyze_results(final_results)\n",
    "\n",
    "# Example usage:\n",
    "evaluate_astar_random(max_samples=1)\n",
    "# To test with different parameters, adjust max_samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba04ca9",
   "metadata": {
    "id": "8ba04ca9",
    "papermill": {
     "duration": 0.032473,
     "end_time": "2025-05-23T01:29:42.814577",
     "exception": false,
     "start_time": "2025-05-23T01:29:42.782104",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Monte Carlo Tree Search (MCTS) for Mathematical Reasoning\n",
    "\n",
    "Monte Carlo Tree Search (MCTS) is a probabilistic search algorithm particularly well-suited for decision-making tasks in large, complex search spaces. In the context of mathematical problem solving with language models, each node in the search tree represents a partial reasoning step. MCTS incrementally builds the tree by exploring the most promising reasoning paths through random simulations, balancing exploration of new paths with exploitation of those that appear promising.\n",
    "\n",
    "## Core Components of MCTS\n",
    "\n",
    "MCTS operates in four main stages:\n",
    "\n",
    "1. **Selection:**  \n",
    "   Starting at the root (the initial problem), the algorithm traverses the tree by selecting child nodes based on a policy that balances two factors:  \n",
    "   - **Exploitation:** Favoring nodes that have already shown high promise (i.e., those with a good reward or low cost).  \n",
    "   - **Exploration:** Giving a chance to less-visited nodes to discover potentially promising new paths.  \n",
    "   This balance is often managed by a criterion such as the Upper Confidence Bound (UCB).\n",
    "\n",
    "2. **Expansion:**  \n",
    "   When a leaf node is reached—one that has not been fully expanded—the algorithm expands it by generating one or more child nodes. Each new node represents a possible next step in the reasoning process, such as a candidate final answer generated by the language model.\n",
    "\n",
    "3. **Simulation (Rollout):**  \n",
    "   From the newly expanded node, the algorithm performs a simulation (or rollout) to estimate the outcome if that reasoning path were followed to completion. For mathematical reasoning, this may involve prompting the language model to complete the remaining reasoning and produce a final answer. The outcome of the simulation provides an estimated reward or cost for that path.\n",
    "\n",
    "4. **Backpropagation:**  \n",
    "   The result of the simulation is then propagated back up the tree. Each node along the path has its evaluation updated based on the outcome, which in turn refines the selection policy for future iterations. This backpropagation ensures that nodes contributing to more promising outcomes are prioritized.\n",
    "\n",
    "## How MCTS Applies to Mathematical Reasoning\n",
    "\n",
    "- **State Representation:**  \n",
    "  Each node encapsulates a partial solution or chain-of-thought produced by the language model. The goal is to eventually obtain a final answer formatted in a concise manner (e.g., wrapped in `\\boxed{...}`).\n",
    "\n",
    "- **Reward and Heuristic:**  \n",
    "  The reward signal derived from the simulation reflects how close a reasoning path is to a correct final answer. The model's evaluations—such as plausibility scores—help guide the search by penalizing incomplete or incorrect paths.\n",
    "\n",
    "- **Balancing Exploration and Exploitation:**  \n",
    "  MCTS effectively manages the trade-off between exploring new, untested reasoning paths and exploiting those paths that have already demonstrated potential. This balance is critical in navigating the vast space of possible reasoning steps.\n",
    "\n",
    "## Challenges and Considerations\n",
    "\n",
    "- **Simulation Cost:**  \n",
    "  Running multiple simulations per node can be computationally intensive, especially when each simulation involves multiple language model calls.\n",
    "\n",
    "- **Reward Signal Design:**  \n",
    "  Defining an accurate and meaningful reward (or evaluation) for partial solutions is challenging. The reward must correlate well with the likelihood of ultimately arriving at a complete and correct final answer.\n",
    "\n",
    "- **Parameter Tuning:**  \n",
    "  Effective implementation of MCTS requires careful tuning of parameters like the exploration constant and the number of simulations per node. This tuning is essential to balance the search effectively.\n",
    "\n",
    "- **Ensuring Concise Final Answers:**  \n",
    "  One of the key objectives is to force the model to output only the final answer (without additional chain-of-thought text). This requires precise prompt engineering and robust fallback strategies in both candidate generation and simulation phases.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "MCTS provides a powerful framework for exploring the reasoning process in language models by combining random simulation with informed backpropagation. Through iterative exploration of promising reasoning paths and careful balancing of exploration and exploitation, MCTS aims to identify the most promising route to a final answer. With a strong focus on ensuring that only a concise final answer is produced (wrapped in a format like `\\boxed{...}`), this method offers significant potential for enhancing mathematical problem solving.\n",
    "\n",
    "By adjusting hyperparameters and refining the simulation and evaluation processes, you can experiment with different configurations to improve the efficiency and accuracy of the final answers. This makes MCTS a flexible and promising approach for further research and practical applications in guided reasoning with language models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db4c950",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T01:29:42.881775Z",
     "iopub.status.busy": "2025-05-23T01:29:42.881427Z",
     "iopub.status.idle": "2025-05-23T01:29:42.901601Z",
     "shell.execute_reply": "2025-05-23T01:29:42.900820Z"
    },
    "id": "0db4c950",
    "papermill": {
     "duration": 0.056224,
     "end_time": "2025-05-23T01:29:42.902798",
     "exception": false,
     "start_time": "2025-05-23T01:29:42.846574",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import math\n",
    "import heapq\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "def evaluate_with_gpt(problem: str, candidate: str) -> float:\n",
    "    \"\"\"\n",
    "    Ask the verifier LLM (via get_api_response) whether the boxed answer is correct.\n",
    "    Returns 1.0 for “yes”, 0.0 for “no”, or an intermediate score if provided.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You are a precise math grader.\\n\\n\"\n",
    "        f\"Problem:\\n\\\"{problem}\\\"\\n\\n\"\n",
    "        \"Final answer to check (in \\\\boxed{...} format):\\n\"\n",
    "        f\"\\\"{candidate}\\\"\\n\\n\"\n",
    "        \"Is this answer correct? Reply with a number between 0 (incorrect) and 1 (fully correct).\"\n",
    "    )\n",
    "    # TODO: Send `prompt` to `get_api_response`, strip whitespace\n",
    "    resp = get_api_response(prompt).strip()\n",
    "\n",
    "    # TODO: Extract the first numeric match with `re.search`\n",
    "    match = re.search(r'0(\\.\\d+)?|1(\\.0+)?', resp)\n",
    "\n",
    "    # TODO: Convert to float (fallback to 0.0 on failure)\n",
    "    # TODO: Clamp the result to [0.0, 1.0] and return it\n",
    "    score = float(match.group(0)) if match else 0.0\n",
    "    return max(0.0, min(1.0, score))\n",
    "\n",
    "\n",
    "class MCTSNode:\n",
    "    def __init__(self, state: str, parent: Optional['MCTSNode'] = None):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.children: List[MCTSNode] = []\n",
    "        self.visits = 0\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    def add_child(self, child: 'MCTSNode'):\n",
    "        self.children.append(child)\n",
    "\n",
    "    def __repr__(self):\n",
    "        preview = self.state if len(self.state) < 50 else self.state[:47] + \"...\"\n",
    "        return f\"MCTSNode(visits={self.visits}, reward={self.total_reward:.2f}, state='{preview}')\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MCTSSearch:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_simulations: int = 10,\n",
    "        exploration_const: float = 1.41,\n",
    "        max_depth: int = 3,\n",
    "        max_fallback: int = 3,\n",
    "        num_candidates: int = 1,\n",
    "        verbose: bool = False\n",
    "    ):\n",
    "        self.num_simulations = num_simulations\n",
    "        self.exploration_const = exploration_const\n",
    "        self.max_depth = max_depth\n",
    "        self.max_fallback = max_fallback\n",
    "        self.num_candidates = num_candidates\n",
    "        self.verbose = verbose\n",
    "        self.root_problem: Optional[str] = None\n",
    "\n",
    "    def is_solution(self, state: str) -> bool:\n",
    "        # TODO: return True if `extract_answer(state)` yields a non-empty boxed answer\n",
    "        return extract_answer(state) is not None\n",
    "\n",
    "    def generate_candidates(self, state: str) -> List[str]:\n",
    "        # TODO: build a prompt asking for `self.num_candidates` LaTeX \\\\boxed{...} answers to `state`\n",
    "        prompt = (\n",
    "            f\"Given the problem or partial solution: {state}\\n\\n\"\n",
    "            f\"Provide {self.num_candidates} possible complete solutions, each with a boxed final answer. \"\n",
    "            \"Provide each solution on a separate line.\"\n",
    "        )\n",
    "\n",
    "        # TODO: call `get_llm_response(prompt)`\n",
    "        response = get_llm_response(prompt)\n",
    "\n",
    "        # TODO: split response into non-empty lines\n",
    "        candidates = [line.strip() for line in response.split(\"\\n\") if line.strip()]\n",
    "\n",
    "        # TODO: if fewer than `self.num_candidates`, replicate lines to match\n",
    "        # TODO: return exactly `self.num_candidates` strings\n",
    "        if len(candidates) < self.num_candidates:\n",
    "            candidates += [candidates[-1]] * (self.num_candidates - len(candidates))\n",
    "        return candidates[:self.num_candidates]\n",
    "\n",
    "    def expand(self, node: MCTSNode) -> List[MCTSNode]:\n",
    "        if self.verbose:\n",
    "            print(f\"\\nExpanding node (depth {self._depth(node)}):\\n{node.state}\\n\")\n",
    "\n",
    "        # ensure we remember the original problem\n",
    "        if self.root_problem is None:\n",
    "            self.root_problem = node.state\n",
    "\n",
    "        # TODO: candidates = self.generate_candidates(node.state)\n",
    "        candidates = self.generate_candidates(node.state)\n",
    "        children: List[MCTSNode] = []\n",
    "\n",
    "        # TODO: for each `cand` in `candidates`:\n",
    "        for cand in candidates:\n",
    "            final = cand\n",
    "            attempts = 0\n",
    "            while not self.is_solution(final) and attempts < self.max_fallback:\n",
    "                fallback_prompt = (\n",
    "                    f\"The following solution is incomplete or incorrect: {final}\\n\\n\"\n",
    "                    f\"Please provide a correct, complete solution with a boxed final answer for the problem: {self.root_problem} \\n\\n\"\n",
    "                    \"Please provide ONLY your final answer in the exact format \\\\boxed{...} \"\n",
    "                    \"with no additional explanation.\"\n",
    "                )\n",
    "                # send strict fallback prompt via `get_llm_response`\n",
    "                # update `final = response.strip()`\n",
    "                final = get_llm_response(fallback_prompt).strip()\n",
    "                attempts += 1\n",
    "                if self.verbose: print(f\"Fallback attempt {attempts}: {final}\")\n",
    "\n",
    "            child = MCTSNode(state=final, parent=node)\n",
    "            node.add_child(child)\n",
    "            children.append(child)\n",
    "        return children\n",
    "\n",
    "\n",
    "    def simulate(self, node: MCTSNode) -> float:\n",
    "        \"\"\"\n",
    "        Instead of a blind rollout, directly evaluate the final boxed answer.\n",
    "        \"\"\"\n",
    "        # TODO: if `self.root_problem` is None, return 0.0\n",
    "        if self.root_problem is None:\n",
    "            return 0.0\n",
    "\n",
    "        # TODO: if not self.is_solution(node.state):\n",
    "        #         * send fallback prompt to `get_llm_response`\n",
    "        #         * node.state = response.strip()\n",
    "        if not self.is_solution(node.state):\n",
    "            fallback_prompt = (\n",
    "                f\"Problem: {self.root_problem}\\n\\n\"\n",
    "                \"Please provide ONLY your final answer in the exact format \\\\boxed{...} \"\n",
    "                \"with no additional explanation.\"\n",
    "            )\n",
    "            node.state = get_llm_response(fallback_prompt).strip()\n",
    "\n",
    "        # TODO: return evaluate_with_gpt(self.root_problem, node.state)\n",
    "        return evaluate_with_gpt(self.root_problem, node.state)\n",
    "\n",
    "\n",
    "    def ucb_score(self, child: MCTSNode, parent_visits: int) -> float:\n",
    "        # TODO: if child.visits == 0: return float('inf')\n",
    "        if child.visits == 0: return float('inf')\n",
    "\n",
    "        exploit = child.total_reward / child.visits\n",
    "        explore = self.exploration_const * math.sqrt(math.log(parent_visits) / child.visits)\n",
    "        return exploit + explore\n",
    "\n",
    "\n",
    "    def select(self, root: MCTSNode) -> MCTSNode:\n",
    "        # TODO: starting at `root`, repeatedly pick the child with highest `ucb_score`\n",
    "        #       until you reach a node with no children, then return it\n",
    "        node = root\n",
    "        while node.children:\n",
    "            best_child = max(node.children, key=lambda c: self.ucb_score(c, node.visits))\n",
    "            node = best_child\n",
    "        return node\n",
    "\n",
    "    def backpropagate(self, node: MCTSNode, reward: float):\n",
    "        while node is not None:\n",
    "            node.visits += 1\n",
    "            node.total_reward += reward\n",
    "            node = node.parent\n",
    "\n",
    "\n",
    "    def _depth(self, node: MCTSNode) -> int:\n",
    "        d = 0\n",
    "        while node.parent:\n",
    "            d += 1\n",
    "            node = node.parent\n",
    "        return d\n",
    "\n",
    "    def search(self, initial_problem: str) -> Optional[MCTSNode]:\n",
    "        \"\"\"\n",
    "        Perform MCTS using GPT as the verifier:\n",
    "        - Selection: `select`\n",
    "        - Expansion: `expand`\n",
    "        - Simulation/Backprop: `simulate` + `backpropagate`\n",
    "        \"\"\"\n",
    "        # TODO: set `self.root_problem = initial_problem`\n",
    "        # TODO: create `root = MCTSNode(state=initial_problem)`.\n",
    "        self.root_problem = initial_problem\n",
    "        root = MCTSNode(state=initial_problem)\n",
    "\n",
    "        # TODO: for `_` in range(self.num_simulations):\n",
    "        for _ in range(self.num_simulations):\n",
    "            leaf = self.select(root)\n",
    "            if self._depth(leaf) >= self.max_depth:\n",
    "                reward = self.simulate(leaf)\n",
    "                self.backpropagate(leaf, reward)\n",
    "                continue\n",
    "\n",
    "            children = self.expand(leaf)\n",
    "            if not children: continue\n",
    "\n",
    "            reward = self.simulate(children[0])\n",
    "            self.backpropagate(children[0], reward)\n",
    "\n",
    "        return max(root.children, key=lambda c: c.total_reward / c.visits if c.visits > 0 else 0) if root.children else root\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e34985c",
   "metadata": {
    "id": "7e34985c",
    "papermill": {
     "duration": 0.031508,
     "end_time": "2025-05-23T01:29:42.967389",
     "exception": false,
     "start_time": "2025-05-23T01:29:42.935881",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test Code for MCTS Search Method with Minimal Hyperparameters\n",
    "\n",
    "Below is a description of the test procedure for the MCTS search method using minimal hyperparameters. This test ensures that the algorithm returns only the final answer in the proper format, without extra chain-of-thought text.\n",
    "\n",
    "- **Initialization:**\n",
    "  - An MCTS search instance is created with the following settings:\n",
    "    - `num_simulations`: 5 (number of simulations to run for exploring reasoning paths)\n",
    "    - `exploration_const`: 1.41 (parameter to balance exploration and exploitation)\n",
    "    - `max_depth`: 1 (the search tree is kept shallow for testing)\n",
    "    - `max_fallback`: 3 (up to three fallback attempts are made to force a final answer)\n",
    "    - `num_candidates`: 1 (only one candidate is generated per node)\n",
    "    - `verbose`: True (detailed debug information is printed)\n",
    "  - The initial problem is set as:\n",
    "    - \"Solve the integral: \\( \\int_0^1 x^2 \\, dx \\)\"\n",
    "\n",
    "- **Search Execution:**\n",
    "  - The MCTS search is executed on the initial problem to obtain a solution node.\n",
    "\n",
    "- **Tree Printing:**\n",
    "  - A recursive function (e.g., `print_mcts_tree`) prints the entire MCTS search tree starting from the root. This allows inspection of all nodes and the reasoning process.\n",
    "\n",
    "- **Final Answer Extraction:**\n",
    "  - If a solution node is found, the algorithm backtracks to the root and prints the full tree.\n",
    "  - The final answer is then extracted from the solution node using a helper function (e.g., `extract_answer`) and reformatted to display only the final answer in the exact format (e.g., `\\boxed{<final answer>}`), ensuring that no extra explanation or chain-of-thought text is present.\n",
    "\n",
    "- **Fallback Handling:**\n",
    "  - If no solution node is found, a fallback prompt is issued that instructs the model to provide ONLY the final answer in the correct format, with no additional explanation.\n",
    "  - The fallback final answer is printed.\n",
    "\n",
    "This test code is designed to verify that, with minimal hyperparameters, the MCTS search method consistently returns a complete final answer. Users are encouraged to experiment with these hyperparameters and refine the prompts to further improve performance and compare the results with other methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad3821b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T01:29:43.031428Z",
     "iopub.status.busy": "2025-05-23T01:29:43.031094Z",
     "iopub.status.idle": "2025-05-23T01:31:04.091385Z",
     "shell.execute_reply": "2025-05-23T01:31:04.090519Z"
    },
    "id": "7ad3821b",
    "outputId": "76a7a4ab-724d-41cd-fde1-55caa962e13c",
    "papermill": {
     "duration": 81.125474,
     "end_time": "2025-05-23T01:31:04.123795",
     "exception": false,
     "start_time": "2025-05-23T01:29:42.998321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expanding node (depth 0):\n",
      "Solve the integral: \\( \\int_0^1 x^2 \\, dx \\)\n",
      "\n",
      "Fallback attempt 1: Okay, so I need to solve the integral of x squared from 0 to 1. Hmm, integrals can sometimes be tricky, but I remember that integration is essentially finding the area under a curve. In this case, the function is x squared, which is a parabola that opens upwards. The limits are from 0 to 1, so I'm looking for the area under that curve between those two points.\n",
      "\n",
      "First, I recall that the integral of x^n is a standard formula. I think it's something like (x^(n+1))/(n+1) plus a constant, right? So, for x squared, which is x^2, n would be 2. Applying the formula, the integral should be (x^(2+1))/(2+1) = x^3/3. So, the indefinite integral of x squared is x cubed over 3.\n",
      "\n",
      "But wait, since we're dealing with a definite integral from 0 to 1, I need to evaluate the indefinite integral at the upper limit and subtract its value at the lower limit. That means I should compute [x^3/3] from 0 to 1. \n",
      "\n",
      "Let me plug in the upper limit, which is 1. So, (1)^3 / 3 is 1/3. Now, plug in the lower limit, which is 0. (0)^3 / 3 is 0. So, subtracting the lower limit from the upper limit gives me 1/3 - 0, which is just 1/3. \n",
      "\n",
      "But wait, is that it? I feel like I might be missing something. Let me double-check. The integral of x squared is indeed x cubed over 3, and evaluating that from 0 to 1 gives 1/3. Hmm, that seems right. Maybe I can visualize it. If I have a parabola starting at (0,0) and going up to (1,1), the area under it should be less than 1 because the function is increasing, but more than 0. Since the integral is 1/3, that makes sense because 1/3 is approximately 0.333, which is between 0 and 1. \n",
      "\n",
      "Alternatively, I remember that the area under the curve y = x^2 from 0 to 1 can be approximated using Riemann sums. If\n",
      "Fallback attempt 2: Okay, so I need to solve the integral of x squared from 0 to 1. Hmm, let me think about how to approach this. I remember that integration is about finding the area under a curve, right? In this case, the curve is y = x squared, which is a parabola opening upwards. The limits are from 0 to 1, so we're looking for the area between the curve and the x-axis from x = 0 to x = 1.\n",
      "\n",
      "I think the first step is to find the indefinite integral of x squared. I recall that the integral of x^n is (x^(n+1))/(n+1) + C, where C is the constant of integration. So, applying that formula to x squared, which is x^2, we get n = 2. So, the integral should be (x^(2+1))/(2+1) = x^3/3 + C. That makes sense.\n",
      "\n",
      "Now, since we have a definite integral from 0 to 1, I need to evaluate the indefinite integral at the upper limit and subtract its value at the lower limit. So, let's compute F(1) - F(0), where F(x) is the antiderivative of x squared, which is x^3/3.\n",
      "\n",
      "Calculating F(1): (1)^3 / 3 = 1/3.\n",
      "\n",
      "Calculating F(0): (0)^3 / 3 = 0.\n",
      "\n",
      "Subtracting these, we get 1/3 - 0 = 1/3.\n",
      "\n",
      "Wait, so the integral of x squared from 0 to 1 is 1/3. That seems straightforward. But just to make sure I haven't made a mistake, let me think through it again.\n",
      "\n",
      "If I consider the function y = x^2, the area under the curve from 0 to 1 is indeed a classic example. I remember that the integral of x squared is x cubed over 3, so plugging in 1 and 0 gives me 1/3 and 0, respectively. Subtracting, I get 1/3. Yeah, that checks out.\n",
      "\n",
      "Alternatively, I can approximate the integral using Riemann sums to see if it converges to 1/3. Let's try a simple case with a small number of rectangles, say n = 4, and see what the approximate area is.\n",
      "Fallback attempt 3: Okay, so I need to solve the integral of x squared from 0 to 1. Hmm, let me think about how to approach this. I remember that integration is about finding the area under a curve, right? In this case, the curve is y = x squared, which is a parabola opening upwards. The limits are from 0 to 1, so we're looking for the area between the curve and the x-axis from x = 0 to x = 1.\n",
      "\n",
      "I think the first step is to find the indefinite integral of x squared. I recall that the integral of x^n is (x^(n+1))/(n+1) + C, where C is the constant of integration. So, applying that formula to x squared, which is x^2, we get n = 2. So, the integral should be (x^(2+1))/(2+1) = x^3/3 + C. That makes sense.\n",
      "\n",
      "Now, since we have a definite integral from 0 to 1, I need to evaluate the indefinite integral at the upper limit and subtract its value at the lower limit. So, let's compute F(1) - F(0), where F(x) is the antiderivative of x squared, which is x^3/3.\n",
      "\n",
      "Calculating F(1): (1)^3 / 3 = 1/3.\n",
      "\n",
      "Calculating F(0): (0)^3 / 3 = 0.\n",
      "\n",
      "Subtracting these, we get 1/3 - 0 = 1/3.\n",
      "\n",
      "Wait, so the integral of x squared from 0 to 1 is 1/3. That seems straightforward. But just to make sure I haven't made a mistake, let me think through it again.\n",
      "\n",
      "If I consider the function y = x^2, the area under the curve from 0 to 1 is indeed a classic example. I remember that the integral of x squared is x cubed over 3, so plugging in 1 and 0 gives me 1/3 and 0, respectively. Subtracting, I get 1/3. Yeah, that checks out.\n",
      "\n",
      "Alternatively, I can approximate the integral using Riemann sums to see if it converges to 1/3. Let's try a simple case with a small number of rectangles, say n = 4, and see what the approximate area is.\n",
      "MCTSNode(visits=5, reward=5.00, state='Solve the integral: \\( \\int_0^1 x^2 \\, dx \\)')\n",
      "  MCTSNode(visits=5, reward=5.00, state='Okay, so I need to solve the integral \\( \\int_0...')\n",
      "\n",
      "Final Answer Found:\n",
      "Okay, so I need to solve the integral \\( \\int_0^1 x^2 \\, dx \\). Hmm, I remember that integrals are about finding the area under a curve, right? In this case, the function is \\( x^2 \\) from 0 to 1. Let me think about how to approach this.\n",
      "\n",
      "First, I should recall the basic formula for integrating a power function. I think it's something like \\( \\int x^n \\, dx = \\frac{x^{n+1}}{n+1} + C \\), where \\( C \\) is the constant of integration. But wait, this is a definite integral, so I don't need the constant. The limits of integration will handle that.\n",
      "\n",
      "So, applying that formula to \\( x^2 \\), I can set \\( n = 2 \\). That gives me \\( \\int x^2 \\, dx = \\frac{x^{3}}{3} + C \\). But since it's a definite integral from 0 to 1, I should evaluate this from 0 to 1.\n",
      "\n",
      "Let me write that out: \\( \\left[ \\frac{x^3}{3} \\right]_0^1 \\). That means I substitute the upper limit first, which is 1, and then subtract the value when \\( x = 0 \\).\n",
      "\n",
      "Calculating the upper limit: \\( \\frac{1^3}{3} = \\frac{1}{3} \\).\n",
      "\n",
      "Now, the lower limit: \\( \\frac{0^3}{3} = 0 \\).\n",
      "\n",
      "Subtracting the lower from the upper: \\( \\frac{1}{3} - 0 = \\frac{1}{3} \\).\n",
      "\n",
      "Wait, so the integral \\( \\int_0^1 x^2 \\, dx \\) equals \\( \\frac{1}{3} \\). That seems straightforward, but let me double-check to make sure I didn't make any mistakes.\n",
      "\n",
      "I remember that the integral of \\( x^2 \\) is indeed \\( \\frac{x^3}{3} \\). So, plugging in 1 gives \\( \\frac{1}{3} \\), and plugging in 0 gives 0. Subtracting them gives \\( \\frac{1}{3} \\). Hmm, that seems right.\n",
      "\n",
      "Alternatively, I can think of it as the area under the curve \\( y = x^2 \\\n"
     ]
    }
   ],
   "source": [
    "# Test Code for MCTS Search Method with Minimal Hyperparameters\n",
    "\n",
    "# Initialize the MCTS search instance with minimal settings.\n",
    "# Generation uses get_llm_response(...), evaluation/fallback uses get_api_response(...)\n",
    "mcts = MCTSSearch(\n",
    "    num_simulations=5,\n",
    "    exploration_const=1.41,\n",
    "    max_depth=1,\n",
    "    max_fallback=3,\n",
    "    num_candidates=1,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "initial_problem = \"Solve the integral: \\\\( \\\\int_0^1 x^2 \\\\, dx \\\\)\"\n",
    "solution_node = mcts.search(initial_problem)\n",
    "\n",
    "# Recursive printer for the MCTS tree.\n",
    "def print_mcts_tree(node, indent=\"\"):\n",
    "    print(indent + repr(node))\n",
    "    for child in node.children:\n",
    "        print_mcts_tree(child, indent + \"  \")\n",
    "\n",
    "if solution_node:\n",
    "    # Backtrack to the root node.\n",
    "    root = solution_node\n",
    "    while root.parent is not None:\n",
    "        root = root.parent\n",
    "\n",
    "    # Print the entire tree from the root.\n",
    "    print_mcts_tree(root)\n",
    "\n",
    "    # Extract and normalize the final boxed answer.\n",
    "    final = extract_answer(solution_node.state)\n",
    "    if final:\n",
    "        solution_node.state = f\"\\\\boxed{{{final}}}\"\n",
    "    print(\"\\nFinal Answer Found:\")\n",
    "    print(solution_node.state)\n",
    "else:\n",
    "    # No solution found: use GPT/Gemini verifier directly for final answer.\n",
    "    fallback_prompt = (\n",
    "        f\"Based on the problem: \\\"{initial_problem}\\\", please provide ONLY your final answer \"\n",
    "        \"in the exact format \\\\boxed{<final answer>}. Do not include any chain-of-thought or extra explanation.\"\n",
    "    )\n",
    "    forced_final_answer = get_api_response(fallback_prompt).strip()\n",
    "    final = extract_answer(forced_final_answer) or forced_final_answer\n",
    "    forced_final_answer = f\"\\\\boxed{{{final}}}\"\n",
    "    print(\"\\nNo complete solution found. Forced final answer:\")\n",
    "    print(forced_final_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246049a8",
   "metadata": {
    "id": "246049a8",
    "papermill": {
     "duration": 0.131566,
     "end_time": "2025-05-23T01:31:04.288073",
     "exception": false,
     "start_time": "2025-05-23T01:31:04.156507",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluation of MCTS Search Method on the Math500 Dataset\n",
    "\n",
    "This evaluation framework is designed to test the MCTS search method on a subset of the Math500 dataset. The framework is highly configurable via hyperparameters and forces the model to output only the final answer in the expected format (e.g., `\\boxed{<final answer>}`) with no additional chain-of-thought text.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Unique Results File:**  \n",
    "  A dedicated results file (e.g., `evaluation_results_mcts_test.json`) is used to save evaluation data. The file is cleared at the beginning of each run to ensure a fresh start without interference from previous evaluations.\n",
    "\n",
    "- **Configurable Hyperparameters:**  \n",
    "  You can adjust parameters such as:\n",
    "  - `num_simulations`: Number of MCTS simulations used to explore reasoning paths.\n",
    "  - `exploration_const`: The constant used in the UCB formula to balance exploration and exploitation.\n",
    "  - `max_depth`: Maximum depth of the search tree.\n",
    "  - `max_fallback`: Maximum number of fallback attempts to force the model to output a final answer.\n",
    "  - `num_candidates`: Number of candidate final answers generated per node.\n",
    "  \n",
    "  These settings allow you to experiment with different reasoning strategies and trade-offs between search depth and the precision of the final answer.\n",
    "\n",
    "- **Sample Selection:**  \n",
    "  The `max_samples` parameter controls the number of problems from the dataset to evaluate. This lets you start by testing on a single sample and then scale the evaluation to larger subsets (e.g., 100 or more problems) as desired.\n",
    "\n",
    "- **Final Answer Extraction and Fallback Mechanism:**  \n",
    "  The evaluation process extracts only the final answer from the model’s output (using a helper like `extract_answer`), ensuring that only a concise final answer is compared against the expected solution. If no complete final answer is found within the search tree, a fallback prompt is issued to force the model to provide the final answer in the correct format.\n",
    "\n",
    "- **Result Saving and Analysis:**  \n",
    "  Each problem’s evaluation result—including the problem, the model’s raw response, the extracted final answer, and correctness—is saved to the results file. A summary report is then generated, which includes key metrics such as total evaluated problems, number of correct answers, and overall accuracy.\n",
    "\n",
    "## Encouragement for Further Improvement\n",
    "\n",
    "- **Experiment with the Exploration Constant:**  \n",
    "  Try using different values for the exploration constant (e.g., 0.5, 1.41, 2.0) to see how they affect the balance between exploring new nodes and exploiting known promising ones. Compare the results and observe how the search tree structure and the final answer accuracy change with each setting.\n",
    "\n",
    "- **Tuning Other Parameters:**  \n",
    "  Experiment with other hyperparameters such as `num_simulations`, `max_depth`, and `num_candidates`. Adjusting these values can impact the thoroughness of the search and the likelihood of obtaining a complete final answer.\n",
    "\n",
    "- **Document Your Observations:**  \n",
    "  As you tweak the parameters, please comment on your expectations and the outcomes:\n",
    "  - What changes do you observe when you adjust the exploration constant?\n",
    "  - How does increasing the maximum depth influence the quality and correctness of the final answer?\n",
    "  - Does generating more candidates per node lead to more accurate results?\n",
    "  \n",
    "  Sharing your observations and comparing them with results from other approaches (such as A* or the Tree of Thoughts method) can provide valuable insights and help guide further improvements.\n",
    "\n",
    "This framework is designed to be flexible, so feel free to adjust the parameters and prompts to suit your research needs and push the performance of the MCTS method further.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f24ca0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T01:31:04.352418Z",
     "iopub.status.busy": "2025-05-23T01:31:04.351917Z",
     "iopub.status.idle": "2025-05-23T01:32:26.028854Z",
     "shell.execute_reply": "2025-05-23T01:32:26.028025Z"
    },
    "id": "3f24ca0d",
    "outputId": "e05414d1-4a21-4f11-b2e7-01a6e813ff94",
    "papermill": {
     "duration": 81.714243,
     "end_time": "2025-05-23T01:32:26.033852",
     "exception": false,
     "start_time": "2025-05-23T01:31:04.319609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expanding node (depth 0):\n",
      "Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the form $(r,\\theta),$ where $r > 0$ and $0 \\le \\theta < 2 \\pi.$\n",
      "\n",
      "Fallback attempt 1: Alright, so I need to convert the rectangular coordinates (0,3) to polar coordinates. Hmm, okay. I remember that polar coordinates are (r, θ), where r is the distance from the origin to the point, and θ is the angle made with the positive x-axis. \n",
      "\n",
      "Let me recall the formulas for converting from rectangular to polar coordinates. I think it's something like r = sqrt(x² + y²) and θ = arctan(y/x). Yeah, that sounds right. So, in this case, the point is (0,3). Let me plug those values into the formulas.\n",
      "\n",
      "First, calculating r. Since x is 0 and y is 3, plugging into the formula for r: r = sqrt(0² + 3²) = sqrt(0 + 9) = sqrt(9) = 3. Okay, so r is 3. That seems straightforward.\n",
      "\n",
      "Now, for θ. The formula is θ = arctan(y/x). So plugging in the values, θ = arctan(3/0). Wait a second, division by zero. Hmm, what does arctan(3/0) mean? Well, 3/0 is undefined, but in the context of limits, if x approaches 0 from the positive side, arctan(y/x) approaches π/2, and from the negative side, it approaches -π/2. But in this case, since x is 0 and y is positive, we should be in the case where the point is on the positive y-axis.\n",
      "\n",
      "I remember that in polar coordinates, the angle θ is measured from the positive x-axis. So when x is 0 and y is positive, the angle should be π/2 radians, which is 90 degrees. That makes sense because the point (0,3) is straight up along the y-axis.\n",
      "\n",
      "But wait, let me double-check. If x is 0 and y is positive, the point is on the positive y-axis, so θ is π/2. If x were 0 and y were negative, θ would be 3π/2. Since in our case y is positive, θ is π/2. So, θ = π/2.\n",
      "\n",
      "Therefore, the polar coordinates are (3, π/2). Let me just visualize this. On the coordinate plane, the\n",
      "Fallback attempt 2: Alright, so I need to convert the rectangular coordinates (0, 3) to polar coordinates. Hmm, I remember that polar coordinates are represented as (r, θ), where r is the distance from the origin to the point, and θ is the angle made with the positive x-axis. \n",
      "\n",
      "Let me start by recalling the formulas for converting from rectangular (x, y) to polar (r, θ) coordinates. I think the formulas are:\n",
      "\n",
      "r = √(x² + y²)\n",
      "\n",
      "θ = arctan(y/x)\n",
      "\n",
      "Okay, so plugging in the values x = 0 and y = 3. \n",
      "\n",
      "First, calculating r. \n",
      "\n",
      "r = √(0² + 3²) = √(0 + 9) = √9 = 3\n",
      "\n",
      "So, r is 3. That seems straightforward. \n",
      "\n",
      "Now, for θ. The formula is θ = arctan(y/x). So, plugging in the values, θ = arctan(3/0). Hmm, division by zero. \n",
      "\n",
      "Wait, arctan of infinity? Or is it undefined? Hmm, when x is 0 and y is positive, we're at the positive y-axis. So, in standard position, that's 90 degrees, or π/2 radians. \n",
      "\n",
      "But let me think about this again. If I have x = 0 and y positive, arctan(y/x) is arctan(∞), which is π/2. But in reality, arctan(∞) is π/2. So, θ is π/2.\n",
      "\n",
      "Alternatively, sometimes people use the atan2 function, which takes into account the signs of both x and y. But in this case, x is 0 and y is positive, so atan2(3, 0) should give π/2. \n",
      "\n",
      "Wait, let me recall: atan2(y, x) returns the angle in the correct quadrant based on the signs of x and y. So, if x is positive, it's in the first or fourth quadrant; if x is negative, it's in the second or third. If x is zero, then it's on the positive or negative y-axis. Since y is positive here, it's the positive y-axis, which is π/2. \n",
      "\n",
      "So, yeah, θ is π/2. \n",
      "\n",
      "Therefore, the polar coordinates\n",
      "Fallback attempt 3: Okay, so I need to convert the rectangular coordinates (0, 3) to polar coordinates. Hmm, I remember that polar coordinates are represented as (r, θ), where r is the distance from the origin to the point, and θ is the angle made with the positive x-axis. \n",
      "\n",
      "First, let me recall the formulas for converting from rectangular (x, y) to polar (r, θ) coordinates. I think the formulas are:\n",
      "\n",
      "r = √(x² + y²)\n",
      "\n",
      "θ = arctan(y/x)\n",
      "\n",
      "Alright, so plugging in the values x = 0 and y = 3.\n",
      "\n",
      "Starting with r: \n",
      "\n",
      "r = √(0² + 3²) = √(0 + 9) = √9 = 3\n",
      "\n",
      "So, r is 3. That seems straightforward.\n",
      "\n",
      "Now, for θ: \n",
      "\n",
      "θ = arctan(y/x) = arctan(3/0)\n",
      "\n",
      "Wait, division by zero. Hmm, arctan of infinity? Or is it undefined? Let me think about this again. \n",
      "\n",
      "When x is 0 and y is positive, we're at the positive y-axis. So, in standard position, that's 90 degrees, or π/2 radians. \n",
      "\n",
      "But let me confirm this by considering the atan2 function, which takes into account the signs of both x and y. Since x is 0 and y is positive, atan2(3, 0) should give π/2. \n",
      "\n",
      "Alternatively, sometimes people use the inverse tangent function, but that can give different results depending on the quadrant. For example, if both x and y are positive, it's in the first quadrant, which is 0 to π/2. If both are negative, it's in the third quadrant, which is π to 3π/2. If x is negative and y is positive, it's in the second quadrant, which is π/2 to π. \n",
      "\n",
      "In this case, x is 0 and y is positive, so it's on the positive y-axis, which is the boundary between the first and second quadrants. I think in this case, it's considered to be in the first quadrant, so θ is π/2. \n",
      "\n",
      "So, putting it all together, the polar coordinates are (3, π/2). \n",
      "\n",
      "Wait, let me make sure I'm not making a mistake here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating problems: 100%|██████████| 500/500 [01:21<00:00,  6.16it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Found solution node: Okay, so I need to convert the rectangular coordinates (0, 3) to polar coordinates. Hmm, I remember that polar coordinates are represented as (r, θ), where r is the distance from the origin to the point, and θ is the angle made with the positive x-axis. \n",
      "\n",
      "First, let me recall the formulas for converting from rectangular (Cartesian) coordinates to polar. I think they are:\n",
      "\n",
      "r = √(x² + y²)\n",
      "\n",
      "θ = arctan(y/x)\n",
      "\n",
      "Yeah, that sounds right. So, given the point (0, 3), which is on the y-axis, I can plug those values into the formulas.\n",
      "\n",
      "Starting with r. Since x is 0 and y is 3, let's compute r:\n",
      "\n",
      "r = √(0² + 3²) = √(0 + 9) = √9 = 3\n",
      "\n",
      "Okay, so r is 3. That makes sense because the point is 3 units away from the origin, which is on the positive y-axis.\n",
      "\n",
      "Now, for θ. The formula is θ = arctan(y/x). But wait, since x is 0, we have a division by zero here. Hmm, arctan(∞) or arctan(-∞)? Let me think.\n",
      "\n",
      "When x is 0, the point is on the y-axis. So, if y is positive, it's on the positive y-axis, which is 90 degrees or π/2 radians. If y is negative, it's on the negative y-axis, which is 270 degrees or 3π/2 radians. But in our case, y is positive, so θ should be π/2 radians.\n",
      "\n",
      "But let me double-check using the arctan formula. Normally, arctan(y/x) gives the angle in the first quadrant if x and y are both positive. But since x is 0, it's undefined. So, I need to handle this case separately.\n",
      "\n",
      "I remember that when x is 0, if y is positive, θ is π/2, and if y is negative, θ is 3π/2. Since in our case, y is positive, θ is π/2.\n",
      "\n",
      "Alternatively, another way to think about it is that when x is 0, the angle is 90 degrees or π/2 radians.\n",
      "\n",
      "So, putting\n",
      "DEBUG: predicted_answer is empty. Raw response was:\n",
      " Okay, so I need to convert the rectangular coordinates (0, 3) to polar coordinates. Hmm, I remember that polar coordinates are represented as (r, θ), where r is the distance from the origin to the point, and θ is the angle made with the positive x-axis. \n",
      "\n",
      "First, let me recall the formulas for converting from rectangular (Cartesian) coordinates to polar. I think they are:\n",
      "\n",
      "r = √(x² + y²)\n",
      "\n",
      "θ = arctan(y/x)\n",
      "\n",
      "Yeah, that sounds right. So, given the point (0, 3), which is on the y-axis, I can plug those values into the formulas.\n",
      "\n",
      "Starting with r. Since x is 0 and y is 3, let's compute r:\n",
      "\n",
      "r = √(0² + 3²) = √(0 + 9) = √9 = 3\n",
      "\n",
      "Okay, so r is 3. That makes sense because the point is 3 units away from the origin, which is on the positive y-axis.\n",
      "\n",
      "Now, for θ. The formula is θ = arctan(y/x). But wait, since x is 0, we have a division by zero here. Hmm, arctan(∞) or arctan(-∞)? Let me think.\n",
      "\n",
      "When x is 0, the point is on the y-axis. So, if y is positive, it's on the positive y-axis, which is 90 degrees or π/2 radians. If y is negative, it's on the negative y-axis, which is 270 degrees or 3π/2 radians. But in our case, y is positive, so θ should be π/2 radians.\n",
      "\n",
      "But let me double-check using the arctan formula. Normally, arctan(y/x) gives the angle in the first quadrant if x and y are both positive. But since x is 0, it's undefined. So, I need to handle this case separately.\n",
      "\n",
      "I remember that when x is 0, if y is positive, θ is π/2, and if y is negative, θ is 3π/2. Since in our case, y is positive, θ is π/2.\n",
      "\n",
      "Alternatively, another way to think about it is that when x is 0, the angle is 90 degrees or π/2 radians.\n",
      "\n",
      "So, putting\n",
      "Correct: 0/1 | Index: 0\n",
      "\n",
      "=== Results Summary ===\n",
      "Total problems: 1\n",
      "Correct answers: 0\n",
      "Accuracy: 0.00%\n",
      "\n",
      "=== Incorrect Problems ===\n",
      "Problem 0:\n",
      "Expected: \\left( 3, \\frac{\\pi}{2} \\right)\n",
      "Predicted: \n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_mcts(max_samples=1):\n",
    "    \"\"\"\n",
    "    Evaluate the MCTS search method on a subset of Math500, using GPT/Gemini\n",
    "    (via get_api_response) for any fallback final‑answer requests.\n",
    "    Assumes these helpers exist:\n",
    "      - load_math500_dataset()\n",
    "      - extract_answer(text)\n",
    "      - compare_answers(correct, pred)\n",
    "      - save_result(filename, result_dict)\n",
    "      - load_existing_results(filename)\n",
    "      - analyze_results(results_list)\n",
    "      - get_llm_response(prompt)   # for candidate generation\n",
    "      - get_api_response(prompt)   # for GPT/Gemini verification/fallback\n",
    "    \"\"\"\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    results_file = \"results/evaluation_results_mcts_test.json\"\n",
    "    if os.path.exists(results_file):\n",
    "        os.remove(results_file)\n",
    "\n",
    "    dataset = load_math500_dataset()\n",
    "    existing = load_existing_results(results_file)\n",
    "    processed = {r['index'] for r in existing}\n",
    "\n",
    "    mcts = MCTSSearch(\n",
    "        num_simulations=5,\n",
    "        exploration_const=1.41,\n",
    "        max_depth=1,\n",
    "        max_fallback=3,\n",
    "        num_candidates=1,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    correct_count = 0\n",
    "    evaluated = 0\n",
    "\n",
    "    def collect_all_nodes(node):\n",
    "        nodes = [node]\n",
    "        for child in node.children:\n",
    "            nodes.extend(collect_all_nodes(child))\n",
    "        return nodes\n",
    "\n",
    "    for idx, item in enumerate(tqdm(dataset, desc=\"Evaluating problems\")):\n",
    "        if idx in processed or evaluated >= max_samples:\n",
    "            continue\n",
    "\n",
    "        problem_text = item['problem']\n",
    "        correct_answer = extract_answer(item['solution'])\n",
    "\n",
    "        # Run MCTS search\n",
    "        solution_node = mcts.search(problem_text)\n",
    "\n",
    "        if solution_node:\n",
    "            response = solution_node.state\n",
    "            print(\"DEBUG: Found solution node:\", response)\n",
    "        else:\n",
    "            print(\"DEBUG: No solution node found. Using GPT fallback.\")\n",
    "            fallback_prompt = (\n",
    "                f\"Based on the problem: \\\"{problem_text}\\\", provide ONLY your final answer \"\n",
    "                \"in the exact format \\\\boxed{<final answer>}. Do not include any chain‑of‑thought or explanation.\"\n",
    "            )\n",
    "            response = get_api_response(fallback_prompt).strip()\n",
    "            print(\"DEBUG: Fallback response:\", response)\n",
    "\n",
    "        predicted_answer = extract_answer(response) or \"\"\n",
    "        if not predicted_answer:\n",
    "            print(\"DEBUG: predicted_answer is empty. Raw response was:\\n\", response)\n",
    "\n",
    "        is_correct = compare_answers(correct_answer, predicted_answer)\n",
    "        save_result(results_file, {\n",
    "            \"index\": idx,\n",
    "            \"problem\": problem_text,\n",
    "            \"response\": response,\n",
    "            \"correct_answer\": correct_answer,\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"is_correct\": is_correct\n",
    "        })\n",
    "\n",
    "        if is_correct:\n",
    "            correct_count += 1\n",
    "        evaluated += 1\n",
    "        print(f\"Correct: {correct_count}/{evaluated} | Index: {idx}\")\n",
    "\n",
    "    final_results = load_existing_results(results_file)\n",
    "    analyze_results(final_results)\n",
    "\n",
    "# Example: evaluate a single sample\n",
    "evaluate_mcts(max_samples=1)\n",
    "# To test more problems, increase max_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13f24f5",
   "metadata": {
    "id": "c13f24f5",
    "papermill": {
     "duration": 0.031372,
     "end_time": "2025-05-23T01:32:26.102247",
     "exception": false,
     "start_time": "2025-05-23T01:32:26.070875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17919.420319,
   "end_time": "2025-05-23T01:32:28.772505",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-22T20:33:49.352186",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
